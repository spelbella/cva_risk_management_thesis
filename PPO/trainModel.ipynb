{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\osc16\\miniconda3\\envs\\d2d\\Lib\\site-packages\\torch\\utils\\_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import gymnasium as gym\n",
    "from typing import Callable\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.vec_env import VecMonitor\n",
    "import torch as th\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Import Our environment\n",
    "from dev_env import tradingEng\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Paths\n",
    "with open(\"ZeroCorrFrs1Half\",\"rb\") as fp:\n",
    "    paths1 = pickle.load(fp)\n",
    "\n",
    "# Load Paths\n",
    "with open(\"ZeroCorrFrs2Half\",\"rb\") as fp:\n",
    "    paths1 = paths1 + pickle.load(fp)\n",
    "\n",
    "with open(\"ZeroCorrSnd1Half\",\"rb\") as fp:\n",
    "    paths2 = pickle.load(fp)\n",
    "\n",
    "with open(\"ZeroCorrSnd2Half\",\"rb\") as fp:\n",
    "    paths2 = paths2 + pickle.load(fp)\n",
    "\n",
    "with open(\"ZeroCorrTest\",\"rb\") as fp:\n",
    "    paths_ev = pickle.load(fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## LR schedule\n",
    "def linear_schedule(initial_value: float) -> Callable[[float], float]:\n",
    "    \"\"\"\n",
    "    Linear learning rate schedule.\n",
    "\n",
    "    :param initial_value: Initial learning rate.\n",
    "    :return: schedule that computes\n",
    "      current learning rate depending on remaining progress\n",
    "    \"\"\"\n",
    "    def func(progress_remaining: float) -> float:\n",
    "        \"\"\"\n",
    "        Progress will decrease from 1 (beginning) to 0.\n",
    "\n",
    "        :param progress_remaining:\n",
    "        :return: current learning rate\n",
    "        \"\"\"\n",
    "        return progress_remaining * initial_value\n",
    "\n",
    "    return func\n",
    "policy_kwargs = dict(activation_fn=th.nn.LeakyReLU,\n",
    "                     net_arch=dict(pi=[512,512,256,128,64,64,64,64,36,18], vf=[512,512,256,128,64,64,64,64,36,18], optimizers_class = th.optim.Adam, log_std_init = 0.0005)) #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Eval num_timesteps=20160, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0911      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 20160        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031768065 |\n",
      "|    clip_fraction        | 0.0221       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.8         |\n",
      "|    explained_variance   | -0.769       |\n",
      "|    learning_rate        | 0.00498      |\n",
      "|    loss                 | -0.000396    |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00109     |\n",
      "|    std                  | 0.98         |\n",
      "|    value_loss           | 0.000395     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.768   |\n",
      "| time/              |          |\n",
      "|    fps             | 630      |\n",
      "|    iterations      | 2        |\n",
      "|    time_elapsed    | 31       |\n",
      "|    total_timesteps | 20160    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=40320, episode_reward=-0.09 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0919     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 40320       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027854668 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.61       |\n",
      "|    explained_variance   | -0.0507     |\n",
      "|    learning_rate        | 0.00495     |\n",
      "|    loss                 | -0.0071     |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00841    |\n",
      "|    std                  | 0.875       |\n",
      "|    value_loss           | 3.59e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.729   |\n",
      "| time/              |          |\n",
      "|    fps             | 593      |\n",
      "|    iterations      | 4        |\n",
      "|    time_elapsed    | 67       |\n",
      "|    total_timesteps | 40320    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=60480, episode_reward=-0.09 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0884     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 60480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016041074 |\n",
      "|    clip_fraction        | 0.0705      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.3        |\n",
      "|    explained_variance   | -0.000601   |\n",
      "|    learning_rate        | 0.00492     |\n",
      "|    loss                 | -0.00625    |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00514    |\n",
      "|    std                  | 0.767       |\n",
      "|    value_loss           | 1.65e-05    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.688   |\n",
      "| time/              |          |\n",
      "|    fps             | 551      |\n",
      "|    iterations      | 6        |\n",
      "|    time_elapsed    | 109      |\n",
      "|    total_timesteps | 60480    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=80640, episode_reward=-0.11 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.11      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 80640      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02268681 |\n",
      "|    clip_fraction        | 0.0886     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.95      |\n",
      "|    explained_variance   | 0.00889    |\n",
      "|    learning_rate        | 0.00488    |\n",
      "|    loss                 | -0.00916   |\n",
      "|    n_updates            | 70         |\n",
      "|    policy_gradient_loss | -0.00618   |\n",
      "|    std                  | 0.65       |\n",
      "|    value_loss           | 7.85e-06   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.656   |\n",
      "| time/              |          |\n",
      "|    fps             | 526      |\n",
      "|    iterations      | 8        |\n",
      "|    time_elapsed    | 153      |\n",
      "|    total_timesteps | 80640    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=100800, episode_reward=-0.11 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.106      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 100800      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014895462 |\n",
      "|    clip_fraction        | 0.0585      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.67       |\n",
      "|    explained_variance   | 0.00558     |\n",
      "|    learning_rate        | 0.00485     |\n",
      "|    loss                 | -0.00246    |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00425    |\n",
      "|    std                  | 0.593       |\n",
      "|    value_loss           | 4.93e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.61    |\n",
      "| time/              |          |\n",
      "|    fps             | 520      |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 193      |\n",
      "|    total_timesteps | 100800   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=120960, episode_reward=-0.08 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0847     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 120960      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020611173 |\n",
      "|    clip_fraction        | 0.107       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.4        |\n",
      "|    explained_variance   | 0.0571      |\n",
      "|    learning_rate        | 0.00482     |\n",
      "|    loss                 | -0.00904    |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.00808    |\n",
      "|    std                  | 0.533       |\n",
      "|    value_loss           | 1.94e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.584   |\n",
      "| time/              |          |\n",
      "|    fps             | 516      |\n",
      "|    iterations      | 12       |\n",
      "|    time_elapsed    | 234      |\n",
      "|    total_timesteps | 120960   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=141120, episode_reward=-0.11 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.107      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 141120      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009375252 |\n",
      "|    clip_fraction        | 0.0414      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.15       |\n",
      "|    explained_variance   | -0.00952    |\n",
      "|    learning_rate        | 0.00478     |\n",
      "|    loss                 | -0.00233    |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.00248    |\n",
      "|    std                  | 0.482       |\n",
      "|    value_loss           | 1.62e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.53    |\n",
      "| time/              |          |\n",
      "|    fps             | 517      |\n",
      "|    iterations      | 14       |\n",
      "|    time_elapsed    | 272      |\n",
      "|    total_timesteps | 141120   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=161280, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0895    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 161280     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02145565 |\n",
      "|    clip_fraction        | 0.0894     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.789     |\n",
      "|    explained_variance   | 0.0227     |\n",
      "|    learning_rate        | 0.00475    |\n",
      "|    loss                 | -0.00431   |\n",
      "|    n_updates            | 150        |\n",
      "|    policy_gradient_loss | -0.00521   |\n",
      "|    std                  | 0.404      |\n",
      "|    value_loss           | 1.65e-06   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.458   |\n",
      "| time/              |          |\n",
      "|    fps             | 502      |\n",
      "|    iterations      | 16       |\n",
      "|    time_elapsed    | 320      |\n",
      "|    total_timesteps | 161280   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=181440, episode_reward=-0.10 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0967    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 181440     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01698615 |\n",
      "|    clip_fraction        | 0.171      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.394     |\n",
      "|    explained_variance   | 0.0614     |\n",
      "|    learning_rate        | 0.00471    |\n",
      "|    loss                 | -0.0176    |\n",
      "|    n_updates            | 170        |\n",
      "|    policy_gradient_loss | -0.0153    |\n",
      "|    std                  | 0.339      |\n",
      "|    value_loss           | 4.02e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.395   |\n",
      "| time/              |          |\n",
      "|    fps             | 508      |\n",
      "|    iterations      | 18       |\n",
      "|    time_elapsed    | 356      |\n",
      "|    total_timesteps | 181440   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=201600, episode_reward=-0.09 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0924     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 201600      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020622214 |\n",
      "|    clip_fraction        | 0.139       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.054      |\n",
      "|    explained_variance   | 0.12        |\n",
      "|    learning_rate        | 0.00468     |\n",
      "|    loss                 | -0.0149     |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.00976    |\n",
      "|    std                  | 0.282       |\n",
      "|    value_loss           | 3.37e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.342   |\n",
      "| time/              |          |\n",
      "|    fps             | 514      |\n",
      "|    iterations      | 20       |\n",
      "|    time_elapsed    | 391      |\n",
      "|    total_timesteps | 201600   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=221760, episode_reward=-0.09 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0914     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 221760      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020774316 |\n",
      "|    clip_fraction        | 0.112       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.322       |\n",
      "|    explained_variance   | 0.105       |\n",
      "|    learning_rate        | 0.00465     |\n",
      "|    loss                 | -0.00665    |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.00809    |\n",
      "|    std                  | 0.23        |\n",
      "|    value_loss           | 2.98e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.297   |\n",
      "| time/              |          |\n",
      "|    fps             | 507      |\n",
      "|    iterations      | 22       |\n",
      "|    time_elapsed    | 437      |\n",
      "|    total_timesteps | 221760   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=241920, episode_reward=-0.10 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.101      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 241920      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019266699 |\n",
      "|    clip_fraction        | 0.137       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.671       |\n",
      "|    explained_variance   | 0.142       |\n",
      "|    learning_rate        | 0.00461     |\n",
      "|    loss                 | -0.0113     |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.00967    |\n",
      "|    std                  | 0.194       |\n",
      "|    value_loss           | 6.48e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.253   |\n",
      "| time/              |          |\n",
      "|    fps             | 508      |\n",
      "|    iterations      | 24       |\n",
      "|    time_elapsed    | 475      |\n",
      "|    total_timesteps | 241920   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=262080, episode_reward=-0.10 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0999      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 262080       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071927654 |\n",
      "|    clip_fraction        | 0.0565       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 0.9          |\n",
      "|    explained_variance   | 0.0255       |\n",
      "|    learning_rate        | 0.00458      |\n",
      "|    loss                 | -0.00429     |\n",
      "|    n_updates            | 250          |\n",
      "|    policy_gradient_loss | -0.0029      |\n",
      "|    std                  | 0.174        |\n",
      "|    value_loss           | 2.48e-06     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.214   |\n",
      "| time/              |          |\n",
      "|    fps             | 511      |\n",
      "|    iterations      | 26       |\n",
      "|    time_elapsed    | 512      |\n",
      "|    total_timesteps | 262080   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=282240, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0927     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 282240      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011223039 |\n",
      "|    clip_fraction        | 0.0642      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.19        |\n",
      "|    explained_variance   | -0.238      |\n",
      "|    learning_rate        | 0.00455     |\n",
      "|    loss                 | -0.00534    |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.00523    |\n",
      "|    std                  | 0.15        |\n",
      "|    value_loss           | 8.22e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.187   |\n",
      "| time/              |          |\n",
      "|    fps             | 511      |\n",
      "|    iterations      | 28       |\n",
      "|    time_elapsed    | 552      |\n",
      "|    total_timesteps | 282240   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=302400, episode_reward=-0.11 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.111      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 302400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009319744 |\n",
      "|    clip_fraction        | 0.0711      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.47        |\n",
      "|    explained_variance   | 0.0421      |\n",
      "|    learning_rate        | 0.00451     |\n",
      "|    loss                 | -0.00484    |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.00284    |\n",
      "|    std                  | 0.131       |\n",
      "|    value_loss           | 9.99e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.167   |\n",
      "| time/              |          |\n",
      "|    fps             | 515      |\n",
      "|    iterations      | 30       |\n",
      "|    time_elapsed    | 586      |\n",
      "|    total_timesteps | 302400   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=322560, episode_reward=-0.07 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0718    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 322560     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01563487 |\n",
      "|    clip_fraction        | 0.0623     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 1.75       |\n",
      "|    explained_variance   | 0.0612     |\n",
      "|    learning_rate        | 0.00448    |\n",
      "|    loss                 | -0.000971  |\n",
      "|    n_updates            | 310        |\n",
      "|    policy_gradient_loss | -0.00466   |\n",
      "|    std                  | 0.113      |\n",
      "|    value_loss           | 1.72e-07   |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.151   |\n",
      "| time/              |          |\n",
      "|    fps             | 513      |\n",
      "|    iterations      | 32       |\n",
      "|    time_elapsed    | 628      |\n",
      "|    total_timesteps | 322560   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=342720, episode_reward=-0.12 +/- 0.07\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.121       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 342720       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061385296 |\n",
      "|    clip_fraction        | 0.0406       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 1.99         |\n",
      "|    explained_variance   | 0.159        |\n",
      "|    learning_rate        | 0.00445      |\n",
      "|    loss                 | -0.00419     |\n",
      "|    n_updates            | 330          |\n",
      "|    policy_gradient_loss | -0.0029      |\n",
      "|    std                  | 0.104        |\n",
      "|    value_loss           | 8.12e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.142   |\n",
      "| time/              |          |\n",
      "|    fps             | 512      |\n",
      "|    iterations      | 34       |\n",
      "|    time_elapsed    | 669      |\n",
      "|    total_timesteps | 342720   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=362880, episode_reward=-0.07 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0712      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 362880       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063715586 |\n",
      "|    clip_fraction        | 0.0522       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 2.2          |\n",
      "|    explained_variance   | -0.0525      |\n",
      "|    learning_rate        | 0.00441      |\n",
      "|    loss                 | -0.00397     |\n",
      "|    n_updates            | 350          |\n",
      "|    policy_gradient_loss | -0.00233     |\n",
      "|    std                  | 0.0936       |\n",
      "|    value_loss           | 2.05e-05     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.131   |\n",
      "| time/              |          |\n",
      "|    fps             | 513      |\n",
      "|    iterations      | 36       |\n",
      "|    time_elapsed    | 706      |\n",
      "|    total_timesteps | 362880   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=383040, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0754     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 383040      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004102745 |\n",
      "|    clip_fraction        | 0.0232      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.36        |\n",
      "|    explained_variance   | 0.0269      |\n",
      "|    learning_rate        | 0.00438     |\n",
      "|    loss                 | -0.000321   |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.00122    |\n",
      "|    std                  | 0.0855      |\n",
      "|    value_loss           | 1.11e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.124   |\n",
      "| time/              |          |\n",
      "|    fps             | 515      |\n",
      "|    iterations      | 38       |\n",
      "|    time_elapsed    | 743      |\n",
      "|    total_timesteps | 383040   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=403200, episode_reward=-0.11 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.112       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 403200       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057995925 |\n",
      "|    clip_fraction        | 0.0314       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 2.54         |\n",
      "|    explained_variance   | 0.0868       |\n",
      "|    learning_rate        | 0.00434      |\n",
      "|    loss                 | -0.00192     |\n",
      "|    n_updates            | 390          |\n",
      "|    policy_gradient_loss | -0.00152     |\n",
      "|    std                  | 0.0783       |\n",
      "|    value_loss           | 9.56e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.118   |\n",
      "| time/              |          |\n",
      "|    fps             | 515      |\n",
      "|    iterations      | 40       |\n",
      "|    time_elapsed    | 782      |\n",
      "|    total_timesteps | 403200   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=423360, episode_reward=-0.09 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0896     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 423360      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008869423 |\n",
      "|    clip_fraction        | 0.0513      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.69        |\n",
      "|    explained_variance   | 0.0446      |\n",
      "|    learning_rate        | 0.00431     |\n",
      "|    loss                 | 0.00734     |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.0013     |\n",
      "|    std                  | 0.0735      |\n",
      "|    value_loss           | 2.06e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.114   |\n",
      "| time/              |          |\n",
      "|    fps             | 517      |\n",
      "|    iterations      | 42       |\n",
      "|    time_elapsed    | 817      |\n",
      "|    total_timesteps | 423360   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=443520, episode_reward=-0.10 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0996      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 443520       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009771108 |\n",
      "|    clip_fraction        | 0.0163       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 2.8          |\n",
      "|    explained_variance   | 0.162        |\n",
      "|    learning_rate        | 0.00428      |\n",
      "|    loss                 | -0.000196    |\n",
      "|    n_updates            | 430          |\n",
      "|    policy_gradient_loss | -0.000275    |\n",
      "|    std                  | 0.07         |\n",
      "|    value_loss           | 1.08e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.112   |\n",
      "| time/              |          |\n",
      "|    fps             | 522      |\n",
      "|    iterations      | 44       |\n",
      "|    time_elapsed    | 848      |\n",
      "|    total_timesteps | 443520   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=463680, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0889      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 463680       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034463855 |\n",
      "|    clip_fraction        | 0.0339       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 2.88         |\n",
      "|    explained_variance   | 0.0759       |\n",
      "|    learning_rate        | 0.00424      |\n",
      "|    loss                 | -0.000967    |\n",
      "|    n_updates            | 450          |\n",
      "|    policy_gradient_loss | -0.000884    |\n",
      "|    std                  | 0.0665       |\n",
      "|    value_loss           | 2.04e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.107   |\n",
      "| time/              |          |\n",
      "|    fps             | 527      |\n",
      "|    iterations      | 46       |\n",
      "|    time_elapsed    | 879      |\n",
      "|    total_timesteps | 463680   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=483840, episode_reward=-0.10 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.103       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 483840       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034847483 |\n",
      "|    clip_fraction        | 0.0322       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 2.96         |\n",
      "|    explained_variance   | 0.0727       |\n",
      "|    learning_rate        | 0.00421      |\n",
      "|    loss                 | -0.00153     |\n",
      "|    n_updates            | 470          |\n",
      "|    policy_gradient_loss | 9.32e-05     |\n",
      "|    std                  | 0.065        |\n",
      "|    value_loss           | 3.65e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.106   |\n",
      "| time/              |          |\n",
      "|    fps             | 531      |\n",
      "|    iterations      | 48       |\n",
      "|    time_elapsed    | 910      |\n",
      "|    total_timesteps | 483840   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=504000, episode_reward=-0.08 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.076       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 504000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053082737 |\n",
      "|    clip_fraction        | 0.0241       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 3.05         |\n",
      "|    explained_variance   | 0.105        |\n",
      "|    learning_rate        | 0.00418      |\n",
      "|    loss                 | -0.00198     |\n",
      "|    n_updates            | 490          |\n",
      "|    policy_gradient_loss | -0.000331    |\n",
      "|    std                  | 0.0616       |\n",
      "|    value_loss           | 6.87e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.104   |\n",
      "| time/              |          |\n",
      "|    fps             | 535      |\n",
      "|    iterations      | 50       |\n",
      "|    time_elapsed    | 941      |\n",
      "|    total_timesteps | 504000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=524160, episode_reward=-0.08 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0799      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 524160       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029682848 |\n",
      "|    clip_fraction        | 0.0458       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 3.15         |\n",
      "|    explained_variance   | 0.0659       |\n",
      "|    learning_rate        | 0.00414      |\n",
      "|    loss                 | 0.00127      |\n",
      "|    n_updates            | 510          |\n",
      "|    policy_gradient_loss | 0.00124      |\n",
      "|    std                  | 0.0597       |\n",
      "|    value_loss           | 6.4e-08      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.101   |\n",
      "| time/              |          |\n",
      "|    fps             | 538      |\n",
      "|    iterations      | 52       |\n",
      "|    time_elapsed    | 972      |\n",
      "|    total_timesteps | 524160   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=544320, episode_reward=-0.13 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.131       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 544320       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040371353 |\n",
      "|    clip_fraction        | 0.0583       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 3.25         |\n",
      "|    explained_variance   | 0.138        |\n",
      "|    learning_rate        | 0.00411      |\n",
      "|    loss                 | -0.00256     |\n",
      "|    n_updates            | 530          |\n",
      "|    policy_gradient_loss | -0.000355    |\n",
      "|    std                  | 0.0565       |\n",
      "|    value_loss           | 6.17e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.101   |\n",
      "| time/              |          |\n",
      "|    fps             | 542      |\n",
      "|    iterations      | 54       |\n",
      "|    time_elapsed    | 1003     |\n",
      "|    total_timesteps | 544320   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=564480, episode_reward=-0.10 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.101       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 564480       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075983903 |\n",
      "|    clip_fraction        | 0.0874       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 3.32         |\n",
      "|    explained_variance   | 0.136        |\n",
      "|    learning_rate        | 0.00408      |\n",
      "|    loss                 | -0.00216     |\n",
      "|    n_updates            | 550          |\n",
      "|    policy_gradient_loss | -0.00167     |\n",
      "|    std                  | 0.0547       |\n",
      "|    value_loss           | 5.18e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.103   |\n",
      "| time/              |          |\n",
      "|    fps             | 545      |\n",
      "|    iterations      | 56       |\n",
      "|    time_elapsed    | 1034     |\n",
      "|    total_timesteps | 564480   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=584640, episode_reward=-0.07 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0732      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 584640       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033073986 |\n",
      "|    clip_fraction        | 0.018        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 3.38         |\n",
      "|    explained_variance   | 0.137        |\n",
      "|    learning_rate        | 0.00404      |\n",
      "|    loss                 | 0.000456     |\n",
      "|    n_updates            | 570          |\n",
      "|    policy_gradient_loss | 0.000195     |\n",
      "|    std                  | 0.0532       |\n",
      "|    value_loss           | 3.41e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.103   |\n",
      "| time/              |          |\n",
      "|    fps             | 548      |\n",
      "|    iterations      | 58       |\n",
      "|    time_elapsed    | 1065     |\n",
      "|    total_timesteps | 584640   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=604800, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0926     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 604800      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009326415 |\n",
      "|    clip_fraction        | 0.0928      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.48        |\n",
      "|    explained_variance   | 0.0781      |\n",
      "|    learning_rate        | 0.00401     |\n",
      "|    loss                 | -0.0015     |\n",
      "|    n_updates            | 590         |\n",
      "|    policy_gradient_loss | 0.00105     |\n",
      "|    std                  | 0.0499      |\n",
      "|    value_loss           | 5.97e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0984  |\n",
      "| time/              |          |\n",
      "|    fps             | 551      |\n",
      "|    iterations      | 60       |\n",
      "|    time_elapsed    | 1097     |\n",
      "|    total_timesteps | 604800   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=624960, episode_reward=-0.10 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.101       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 624960       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042847283 |\n",
      "|    clip_fraction        | 0.0443       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 3.6          |\n",
      "|    explained_variance   | 0.133        |\n",
      "|    learning_rate        | 0.00398      |\n",
      "|    loss                 | -0.000632    |\n",
      "|    n_updates            | 610          |\n",
      "|    policy_gradient_loss | -0.000329    |\n",
      "|    std                  | 0.047        |\n",
      "|    value_loss           | 9.76e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0953  |\n",
      "| time/              |          |\n",
      "|    fps             | 553      |\n",
      "|    iterations      | 62       |\n",
      "|    time_elapsed    | 1128     |\n",
      "|    total_timesteps | 624960   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=645120, episode_reward=-0.10 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.102      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 645120      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003784929 |\n",
      "|    clip_fraction        | 0.042       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.72        |\n",
      "|    explained_variance   | 0.15        |\n",
      "|    learning_rate        | 0.00394     |\n",
      "|    loss                 | -0.00188    |\n",
      "|    n_updates            | 630         |\n",
      "|    policy_gradient_loss | -0.000526   |\n",
      "|    std                  | 0.0445      |\n",
      "|    value_loss           | 1.54e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0928  |\n",
      "| time/              |          |\n",
      "|    fps             | 556      |\n",
      "|    iterations      | 64       |\n",
      "|    time_elapsed    | 1159     |\n",
      "|    total_timesteps | 645120   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=665280, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0898      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 665280       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031435466 |\n",
      "|    clip_fraction        | 0.0275       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 3.84         |\n",
      "|    explained_variance   | 0.091        |\n",
      "|    learning_rate        | 0.00391      |\n",
      "|    loss                 | 0.00166      |\n",
      "|    n_updates            | 650          |\n",
      "|    policy_gradient_loss | -0.000554    |\n",
      "|    std                  | 0.0426       |\n",
      "|    value_loss           | 2.13e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0916  |\n",
      "| time/              |          |\n",
      "|    fps             | 558      |\n",
      "|    iterations      | 66       |\n",
      "|    time_elapsed    | 1190     |\n",
      "|    total_timesteps | 665280   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=685440, episode_reward=-0.09 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.095       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 685440       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023470467 |\n",
      "|    clip_fraction        | 0.0685       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 3.91         |\n",
      "|    explained_variance   | 0.0948       |\n",
      "|    learning_rate        | 0.00387      |\n",
      "|    loss                 | -0.00168     |\n",
      "|    n_updates            | 670          |\n",
      "|    policy_gradient_loss | 0.000765     |\n",
      "|    std                  | 0.0415       |\n",
      "|    value_loss           | 1.28e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0891  |\n",
      "| time/              |          |\n",
      "|    fps             | 561      |\n",
      "|    iterations      | 68       |\n",
      "|    time_elapsed    | 1221     |\n",
      "|    total_timesteps | 685440   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=705600, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.087      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 705600      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007255031 |\n",
      "|    clip_fraction        | 0.0402      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.96        |\n",
      "|    explained_variance   | 0.299       |\n",
      "|    learning_rate        | 0.00384     |\n",
      "|    loss                 | -0.00134    |\n",
      "|    n_updates            | 690         |\n",
      "|    policy_gradient_loss | -0.00071    |\n",
      "|    std                  | 0.0403      |\n",
      "|    value_loss           | 2.22e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0881  |\n",
      "| time/              |          |\n",
      "|    fps             | 562      |\n",
      "|    iterations      | 70       |\n",
      "|    time_elapsed    | 1253     |\n",
      "|    total_timesteps | 705600   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=725760, episode_reward=-0.11 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.115       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 725760       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024178717 |\n",
      "|    clip_fraction        | 0.0143       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 3.99         |\n",
      "|    explained_variance   | 0.308        |\n",
      "|    learning_rate        | 0.00381      |\n",
      "|    loss                 | -0.00264     |\n",
      "|    n_updates            | 710          |\n",
      "|    policy_gradient_loss | -0.00016     |\n",
      "|    std                  | 0.0399       |\n",
      "|    value_loss           | 8.36e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0885  |\n",
      "| time/              |          |\n",
      "|    fps             | 564      |\n",
      "|    iterations      | 72       |\n",
      "|    time_elapsed    | 1285     |\n",
      "|    total_timesteps | 725760   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=745920, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0848      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 745920       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039331284 |\n",
      "|    clip_fraction        | 0.0169       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.08         |\n",
      "|    explained_variance   | 0.00969      |\n",
      "|    learning_rate        | 0.00377      |\n",
      "|    loss                 | -0.00082     |\n",
      "|    n_updates            | 730          |\n",
      "|    policy_gradient_loss | -0.000135    |\n",
      "|    std                  | 0.0379       |\n",
      "|    value_loss           | 2.18e-06     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0877  |\n",
      "| time/              |          |\n",
      "|    fps             | 566      |\n",
      "|    iterations      | 74       |\n",
      "|    time_elapsed    | 1316     |\n",
      "|    total_timesteps | 745920   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=766080, episode_reward=-0.08 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0764     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 766080      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005545494 |\n",
      "|    clip_fraction        | 0.0938      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.13        |\n",
      "|    explained_variance   | 0.0294      |\n",
      "|    learning_rate        | 0.00374     |\n",
      "|    loss                 | 0.00271     |\n",
      "|    n_updates            | 750         |\n",
      "|    policy_gradient_loss | 0.00228     |\n",
      "|    std                  | 0.0376      |\n",
      "|    value_loss           | 1.5e-05     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0905  |\n",
      "| time/              |          |\n",
      "|    fps             | 568      |\n",
      "|    iterations      | 76       |\n",
      "|    time_elapsed    | 1347     |\n",
      "|    total_timesteps | 766080   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=786240, episode_reward=-0.11 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.111       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 786240       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028353075 |\n",
      "|    clip_fraction        | 0.0337       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.15         |\n",
      "|    explained_variance   | 0.0252       |\n",
      "|    learning_rate        | 0.00371      |\n",
      "|    loss                 | -0.000886    |\n",
      "|    n_updates            | 770          |\n",
      "|    policy_gradient_loss | -0.000148    |\n",
      "|    std                  | 0.0367       |\n",
      "|    value_loss           | 2.53e-06     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0953  |\n",
      "| time/              |          |\n",
      "|    fps             | 570      |\n",
      "|    iterations      | 78       |\n",
      "|    time_elapsed    | 1379     |\n",
      "|    total_timesteps | 786240   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=806400, episode_reward=-0.08 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0802     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 806400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025843268 |\n",
      "|    clip_fraction        | 0.172       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.2         |\n",
      "|    explained_variance   | 0.0465      |\n",
      "|    learning_rate        | 0.00367     |\n",
      "|    loss                 | 0.00284     |\n",
      "|    n_updates            | 790         |\n",
      "|    policy_gradient_loss | 0.00692     |\n",
      "|    std                  | 0.0357      |\n",
      "|    value_loss           | 1.16e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0979  |\n",
      "| time/              |          |\n",
      "|    fps             | 571      |\n",
      "|    iterations      | 80       |\n",
      "|    time_elapsed    | 1410     |\n",
      "|    total_timesteps | 806400   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=826560, episode_reward=-0.10 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.102       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 826560       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031236536 |\n",
      "|    clip_fraction        | 0.0825       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.29         |\n",
      "|    explained_variance   | -0.269       |\n",
      "|    learning_rate        | 0.00364      |\n",
      "|    loss                 | -0.00274     |\n",
      "|    n_updates            | 810          |\n",
      "|    policy_gradient_loss | -0.00101     |\n",
      "|    std                  | 0.0337       |\n",
      "|    value_loss           | 4.75e-06     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 573      |\n",
      "|    iterations      | 82       |\n",
      "|    time_elapsed    | 1441     |\n",
      "|    total_timesteps | 826560   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=846720, episode_reward=-0.07 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.071      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 846720      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013831722 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.34        |\n",
      "|    explained_variance   | 0.128       |\n",
      "|    learning_rate        | 0.00361     |\n",
      "|    loss                 | 0.0226      |\n",
      "|    n_updates            | 830         |\n",
      "|    policy_gradient_loss | 0.000952    |\n",
      "|    std                  | 0.0332      |\n",
      "|    value_loss           | 5.66e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0997  |\n",
      "| time/              |          |\n",
      "|    fps             | 574      |\n",
      "|    iterations      | 84       |\n",
      "|    time_elapsed    | 1473     |\n",
      "|    total_timesteps | 846720   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=866880, episode_reward=-0.10 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0959     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 866880      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005598051 |\n",
      "|    clip_fraction        | 0.0794      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.39        |\n",
      "|    explained_variance   | -0.0578     |\n",
      "|    learning_rate        | 0.00357     |\n",
      "|    loss                 | 0.00763     |\n",
      "|    n_updates            | 850         |\n",
      "|    policy_gradient_loss | 0.00161     |\n",
      "|    std                  | 0.032       |\n",
      "|    value_loss           | 2.51e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.104   |\n",
      "| time/              |          |\n",
      "|    fps             | 576      |\n",
      "|    iterations      | 86       |\n",
      "|    time_elapsed    | 1504     |\n",
      "|    total_timesteps | 866880   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=887040, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0851      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 887040       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044295536 |\n",
      "|    clip_fraction        | 0.0306       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.44         |\n",
      "|    explained_variance   | 0.0394       |\n",
      "|    learning_rate        | 0.00354      |\n",
      "|    loss                 | -0.000732    |\n",
      "|    n_updates            | 870          |\n",
      "|    policy_gradient_loss | -6.65e-05    |\n",
      "|    std                  | 0.0314       |\n",
      "|    value_loss           | 5.48e-06     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.103   |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 88       |\n",
      "|    time_elapsed    | 1535     |\n",
      "|    total_timesteps | 887040   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=907200, episode_reward=-0.10 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0997      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 907200       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026776663 |\n",
      "|    clip_fraction        | 0.0263       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.46         |\n",
      "|    explained_variance   | -0.0102      |\n",
      "|    learning_rate        | 0.0035       |\n",
      "|    loss                 | 0.000352     |\n",
      "|    n_updates            | 890          |\n",
      "|    policy_gradient_loss | 0.000153     |\n",
      "|    std                  | 0.0308       |\n",
      "|    value_loss           | 2.09e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0989  |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 90       |\n",
      "|    time_elapsed    | 1566     |\n",
      "|    total_timesteps | 907200   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=927360, episode_reward=-0.09 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0866     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 927360      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008421266 |\n",
      "|    clip_fraction        | 0.0758      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.5         |\n",
      "|    explained_variance   | 0.0394      |\n",
      "|    learning_rate        | 0.00347     |\n",
      "|    loss                 | 0.00254     |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | -0.000614   |\n",
      "|    std                  | 0.0302      |\n",
      "|    value_loss           | 4.84e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0948  |\n",
      "| time/              |          |\n",
      "|    fps             | 580      |\n",
      "|    iterations      | 92       |\n",
      "|    time_elapsed    | 1597     |\n",
      "|    total_timesteps | 927360   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=947520, episode_reward=-0.08 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0762     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 947520      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007518833 |\n",
      "|    clip_fraction        | 0.0633      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.49        |\n",
      "|    explained_variance   | 0.00689     |\n",
      "|    learning_rate        | 0.00344     |\n",
      "|    loss                 | 0.00141     |\n",
      "|    n_updates            | 930         |\n",
      "|    policy_gradient_loss | -0.000583   |\n",
      "|    std                  | 0.0309      |\n",
      "|    value_loss           | 1.74e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0916  |\n",
      "| time/              |          |\n",
      "|    fps             | 581      |\n",
      "|    iterations      | 94       |\n",
      "|    time_elapsed    | 1628     |\n",
      "|    total_timesteps | 947520   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=967680, episode_reward=-0.10 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0958     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 967680      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004365496 |\n",
      "|    clip_fraction        | 0.0269      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.48        |\n",
      "|    explained_variance   | 0.0325      |\n",
      "|    learning_rate        | 0.0034      |\n",
      "|    loss                 | -0.000266   |\n",
      "|    n_updates            | 950         |\n",
      "|    policy_gradient_loss | 9.02e-05    |\n",
      "|    std                  | 0.0308      |\n",
      "|    value_loss           | 1.99e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0883  |\n",
      "| time/              |          |\n",
      "|    fps             | 583      |\n",
      "|    iterations      | 96       |\n",
      "|    time_elapsed    | 1659     |\n",
      "|    total_timesteps | 967680   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=987840, episode_reward=-0.08 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0784     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 987840      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004482058 |\n",
      "|    clip_fraction        | 0.0229      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.49        |\n",
      "|    explained_variance   | 0.08        |\n",
      "|    learning_rate        | 0.00337     |\n",
      "|    loss                 | -0.000342   |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | 0.000117    |\n",
      "|    std                  | 0.0304      |\n",
      "|    value_loss           | 8.08e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0872  |\n",
      "| time/              |          |\n",
      "|    fps             | 584      |\n",
      "|    iterations      | 98       |\n",
      "|    time_elapsed    | 1690     |\n",
      "|    total_timesteps | 987840   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1008000, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0792      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1008000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036662314 |\n",
      "|    clip_fraction        | 0.0357       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.53         |\n",
      "|    explained_variance   | 0.0978       |\n",
      "|    learning_rate        | 0.00334      |\n",
      "|    loss                 | 0.00166      |\n",
      "|    n_updates            | 990          |\n",
      "|    policy_gradient_loss | 0.000389     |\n",
      "|    std                  | 0.0301       |\n",
      "|    value_loss           | 2.67e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0874  |\n",
      "| time/              |          |\n",
      "|    fps             | 585      |\n",
      "|    iterations      | 100      |\n",
      "|    time_elapsed    | 1721     |\n",
      "|    total_timesteps | 1008000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1028160, episode_reward=-0.10 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0967      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1028160      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036139963 |\n",
      "|    clip_fraction        | 0.088        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.56         |\n",
      "|    explained_variance   | 0.0807       |\n",
      "|    learning_rate        | 0.0033       |\n",
      "|    loss                 | -0.00226     |\n",
      "|    n_updates            | 1010         |\n",
      "|    policy_gradient_loss | 0.00184      |\n",
      "|    std                  | 0.0293       |\n",
      "|    value_loss           | 1.04e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0894  |\n",
      "| time/              |          |\n",
      "|    fps             | 586      |\n",
      "|    iterations      | 102      |\n",
      "|    time_elapsed    | 1752     |\n",
      "|    total_timesteps | 1028160  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1048320, episode_reward=-0.08 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0765     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1048320     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006758038 |\n",
      "|    clip_fraction        | 0.0872      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.57        |\n",
      "|    explained_variance   | 0.0338      |\n",
      "|    learning_rate        | 0.00327     |\n",
      "|    loss                 | 0.0073      |\n",
      "|    n_updates            | 1030        |\n",
      "|    policy_gradient_loss | 0.00195     |\n",
      "|    std                  | 0.0294      |\n",
      "|    value_loss           | 1.74e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0907  |\n",
      "| time/              |          |\n",
      "|    fps             | 587      |\n",
      "|    iterations      | 104      |\n",
      "|    time_elapsed    | 1784     |\n",
      "|    total_timesteps | 1048320  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1068480, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0901      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1068480      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054408372 |\n",
      "|    clip_fraction        | 0.0385       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.6          |\n",
      "|    explained_variance   | 0.0463       |\n",
      "|    learning_rate        | 0.00324      |\n",
      "|    loss                 | 0.00257      |\n",
      "|    n_updates            | 1050         |\n",
      "|    policy_gradient_loss | 0.000629     |\n",
      "|    std                  | 0.0289       |\n",
      "|    value_loss           | 1.63e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0899  |\n",
      "| time/              |          |\n",
      "|    fps             | 588      |\n",
      "|    iterations      | 106      |\n",
      "|    time_elapsed    | 1815     |\n",
      "|    total_timesteps | 1068480  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1088640, episode_reward=-0.11 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.109       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1088640      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018358676 |\n",
      "|    clip_fraction        | 0.0507       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.63         |\n",
      "|    explained_variance   | 0.15         |\n",
      "|    learning_rate        | 0.0032       |\n",
      "|    loss                 | 0.000188     |\n",
      "|    n_updates            | 1070         |\n",
      "|    policy_gradient_loss | 0.00072      |\n",
      "|    std                  | 0.0281       |\n",
      "|    value_loss           | 7.68e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0917  |\n",
      "| time/              |          |\n",
      "|    fps             | 589      |\n",
      "|    iterations      | 108      |\n",
      "|    time_elapsed    | 1846     |\n",
      "|    total_timesteps | 1088640  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1108800, episode_reward=-0.08 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0755     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1108800     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012822645 |\n",
      "|    clip_fraction        | 0.0874      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.65        |\n",
      "|    explained_variance   | -0.266      |\n",
      "|    learning_rate        | 0.00317     |\n",
      "|    loss                 | 0.00377     |\n",
      "|    n_updates            | 1090        |\n",
      "|    policy_gradient_loss | 0.00238     |\n",
      "|    std                  | 0.0278      |\n",
      "|    value_loss           | 9.59e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0888  |\n",
      "| time/              |          |\n",
      "|    fps             | 590      |\n",
      "|    iterations      | 110      |\n",
      "|    time_elapsed    | 1877     |\n",
      "|    total_timesteps | 1108800  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1128960, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0846     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1128960     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016896216 |\n",
      "|    clip_fraction        | 0.112       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.72        |\n",
      "|    explained_variance   | 0.0472      |\n",
      "|    learning_rate        | 0.00314     |\n",
      "|    loss                 | 0.00689     |\n",
      "|    n_updates            | 1110        |\n",
      "|    policy_gradient_loss | 0.00333     |\n",
      "|    std                  | 0.0273      |\n",
      "|    value_loss           | 1.13e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0894  |\n",
      "| time/              |          |\n",
      "|    fps             | 591      |\n",
      "|    iterations      | 112      |\n",
      "|    time_elapsed    | 1907     |\n",
      "|    total_timesteps | 1128960  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1149120, episode_reward=-0.12 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.12        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1149120      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021662014 |\n",
      "|    clip_fraction        | 0.063        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.67         |\n",
      "|    explained_variance   | 0.106        |\n",
      "|    learning_rate        | 0.0031       |\n",
      "|    loss                 | 0.000812     |\n",
      "|    n_updates            | 1130         |\n",
      "|    policy_gradient_loss | 0.00106      |\n",
      "|    std                  | 0.028        |\n",
      "|    value_loss           | 3.32e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0885  |\n",
      "| time/              |          |\n",
      "|    fps             | 593      |\n",
      "|    iterations      | 114      |\n",
      "|    time_elapsed    | 1936     |\n",
      "|    total_timesteps | 1149120  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1169280, episode_reward=-0.15 +/- 0.07\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.148      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1169280     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004568893 |\n",
      "|    clip_fraction        | 0.0997      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.65        |\n",
      "|    explained_variance   | 0.0325      |\n",
      "|    learning_rate        | 0.00307     |\n",
      "|    loss                 | 0.000416    |\n",
      "|    n_updates            | 1150        |\n",
      "|    policy_gradient_loss | 0.00323     |\n",
      "|    std                  | 0.0282      |\n",
      "|    value_loss           | 6.15e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0866  |\n",
      "| time/              |          |\n",
      "|    fps             | 594      |\n",
      "|    iterations      | 116      |\n",
      "|    time_elapsed    | 1966     |\n",
      "|    total_timesteps | 1169280  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1189440, episode_reward=-0.09 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0856      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1189440      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032520005 |\n",
      "|    clip_fraction        | 0.0453       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.68         |\n",
      "|    explained_variance   | -0.00198     |\n",
      "|    learning_rate        | 0.00303      |\n",
      "|    loss                 | -0.00164     |\n",
      "|    n_updates            | 1170         |\n",
      "|    policy_gradient_loss | 0.000294     |\n",
      "|    std                  | 0.0279       |\n",
      "|    value_loss           | 1.71e-06     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0865  |\n",
      "| time/              |          |\n",
      "|    fps             | 595      |\n",
      "|    iterations      | 118      |\n",
      "|    time_elapsed    | 1996     |\n",
      "|    total_timesteps | 1189440  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1209600, episode_reward=-0.10 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.104      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1209600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003932908 |\n",
      "|    clip_fraction        | 0.0167      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.73        |\n",
      "|    explained_variance   | 0.00942     |\n",
      "|    learning_rate        | 0.003       |\n",
      "|    loss                 | -0.00208    |\n",
      "|    n_updates            | 1190        |\n",
      "|    policy_gradient_loss | 0.000415    |\n",
      "|    std                  | 0.0275      |\n",
      "|    value_loss           | 7.93e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0899  |\n",
      "| time/              |          |\n",
      "|    fps             | 596      |\n",
      "|    iterations      | 120      |\n",
      "|    time_elapsed    | 2026     |\n",
      "|    total_timesteps | 1209600  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1229760, episode_reward=-0.11 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.112       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1229760      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027084495 |\n",
      "|    clip_fraction        | 0.0177       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.76         |\n",
      "|    explained_variance   | 0.077        |\n",
      "|    learning_rate        | 0.00297      |\n",
      "|    loss                 | 0.00108      |\n",
      "|    n_updates            | 1210         |\n",
      "|    policy_gradient_loss | 0.00042      |\n",
      "|    std                  | 0.0271       |\n",
      "|    value_loss           | 1.34e-06     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0921  |\n",
      "| time/              |          |\n",
      "|    fps             | 598      |\n",
      "|    iterations      | 122      |\n",
      "|    time_elapsed    | 2056     |\n",
      "|    total_timesteps | 1229760  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1249920, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0629     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1249920     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003193387 |\n",
      "|    clip_fraction        | 0.0468      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.78        |\n",
      "|    explained_variance   | 0.0199      |\n",
      "|    learning_rate        | 0.00293     |\n",
      "|    loss                 | -0.00119    |\n",
      "|    n_updates            | 1230        |\n",
      "|    policy_gradient_loss | 0.000331    |\n",
      "|    std                  | 0.0266      |\n",
      "|    value_loss           | 3.15e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0929  |\n",
      "| time/              |          |\n",
      "|    fps             | 599      |\n",
      "|    iterations      | 124      |\n",
      "|    time_elapsed    | 2086     |\n",
      "|    total_timesteps | 1249920  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1270080, episode_reward=-0.08 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.076       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1270080      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029667523 |\n",
      "|    clip_fraction        | 0.0331       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.81         |\n",
      "|    explained_variance   | 0.00643      |\n",
      "|    learning_rate        | 0.0029       |\n",
      "|    loss                 | 0.000555     |\n",
      "|    n_updates            | 1250         |\n",
      "|    policy_gradient_loss | 2.11e-05     |\n",
      "|    std                  | 0.0261       |\n",
      "|    value_loss           | 2.56e-06     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0914  |\n",
      "| time/              |          |\n",
      "|    fps             | 600      |\n",
      "|    iterations      | 126      |\n",
      "|    time_elapsed    | 2116     |\n",
      "|    total_timesteps | 1270080  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1290240, episode_reward=-0.09 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0852     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1290240     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003315277 |\n",
      "|    clip_fraction        | 0.0825      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.85        |\n",
      "|    explained_variance   | 0.00565     |\n",
      "|    learning_rate        | 0.00287     |\n",
      "|    loss                 | 0.000146    |\n",
      "|    n_updates            | 1270        |\n",
      "|    policy_gradient_loss | 0.00101     |\n",
      "|    std                  | 0.0256      |\n",
      "|    value_loss           | 3.46e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0942  |\n",
      "| time/              |          |\n",
      "|    fps             | 601      |\n",
      "|    iterations      | 128      |\n",
      "|    time_elapsed    | 2146     |\n",
      "|    total_timesteps | 1290240  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1310400, episode_reward=-0.10 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0958     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1310400     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002606721 |\n",
      "|    clip_fraction        | 0.0243      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.94        |\n",
      "|    explained_variance   | -0.0235     |\n",
      "|    learning_rate        | 0.00283     |\n",
      "|    loss                 | 0.000112    |\n",
      "|    n_updates            | 1290        |\n",
      "|    policy_gradient_loss | -0.000173   |\n",
      "|    std                  | 0.0243      |\n",
      "|    value_loss           | 2.74e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0964  |\n",
      "| time/              |          |\n",
      "|    fps             | 602      |\n",
      "|    iterations      | 130      |\n",
      "|    time_elapsed    | 2175     |\n",
      "|    total_timesteps | 1310400  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1330560, episode_reward=-0.10 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0952      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1330560      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059071784 |\n",
      "|    clip_fraction        | 0.0247       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.96         |\n",
      "|    explained_variance   | 0.0747       |\n",
      "|    learning_rate        | 0.0028       |\n",
      "|    loss                 | 0.00127      |\n",
      "|    n_updates            | 1310         |\n",
      "|    policy_gradient_loss | 2.12e-05     |\n",
      "|    std                  | 0.0239       |\n",
      "|    value_loss           | 6.55e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0922  |\n",
      "| time/              |          |\n",
      "|    fps             | 603      |\n",
      "|    iterations      | 132      |\n",
      "|    time_elapsed    | 2205     |\n",
      "|    total_timesteps | 1330560  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1350720, episode_reward=-0.11 +/- 0.07\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.107       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1350720      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043807165 |\n",
      "|    clip_fraction        | 0.0433       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.99         |\n",
      "|    explained_variance   | 0.0715       |\n",
      "|    learning_rate        | 0.00277      |\n",
      "|    loss                 | -0.00113     |\n",
      "|    n_updates            | 1330         |\n",
      "|    policy_gradient_loss | -0.000427    |\n",
      "|    std                  | 0.0238       |\n",
      "|    value_loss           | 7.35e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0912  |\n",
      "| time/              |          |\n",
      "|    fps             | 604      |\n",
      "|    iterations      | 134      |\n",
      "|    time_elapsed    | 2235     |\n",
      "|    total_timesteps | 1350720  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1370880, episode_reward=-0.07 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0746      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1370880      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042680567 |\n",
      "|    clip_fraction        | 0.0538       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.01         |\n",
      "|    explained_variance   | 0.152        |\n",
      "|    learning_rate        | 0.00273      |\n",
      "|    loss                 | -0.000643    |\n",
      "|    n_updates            | 1350         |\n",
      "|    policy_gradient_loss | 0.000741     |\n",
      "|    std                  | 0.0233       |\n",
      "|    value_loss           | 6.41e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0904  |\n",
      "| time/              |          |\n",
      "|    fps             | 604      |\n",
      "|    iterations      | 136      |\n",
      "|    time_elapsed    | 2266     |\n",
      "|    total_timesteps | 1370880  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1391040, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0852      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1391040      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076114703 |\n",
      "|    clip_fraction        | 0.0434       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.05         |\n",
      "|    explained_variance   | 0.0982       |\n",
      "|    learning_rate        | 0.0027       |\n",
      "|    loss                 | 1.78e-05     |\n",
      "|    n_updates            | 1370         |\n",
      "|    policy_gradient_loss | -0.000331    |\n",
      "|    std                  | 0.0227       |\n",
      "|    value_loss           | 2.61e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0932  |\n",
      "| time/              |          |\n",
      "|    fps             | 605      |\n",
      "|    iterations      | 138      |\n",
      "|    time_elapsed    | 2297     |\n",
      "|    total_timesteps | 1391040  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1411200, episode_reward=-0.11 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.107       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1411200      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071786093 |\n",
      "|    clip_fraction        | 0.0479       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.1          |\n",
      "|    explained_variance   | 0.0594       |\n",
      "|    learning_rate        | 0.00266      |\n",
      "|    loss                 | 0.00636      |\n",
      "|    n_updates            | 1390         |\n",
      "|    policy_gradient_loss | -0.000275    |\n",
      "|    std                  | 0.0223       |\n",
      "|    value_loss           | 2.79e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0898  |\n",
      "| time/              |          |\n",
      "|    fps             | 606      |\n",
      "|    iterations      | 140      |\n",
      "|    time_elapsed    | 2328     |\n",
      "|    total_timesteps | 1411200  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1431360, episode_reward=-0.10 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.102       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1431360      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056612846 |\n",
      "|    clip_fraction        | 0.0255       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.12         |\n",
      "|    explained_variance   | 0.102        |\n",
      "|    learning_rate        | 0.00263      |\n",
      "|    loss                 | 0.00144      |\n",
      "|    n_updates            | 1410         |\n",
      "|    policy_gradient_loss | -0.00013     |\n",
      "|    std                  | 0.0218       |\n",
      "|    value_loss           | 3e-07        |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0879  |\n",
      "| time/              |          |\n",
      "|    fps             | 605      |\n",
      "|    iterations      | 142      |\n",
      "|    time_elapsed    | 2362     |\n",
      "|    total_timesteps | 1431360  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1451520, episode_reward=-0.07 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.069       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1451520      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035157627 |\n",
      "|    clip_fraction        | 0.0336       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.16         |\n",
      "|    explained_variance   | 0.149        |\n",
      "|    learning_rate        | 0.0026       |\n",
      "|    loss                 | -0.00137     |\n",
      "|    n_updates            | 1430         |\n",
      "|    policy_gradient_loss | 0.000205     |\n",
      "|    std                  | 0.0214       |\n",
      "|    value_loss           | 8.62e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0884  |\n",
      "| time/              |          |\n",
      "|    fps             | 605      |\n",
      "|    iterations      | 144      |\n",
      "|    time_elapsed    | 2398     |\n",
      "|    total_timesteps | 1451520  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1471680, episode_reward=-0.11 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.11        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1471680      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022993092 |\n",
      "|    clip_fraction        | 0.0887       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.23         |\n",
      "|    explained_variance   | -0.0295      |\n",
      "|    learning_rate        | 0.00256      |\n",
      "|    loss                 | 0.00274      |\n",
      "|    n_updates            | 1450         |\n",
      "|    policy_gradient_loss | 0.00238      |\n",
      "|    std                  | 0.0207       |\n",
      "|    value_loss           | 1.43e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0895  |\n",
      "| time/              |          |\n",
      "|    fps             | 605      |\n",
      "|    iterations      | 146      |\n",
      "|    time_elapsed    | 2430     |\n",
      "|    total_timesteps | 1471680  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1491840, episode_reward=-0.11 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.11       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1491840     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011676532 |\n",
      "|    clip_fraction        | 0.0717      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.25        |\n",
      "|    explained_variance   | 0.0847      |\n",
      "|    learning_rate        | 0.00253     |\n",
      "|    loss                 | -0.00215    |\n",
      "|    n_updates            | 1470        |\n",
      "|    policy_gradient_loss | 0.00157     |\n",
      "|    std                  | 0.0207      |\n",
      "|    value_loss           | 1.04e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.092   |\n",
      "| time/              |          |\n",
      "|    fps             | 605      |\n",
      "|    iterations      | 148      |\n",
      "|    time_elapsed    | 2465     |\n",
      "|    total_timesteps | 1491840  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1512000, episode_reward=-0.10 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0964      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1512000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041724825 |\n",
      "|    clip_fraction        | 0.0175       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.25         |\n",
      "|    explained_variance   | 0.142        |\n",
      "|    learning_rate        | 0.0025       |\n",
      "|    loss                 | 0.00226      |\n",
      "|    n_updates            | 1490         |\n",
      "|    policy_gradient_loss | 0.000199     |\n",
      "|    std                  | 0.0206       |\n",
      "|    value_loss           | 1.2e-07      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0915  |\n",
      "| time/              |          |\n",
      "|    fps             | 604      |\n",
      "|    iterations      | 150      |\n",
      "|    time_elapsed    | 2499     |\n",
      "|    total_timesteps | 1512000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1532160, episode_reward=-0.11 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.114      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1532160     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004503458 |\n",
      "|    clip_fraction        | 0.0347      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.31        |\n",
      "|    explained_variance   | 0.139       |\n",
      "|    learning_rate        | 0.00246     |\n",
      "|    loss                 | 0.00163     |\n",
      "|    n_updates            | 1510        |\n",
      "|    policy_gradient_loss | -0.000657   |\n",
      "|    std                  | 0.0198      |\n",
      "|    value_loss           | 7.72e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0893  |\n",
      "| time/              |          |\n",
      "|    fps             | 604      |\n",
      "|    iterations      | 152      |\n",
      "|    time_elapsed    | 2536     |\n",
      "|    total_timesteps | 1532160  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1552320, episode_reward=-0.08 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0777     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1552320     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013911547 |\n",
      "|    clip_fraction        | 0.0956      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.36        |\n",
      "|    explained_variance   | 0.0375      |\n",
      "|    learning_rate        | 0.00243     |\n",
      "|    loss                 | -0.00217    |\n",
      "|    n_updates            | 1530        |\n",
      "|    policy_gradient_loss | -0.00178    |\n",
      "|    std                  | 0.0195      |\n",
      "|    value_loss           | 1.19e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0894  |\n",
      "| time/              |          |\n",
      "|    fps             | 604      |\n",
      "|    iterations      | 154      |\n",
      "|    time_elapsed    | 2568     |\n",
      "|    total_timesteps | 1552320  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1572480, episode_reward=-0.10 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.102       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1572480      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019549616 |\n",
      "|    clip_fraction        | 0.036        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.4          |\n",
      "|    explained_variance   | 0.0707       |\n",
      "|    learning_rate        | 0.0024       |\n",
      "|    loss                 | -0.000531    |\n",
      "|    n_updates            | 1550         |\n",
      "|    policy_gradient_loss | 0.000117     |\n",
      "|    std                  | 0.0193       |\n",
      "|    value_loss           | 1.6e-07      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0902  |\n",
      "| time/              |          |\n",
      "|    fps             | 604      |\n",
      "|    iterations      | 156      |\n",
      "|    time_elapsed    | 2601     |\n",
      "|    total_timesteps | 1572480  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1592640, episode_reward=-0.11 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.109       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1592640      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020145134 |\n",
      "|    clip_fraction        | 0.0535       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.44         |\n",
      "|    explained_variance   | 0.152        |\n",
      "|    learning_rate        | 0.00236      |\n",
      "|    loss                 | 0.000652     |\n",
      "|    n_updates            | 1570         |\n",
      "|    policy_gradient_loss | -0.00034     |\n",
      "|    std                  | 0.0192       |\n",
      "|    value_loss           | 7.88e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0913  |\n",
      "| time/              |          |\n",
      "|    fps             | 605      |\n",
      "|    iterations      | 158      |\n",
      "|    time_elapsed    | 2632     |\n",
      "|    total_timesteps | 1592640  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1612800, episode_reward=-0.08 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0809     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1612800     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003739139 |\n",
      "|    clip_fraction        | 0.038       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.45        |\n",
      "|    explained_variance   | 0.101       |\n",
      "|    learning_rate        | 0.00233     |\n",
      "|    loss                 | 0.00421     |\n",
      "|    n_updates            | 1590        |\n",
      "|    policy_gradient_loss | 0.000174    |\n",
      "|    std                  | 0.0194      |\n",
      "|    value_loss           | 2.48e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0926  |\n",
      "| time/              |          |\n",
      "|    fps             | 605      |\n",
      "|    iterations      | 160      |\n",
      "|    time_elapsed    | 2662     |\n",
      "|    total_timesteps | 1612800  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1632960, episode_reward=-0.12 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.117       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1632960      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010109068 |\n",
      "|    clip_fraction        | 0.0504       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.44         |\n",
      "|    explained_variance   | 0.0278       |\n",
      "|    learning_rate        | 0.0023       |\n",
      "|    loss                 | -0.00255     |\n",
      "|    n_updates            | 1610         |\n",
      "|    policy_gradient_loss | -0.00104     |\n",
      "|    std                  | 0.0196       |\n",
      "|    value_loss           | 8.08e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0903  |\n",
      "| time/              |          |\n",
      "|    fps             | 606      |\n",
      "|    iterations      | 162      |\n",
      "|    time_elapsed    | 2693     |\n",
      "|    total_timesteps | 1632960  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1653120, episode_reward=-0.11 +/- 0.07\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.112      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1653120     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012955574 |\n",
      "|    clip_fraction        | 0.0464      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.49        |\n",
      "|    explained_variance   | 0.0625      |\n",
      "|    learning_rate        | 0.00226     |\n",
      "|    loss                 | 0.00385     |\n",
      "|    n_updates            | 1630        |\n",
      "|    policy_gradient_loss | -0.00047    |\n",
      "|    std                  | 0.0189      |\n",
      "|    value_loss           | 1.19e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0924  |\n",
      "| time/              |          |\n",
      "|    fps             | 606      |\n",
      "|    iterations      | 164      |\n",
      "|    time_elapsed    | 2723     |\n",
      "|    total_timesteps | 1653120  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1673280, episode_reward=-0.11 +/- 0.07\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.107       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1673280      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035703606 |\n",
      "|    clip_fraction        | 0.0474       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.48         |\n",
      "|    explained_variance   | 0.182        |\n",
      "|    learning_rate        | 0.00223      |\n",
      "|    loss                 | -0.000805    |\n",
      "|    n_updates            | 1650         |\n",
      "|    policy_gradient_loss | 0.000217     |\n",
      "|    std                  | 0.019        |\n",
      "|    value_loss           | 1.11e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0926  |\n",
      "| time/              |          |\n",
      "|    fps             | 607      |\n",
      "|    iterations      | 166      |\n",
      "|    time_elapsed    | 2754     |\n",
      "|    total_timesteps | 1673280  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1693440, episode_reward=-0.10 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0963     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1693440     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004381259 |\n",
      "|    clip_fraction        | 0.0148      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.49        |\n",
      "|    explained_variance   | 0.0431      |\n",
      "|    learning_rate        | 0.00219     |\n",
      "|    loss                 | 0.0016      |\n",
      "|    n_updates            | 1670        |\n",
      "|    policy_gradient_loss | 0.000306    |\n",
      "|    std                  | 0.019       |\n",
      "|    value_loss           | 1.4e-07     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0937  |\n",
      "| time/              |          |\n",
      "|    fps             | 608      |\n",
      "|    iterations      | 168      |\n",
      "|    time_elapsed    | 2784     |\n",
      "|    total_timesteps | 1693440  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1713600, episode_reward=-0.06 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0626     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1713600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003056899 |\n",
      "|    clip_fraction        | 0.0387      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.48        |\n",
      "|    explained_variance   | 0.169       |\n",
      "|    learning_rate        | 0.00216     |\n",
      "|    loss                 | 0.000914    |\n",
      "|    n_updates            | 1690        |\n",
      "|    policy_gradient_loss | -0.000211   |\n",
      "|    std                  | 0.0193      |\n",
      "|    value_loss           | 8.26e-08    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0948  |\n",
      "| time/              |          |\n",
      "|    fps             | 608      |\n",
      "|    iterations      | 170      |\n",
      "|    time_elapsed    | 2815     |\n",
      "|    total_timesteps | 1713600  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1733760, episode_reward=-0.12 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.122      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1733760     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008294213 |\n",
      "|    clip_fraction        | 0.0516      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.47        |\n",
      "|    explained_variance   | 0.0902      |\n",
      "|    learning_rate        | 0.00213     |\n",
      "|    loss                 | 0.00615     |\n",
      "|    n_updates            | 1710        |\n",
      "|    policy_gradient_loss | 9.78e-05    |\n",
      "|    std                  | 0.0193      |\n",
      "|    value_loss           | 4.21e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0954  |\n",
      "| time/              |          |\n",
      "|    fps             | 609      |\n",
      "|    iterations      | 172      |\n",
      "|    time_elapsed    | 2846     |\n",
      "|    total_timesteps | 1733760  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1753920, episode_reward=-0.10 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0961      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1753920      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041024266 |\n",
      "|    clip_fraction        | 0.0326       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.49         |\n",
      "|    explained_variance   | 0.0483       |\n",
      "|    learning_rate        | 0.00209      |\n",
      "|    loss                 | 0.00293      |\n",
      "|    n_updates            | 1730         |\n",
      "|    policy_gradient_loss | -0.000149    |\n",
      "|    std                  | 0.0192       |\n",
      "|    value_loss           | 4.72e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.099   |\n",
      "| time/              |          |\n",
      "|    fps             | 609      |\n",
      "|    iterations      | 174      |\n",
      "|    time_elapsed    | 2877     |\n",
      "|    total_timesteps | 1753920  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1774080, episode_reward=-0.08 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0849    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1774080    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00622262 |\n",
      "|    clip_fraction        | 0.0814     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 5.5        |\n",
      "|    explained_variance   | 0.0541     |\n",
      "|    learning_rate        | 0.00206    |\n",
      "|    loss                 | -0.00209   |\n",
      "|    n_updates            | 1750       |\n",
      "|    policy_gradient_loss | -3.52e-05  |\n",
      "|    std                  | 0.0191     |\n",
      "|    value_loss           | 1.1e-06    |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.101   |\n",
      "| time/              |          |\n",
      "|    fps             | 610      |\n",
      "|    iterations      | 176      |\n",
      "|    time_elapsed    | 2907     |\n",
      "|    total_timesteps | 1774080  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1794240, episode_reward=-0.10 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.101      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1794240     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004793251 |\n",
      "|    clip_fraction        | 0.0397      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.54        |\n",
      "|    explained_variance   | 0.212       |\n",
      "|    learning_rate        | 0.00203     |\n",
      "|    loss                 | 0.00107     |\n",
      "|    n_updates            | 1770        |\n",
      "|    policy_gradient_loss | -0.00113    |\n",
      "|    std                  | 0.0187      |\n",
      "|    value_loss           | 1.76e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.101   |\n",
      "| time/              |          |\n",
      "|    fps             | 610      |\n",
      "|    iterations      | 178      |\n",
      "|    time_elapsed    | 2938     |\n",
      "|    total_timesteps | 1794240  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1814400, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1.26e+03  |\n",
      "|    mean_reward          | -0.089    |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 1814400   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0246832 |\n",
      "|    clip_fraction        | 0.114     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | 5.59      |\n",
      "|    explained_variance   | 0.0482    |\n",
      "|    learning_rate        | 0.00199   |\n",
      "|    loss                 | 0.0106    |\n",
      "|    n_updates            | 1790      |\n",
      "|    policy_gradient_loss | 0.00194   |\n",
      "|    std                  | 0.0183    |\n",
      "|    value_loss           | 1.12e-05  |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.101   |\n",
      "| time/              |          |\n",
      "|    fps             | 611      |\n",
      "|    iterations      | 180      |\n",
      "|    time_elapsed    | 2968     |\n",
      "|    total_timesteps | 1814400  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1834560, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0894      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1834560      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059626354 |\n",
      "|    clip_fraction        | 0.112        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.62         |\n",
      "|    explained_variance   | -0.149       |\n",
      "|    learning_rate        | 0.00196      |\n",
      "|    loss                 | -0.00161     |\n",
      "|    n_updates            | 1810         |\n",
      "|    policy_gradient_loss | 0.00268      |\n",
      "|    std                  | 0.0179       |\n",
      "|    value_loss           | 1.37e-05     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0994  |\n",
      "| time/              |          |\n",
      "|    fps             | 611      |\n",
      "|    iterations      | 182      |\n",
      "|    time_elapsed    | 2999     |\n",
      "|    total_timesteps | 1834560  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1854720, episode_reward=-0.09 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0878      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1854720      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028387157 |\n",
      "|    clip_fraction        | 0.0247       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.61         |\n",
      "|    explained_variance   | -0.126       |\n",
      "|    learning_rate        | 0.00193      |\n",
      "|    loss                 | -0.00372     |\n",
      "|    n_updates            | 1830         |\n",
      "|    policy_gradient_loss | 0.00038      |\n",
      "|    std                  | 0.0179       |\n",
      "|    value_loss           | 9.58e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0937  |\n",
      "| time/              |          |\n",
      "|    fps             | 611      |\n",
      "|    iterations      | 184      |\n",
      "|    time_elapsed    | 3031     |\n",
      "|    total_timesteps | 1854720  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1874880, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0906    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1874880    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00907632 |\n",
      "|    clip_fraction        | 0.049      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 5.61       |\n",
      "|    explained_variance   | -0.0658    |\n",
      "|    learning_rate        | 0.00189    |\n",
      "|    loss                 | 0.000731   |\n",
      "|    n_updates            | 1850       |\n",
      "|    policy_gradient_loss | 5.45e-05   |\n",
      "|    std                  | 0.0179     |\n",
      "|    value_loss           | 1.36e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0917  |\n",
      "| time/              |          |\n",
      "|    fps             | 611      |\n",
      "|    iterations      | 186      |\n",
      "|    time_elapsed    | 3067     |\n",
      "|    total_timesteps | 1874880  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1895040, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.081      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1895040     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007849555 |\n",
      "|    clip_fraction        | 0.0594      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.63        |\n",
      "|    explained_variance   | -0.00753    |\n",
      "|    learning_rate        | 0.00186     |\n",
      "|    loss                 | 0.00118     |\n",
      "|    n_updates            | 1870        |\n",
      "|    policy_gradient_loss | 0.000311    |\n",
      "|    std                  | 0.0176      |\n",
      "|    value_loss           | 4.88e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0854  |\n",
      "| time/              |          |\n",
      "|    fps             | 610      |\n",
      "|    iterations      | 188      |\n",
      "|    time_elapsed    | 3104     |\n",
      "|    total_timesteps | 1895040  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1915200, episode_reward=-0.09 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0931     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1915200     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005478862 |\n",
      "|    clip_fraction        | 0.0885      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.67        |\n",
      "|    explained_variance   | -0.06       |\n",
      "|    learning_rate        | 0.00182     |\n",
      "|    loss                 | -0.00197    |\n",
      "|    n_updates            | 1890        |\n",
      "|    policy_gradient_loss | 0.000888    |\n",
      "|    std                  | 0.0175      |\n",
      "|    value_loss           | 1.93e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0871  |\n",
      "| time/              |          |\n",
      "|    fps             | 609      |\n",
      "|    iterations      | 190      |\n",
      "|    time_elapsed    | 3141     |\n",
      "|    total_timesteps | 1915200  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1935360, episode_reward=-0.09 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0913     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1935360     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014435338 |\n",
      "|    clip_fraction        | 0.0787      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.65        |\n",
      "|    explained_variance   | -0.0466     |\n",
      "|    learning_rate        | 0.00179     |\n",
      "|    loss                 | 0.0103      |\n",
      "|    n_updates            | 1910        |\n",
      "|    policy_gradient_loss | 0.00177     |\n",
      "|    std                  | 0.0176      |\n",
      "|    value_loss           | 8.29e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.083   |\n",
      "| time/              |          |\n",
      "|    fps             | 609      |\n",
      "|    iterations      | 192      |\n",
      "|    time_elapsed    | 3177     |\n",
      "|    total_timesteps | 1935360  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1955520, episode_reward=-0.08 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0771      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1955520      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044935024 |\n",
      "|    clip_fraction        | 0.0256       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.69         |\n",
      "|    explained_variance   | -0.0364      |\n",
      "|    learning_rate        | 0.00176      |\n",
      "|    loss                 | 0.00127      |\n",
      "|    n_updates            | 1930         |\n",
      "|    policy_gradient_loss | 2.36e-05     |\n",
      "|    std                  | 0.0172       |\n",
      "|    value_loss           | 2.06e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0836  |\n",
      "| time/              |          |\n",
      "|    fps             | 608      |\n",
      "|    iterations      | 194      |\n",
      "|    time_elapsed    | 3215     |\n",
      "|    total_timesteps | 1955520  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1975680, episode_reward=-0.09 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0945      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1975680      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043341327 |\n",
      "|    clip_fraction        | 0.0695       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.69         |\n",
      "|    explained_variance   | -0.0361      |\n",
      "|    learning_rate        | 0.00172      |\n",
      "|    loss                 | -0.00123     |\n",
      "|    n_updates            | 1950         |\n",
      "|    policy_gradient_loss | 0.00144      |\n",
      "|    std                  | 0.0174       |\n",
      "|    value_loss           | 2.45e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0852  |\n",
      "| time/              |          |\n",
      "|    fps             | 607      |\n",
      "|    iterations      | 196      |\n",
      "|    time_elapsed    | 3252     |\n",
      "|    total_timesteps | 1975680  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1995840, episode_reward=-0.12 +/- 0.07\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.121       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1995840      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020640255 |\n",
      "|    clip_fraction        | 0.0353       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.73         |\n",
      "|    explained_variance   | -0.0426      |\n",
      "|    learning_rate        | 0.00169      |\n",
      "|    loss                 | 0.00111      |\n",
      "|    n_updates            | 1970         |\n",
      "|    policy_gradient_loss | -0.000537    |\n",
      "|    std                  | 0.017        |\n",
      "|    value_loss           | 2.06e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0868  |\n",
      "| time/              |          |\n",
      "|    fps             | 607      |\n",
      "|    iterations      | 198      |\n",
      "|    time_elapsed    | 3286     |\n",
      "|    total_timesteps | 1995840  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2016000, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1.26e+03  |\n",
      "|    mean_reward          | -0.0837   |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2016000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0035576 |\n",
      "|    clip_fraction        | 0.0138    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | 5.76      |\n",
      "|    explained_variance   | -0.0652   |\n",
      "|    learning_rate        | 0.00166   |\n",
      "|    loss                 | -0.00134  |\n",
      "|    n_updates            | 1990      |\n",
      "|    policy_gradient_loss | 0.000293  |\n",
      "|    std                  | 0.0168    |\n",
      "|    value_loss           | 8.76e-08  |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0901  |\n",
      "| time/              |          |\n",
      "|    fps             | 605      |\n",
      "|    iterations      | 200      |\n",
      "|    time_elapsed    | 3327     |\n",
      "|    total_timesteps | 2016000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2036160, episode_reward=-0.12 +/- 0.07\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.118      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2036160     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002437761 |\n",
      "|    clip_fraction        | 0.044       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.78        |\n",
      "|    explained_variance   | -0.0357     |\n",
      "|    learning_rate        | 0.00162     |\n",
      "|    loss                 | -0.00124    |\n",
      "|    n_updates            | 2010        |\n",
      "|    policy_gradient_loss | -9.19e-05   |\n",
      "|    std                  | 0.0165      |\n",
      "|    value_loss           | 1.61e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0897  |\n",
      "| time/              |          |\n",
      "|    fps             | 604      |\n",
      "|    iterations      | 202      |\n",
      "|    time_elapsed    | 3368     |\n",
      "|    total_timesteps | 2036160  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2056320, episode_reward=-0.10 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.103       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2056320      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022795561 |\n",
      "|    clip_fraction        | 0.0464       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.78         |\n",
      "|    explained_variance   | -0.045       |\n",
      "|    learning_rate        | 0.00159      |\n",
      "|    loss                 | -0.0012      |\n",
      "|    n_updates            | 2030         |\n",
      "|    policy_gradient_loss | 0.000418     |\n",
      "|    std                  | 0.0165       |\n",
      "|    value_loss           | 1.56e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0944  |\n",
      "| time/              |          |\n",
      "|    fps             | 603      |\n",
      "|    iterations      | 204      |\n",
      "|    time_elapsed    | 3406     |\n",
      "|    total_timesteps | 2056320  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2076480, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0824     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2076480     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014929696 |\n",
      "|    clip_fraction        | 0.0495      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.77        |\n",
      "|    explained_variance   | -0.017      |\n",
      "|    learning_rate        | 0.00156     |\n",
      "|    loss                 | 0.00502     |\n",
      "|    n_updates            | 2050        |\n",
      "|    policy_gradient_loss | 0.000975    |\n",
      "|    std                  | 0.0166      |\n",
      "|    value_loss           | 1.77e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0957  |\n",
      "| time/              |          |\n",
      "|    fps             | 602      |\n",
      "|    iterations      | 206      |\n",
      "|    time_elapsed    | 3446     |\n",
      "|    total_timesteps | 2076480  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2096640, episode_reward=-0.10 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0959      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2096640      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044059795 |\n",
      "|    clip_fraction        | 0.051        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.79         |\n",
      "|    explained_variance   | -0.0253      |\n",
      "|    learning_rate        | 0.00152      |\n",
      "|    loss                 | -0.00129     |\n",
      "|    n_updates            | 2070         |\n",
      "|    policy_gradient_loss | 0.000353     |\n",
      "|    std                  | 0.0162       |\n",
      "|    value_loss           | 1.65e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0976  |\n",
      "| time/              |          |\n",
      "|    fps             | 600      |\n",
      "|    iterations      | 208      |\n",
      "|    time_elapsed    | 3489     |\n",
      "|    total_timesteps | 2096640  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2116800, episode_reward=-0.11 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.105       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2116800      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077695865 |\n",
      "|    clip_fraction        | 0.0517       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.8          |\n",
      "|    explained_variance   | 0.00262      |\n",
      "|    learning_rate        | 0.00149      |\n",
      "|    loss                 | 0.000388     |\n",
      "|    n_updates            | 2090         |\n",
      "|    policy_gradient_loss | 0.000588     |\n",
      "|    std                  | 0.0161       |\n",
      "|    value_loss           | 1.06e-06     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0958  |\n",
      "| time/              |          |\n",
      "|    fps             | 598      |\n",
      "|    iterations      | 210      |\n",
      "|    time_elapsed    | 3538     |\n",
      "|    total_timesteps | 2116800  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2136960, episode_reward=-0.07 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0746      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2136960      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034886464 |\n",
      "|    clip_fraction        | 0.0551       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.83         |\n",
      "|    explained_variance   | -0.0267      |\n",
      "|    learning_rate        | 0.00146      |\n",
      "|    loss                 | 0.000258     |\n",
      "|    n_updates            | 2110         |\n",
      "|    policy_gradient_loss | -0.000251    |\n",
      "|    std                  | 0.0158       |\n",
      "|    value_loss           | 9.72e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0953  |\n",
      "| time/              |          |\n",
      "|    fps             | 595      |\n",
      "|    iterations      | 212      |\n",
      "|    time_elapsed    | 3587     |\n",
      "|    total_timesteps | 2136960  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2157120, episode_reward=-0.09 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0898      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2157120      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031004772 |\n",
      "|    clip_fraction        | 0.0367       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.88         |\n",
      "|    explained_variance   | -0.0144      |\n",
      "|    learning_rate        | 0.00142      |\n",
      "|    loss                 | 0.00123      |\n",
      "|    n_updates            | 2130         |\n",
      "|    policy_gradient_loss | -0.000874    |\n",
      "|    std                  | 0.0153       |\n",
      "|    value_loss           | 3.5e-08      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0924  |\n",
      "| time/              |          |\n",
      "|    fps             | 593      |\n",
      "|    iterations      | 214      |\n",
      "|    time_elapsed    | 3633     |\n",
      "|    total_timesteps | 2157120  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2177280, episode_reward=-0.10 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0975      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2177280      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046372917 |\n",
      "|    clip_fraction        | 0.0313       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.91         |\n",
      "|    explained_variance   | -0.00894     |\n",
      "|    learning_rate        | 0.00139      |\n",
      "|    loss                 | 0.000217     |\n",
      "|    n_updates            | 2150         |\n",
      "|    policy_gradient_loss | 0.000139     |\n",
      "|    std                  | 0.0151       |\n",
      "|    value_loss           | 1.18e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0922  |\n",
      "| time/              |          |\n",
      "|    fps             | 592      |\n",
      "|    iterations      | 216      |\n",
      "|    time_elapsed    | 3676     |\n",
      "|    total_timesteps | 2177280  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2197440, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0787      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2197440      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043777865 |\n",
      "|    clip_fraction        | 0.0323       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.95         |\n",
      "|    explained_variance   | -0.00407     |\n",
      "|    learning_rate        | 0.00135      |\n",
      "|    loss                 | -0.000577    |\n",
      "|    n_updates            | 2170         |\n",
      "|    policy_gradient_loss | -0.00127     |\n",
      "|    std                  | 0.015        |\n",
      "|    value_loss           | 1.51e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0903  |\n",
      "| time/              |          |\n",
      "|    fps             | 591      |\n",
      "|    iterations      | 218      |\n",
      "|    time_elapsed    | 3715     |\n",
      "|    total_timesteps | 2197440  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2217600, episode_reward=-0.12 +/- 0.07\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.121      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2217600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013909073 |\n",
      "|    clip_fraction        | 0.0471      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.93        |\n",
      "|    explained_variance   | 0.00276     |\n",
      "|    learning_rate        | 0.00132     |\n",
      "|    loss                 | 0.00828     |\n",
      "|    n_updates            | 2190        |\n",
      "|    policy_gradient_loss | 0.000209    |\n",
      "|    std                  | 0.015       |\n",
      "|    value_loss           | 4.35e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0909  |\n",
      "| time/              |          |\n",
      "|    fps             | 590      |\n",
      "|    iterations      | 220      |\n",
      "|    time_elapsed    | 3756     |\n",
      "|    total_timesteps | 2217600  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2237760, episode_reward=-0.11 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.113       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2237760      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025178455 |\n",
      "|    clip_fraction        | 0.0476       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.93         |\n",
      "|    explained_variance   | 0.00318      |\n",
      "|    learning_rate        | 0.00129      |\n",
      "|    loss                 | 0.00112      |\n",
      "|    n_updates            | 2210         |\n",
      "|    policy_gradient_loss | 0.00101      |\n",
      "|    std                  | 0.0149       |\n",
      "|    value_loss           | 1.87e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.091   |\n",
      "| time/              |          |\n",
      "|    fps             | 589      |\n",
      "|    iterations      | 222      |\n",
      "|    time_elapsed    | 3797     |\n",
      "|    total_timesteps | 2237760  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2257920, episode_reward=-0.07 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.072      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2257920     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006624045 |\n",
      "|    clip_fraction        | 0.0215      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.95        |\n",
      "|    explained_variance   | -0.00249    |\n",
      "|    learning_rate        | 0.00125     |\n",
      "|    loss                 | -0.00029    |\n",
      "|    n_updates            | 2230        |\n",
      "|    policy_gradient_loss | 5.78e-05    |\n",
      "|    std                  | 0.0149      |\n",
      "|    value_loss           | 5.85e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0898  |\n",
      "| time/              |          |\n",
      "|    fps             | 588      |\n",
      "|    iterations      | 224      |\n",
      "|    time_elapsed    | 3837     |\n",
      "|    total_timesteps | 2257920  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2278080, episode_reward=-0.08 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0823      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2278080      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017828982 |\n",
      "|    clip_fraction        | 0.016        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.96         |\n",
      "|    explained_variance   | 0.00988      |\n",
      "|    learning_rate        | 0.00122      |\n",
      "|    loss                 | 0.000423     |\n",
      "|    n_updates            | 2250         |\n",
      "|    policy_gradient_loss | -5.2e-05     |\n",
      "|    std                  | 0.015        |\n",
      "|    value_loss           | 1.05e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0927  |\n",
      "| time/              |          |\n",
      "|    fps             | 586      |\n",
      "|    iterations      | 226      |\n",
      "|    time_elapsed    | 3881     |\n",
      "|    total_timesteps | 2278080  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2298240, episode_reward=-0.08 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0751      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2298240      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036435444 |\n",
      "|    clip_fraction        | 0.052        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.95         |\n",
      "|    explained_variance   | 0.00647      |\n",
      "|    learning_rate        | 0.00119      |\n",
      "|    loss                 | -0.00333     |\n",
      "|    n_updates            | 2270         |\n",
      "|    policy_gradient_loss | -0.00127     |\n",
      "|    std                  | 0.015        |\n",
      "|    value_loss           | 2.17e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.097   |\n",
      "| time/              |          |\n",
      "|    fps             | 585      |\n",
      "|    iterations      | 228      |\n",
      "|    time_elapsed    | 3926     |\n",
      "|    total_timesteps | 2298240  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2318400, episode_reward=-0.12 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.124       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2318400      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015781381 |\n",
      "|    clip_fraction        | 0.0101       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6            |\n",
      "|    explained_variance   | 0.0165       |\n",
      "|    learning_rate        | 0.00115      |\n",
      "|    loss                 | -0.000755    |\n",
      "|    n_updates            | 2290         |\n",
      "|    policy_gradient_loss | -9.08e-05    |\n",
      "|    std                  | 0.0149       |\n",
      "|    value_loss           | 1.62e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.097   |\n",
      "| time/              |          |\n",
      "|    fps             | 583      |\n",
      "|    iterations      | 230      |\n",
      "|    time_elapsed    | 3971     |\n",
      "|    total_timesteps | 2318400  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2338560, episode_reward=-0.10 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.101       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2338560      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017623722 |\n",
      "|    clip_fraction        | 0.0284       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6            |\n",
      "|    explained_variance   | 0.00886      |\n",
      "|    learning_rate        | 0.00112      |\n",
      "|    loss                 | 0.000502     |\n",
      "|    n_updates            | 2310         |\n",
      "|    policy_gradient_loss | 8.23e-05     |\n",
      "|    std                  | 0.015        |\n",
      "|    value_loss           | 8.62e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0924  |\n",
      "| time/              |          |\n",
      "|    fps             | 584      |\n",
      "|    iterations      | 232      |\n",
      "|    time_elapsed    | 4003     |\n",
      "|    total_timesteps | 2338560  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2358720, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0804     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2358720     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007629039 |\n",
      "|    clip_fraction        | 0.0797      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.03        |\n",
      "|    explained_variance   | 0.0164      |\n",
      "|    learning_rate        | 0.00109     |\n",
      "|    loss                 | -0.00127    |\n",
      "|    n_updates            | 2330        |\n",
      "|    policy_gradient_loss | 0.00143     |\n",
      "|    std                  | 0.0149      |\n",
      "|    value_loss           | 7.92e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0895  |\n",
      "| time/              |          |\n",
      "|    fps             | 583      |\n",
      "|    iterations      | 234      |\n",
      "|    time_elapsed    | 4039     |\n",
      "|    total_timesteps | 2358720  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2378880, episode_reward=-0.12 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.119     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2378880    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00316513 |\n",
      "|    clip_fraction        | 0.0714     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 6.03       |\n",
      "|    explained_variance   | 0.0191     |\n",
      "|    learning_rate        | 0.00105    |\n",
      "|    loss                 | 0.000634   |\n",
      "|    n_updates            | 2350       |\n",
      "|    policy_gradient_loss | 9.93e-06   |\n",
      "|    std                  | 0.0148     |\n",
      "|    value_loss           | 2.27e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0938  |\n",
      "| time/              |          |\n",
      "|    fps             | 583      |\n",
      "|    iterations      | 236      |\n",
      "|    time_elapsed    | 4073     |\n",
      "|    total_timesteps | 2378880  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2399040, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0757      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2399040      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050498443 |\n",
      "|    clip_fraction        | 0.0251       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.04         |\n",
      "|    explained_variance   | 0.013        |\n",
      "|    learning_rate        | 0.00102      |\n",
      "|    loss                 | 0.00041      |\n",
      "|    n_updates            | 2370         |\n",
      "|    policy_gradient_loss | -0.000119    |\n",
      "|    std                  | 0.0147       |\n",
      "|    value_loss           | 2.36e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0972  |\n",
      "| time/              |          |\n",
      "|    fps             | 584      |\n",
      "|    iterations      | 238      |\n",
      "|    time_elapsed    | 4105     |\n",
      "|    total_timesteps | 2399040  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2419200, episode_reward=-0.11 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.107       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2419200      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010383419 |\n",
      "|    clip_fraction        | 0.0169       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.08         |\n",
      "|    explained_variance   | 0.0217       |\n",
      "|    learning_rate        | 0.000985     |\n",
      "|    loss                 | -0.000967    |\n",
      "|    n_updates            | 2390         |\n",
      "|    policy_gradient_loss | 8.08e-05     |\n",
      "|    std                  | 0.0146       |\n",
      "|    value_loss           | 9.23e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0918  |\n",
      "| time/              |          |\n",
      "|    fps             | 584      |\n",
      "|    iterations      | 240      |\n",
      "|    time_elapsed    | 4141     |\n",
      "|    total_timesteps | 2419200  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2439360, episode_reward=-0.08 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0753     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2439360     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003834507 |\n",
      "|    clip_fraction        | 0.0268      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.09        |\n",
      "|    explained_variance   | -0.0138     |\n",
      "|    learning_rate        | 0.000951    |\n",
      "|    loss                 | 0.00167     |\n",
      "|    n_updates            | 2410        |\n",
      "|    policy_gradient_loss | -0.00022    |\n",
      "|    std                  | 0.0145      |\n",
      "|    value_loss           | 1.24e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0913  |\n",
      "| time/              |          |\n",
      "|    fps             | 584      |\n",
      "|    iterations      | 242      |\n",
      "|    time_elapsed    | 4174     |\n",
      "|    total_timesteps | 2439360  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2459520, episode_reward=-0.09 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0906    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2459520    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00338778 |\n",
      "|    clip_fraction        | 0.0374     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 6.11       |\n",
      "|    explained_variance   | 0.00619    |\n",
      "|    learning_rate        | 0.000918   |\n",
      "|    loss                 | -0.00127   |\n",
      "|    n_updates            | 2430       |\n",
      "|    policy_gradient_loss | -0.000594  |\n",
      "|    std                  | 0.0143     |\n",
      "|    value_loss           | 1.32e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.094   |\n",
      "| time/              |          |\n",
      "|    fps             | 584      |\n",
      "|    iterations      | 244      |\n",
      "|    time_elapsed    | 4211     |\n",
      "|    total_timesteps | 2459520  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2479680, episode_reward=-0.11 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.11        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2479680      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026257506 |\n",
      "|    clip_fraction        | 0.0325       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.13         |\n",
      "|    explained_variance   | 0.0207       |\n",
      "|    learning_rate        | 0.000884     |\n",
      "|    loss                 | -0.000865    |\n",
      "|    n_updates            | 2450         |\n",
      "|    policy_gradient_loss | -0.000548    |\n",
      "|    std                  | 0.0142       |\n",
      "|    value_loss           | 1.5e-07      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0956  |\n",
      "| time/              |          |\n",
      "|    fps             | 584      |\n",
      "|    iterations      | 246      |\n",
      "|    time_elapsed    | 4244     |\n",
      "|    total_timesteps | 2479680  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2499840, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0897      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2499840      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059681125 |\n",
      "|    clip_fraction        | 0.0247       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.14         |\n",
      "|    explained_variance   | 0.0207       |\n",
      "|    learning_rate        | 0.00085      |\n",
      "|    loss                 | 0.00246      |\n",
      "|    n_updates            | 2470         |\n",
      "|    policy_gradient_loss | 0.000109     |\n",
      "|    std                  | 0.0141       |\n",
      "|    value_loss           | 9.35e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0916  |\n",
      "| time/              |          |\n",
      "|    fps             | 584      |\n",
      "|    iterations      | 248      |\n",
      "|    time_elapsed    | 4279     |\n",
      "|    total_timesteps | 2499840  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2520000, episode_reward=-0.08 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0808      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2520000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071157203 |\n",
      "|    clip_fraction        | 0.0624       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.14         |\n",
      "|    explained_variance   | 0.026        |\n",
      "|    learning_rate        | 0.000817     |\n",
      "|    loss                 | 0.0018       |\n",
      "|    n_updates            | 2490         |\n",
      "|    policy_gradient_loss | 0.00102      |\n",
      "|    std                  | 0.014        |\n",
      "|    value_loss           | 1.99e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0909  |\n",
      "| time/              |          |\n",
      "|    fps             | 583      |\n",
      "|    iterations      | 250      |\n",
      "|    time_elapsed    | 4316     |\n",
      "|    total_timesteps | 2520000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2540160, episode_reward=-0.07 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0731    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2540160    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01048511 |\n",
      "|    clip_fraction        | 0.0651     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 6.16       |\n",
      "|    explained_variance   | 0.0102     |\n",
      "|    learning_rate        | 0.000783   |\n",
      "|    loss                 | -0.00309   |\n",
      "|    n_updates            | 2510       |\n",
      "|    policy_gradient_loss | -0.00161   |\n",
      "|    std                  | 0.0138     |\n",
      "|    value_loss           | 5.34e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0903  |\n",
      "| time/              |          |\n",
      "|    fps             | 584      |\n",
      "|    iterations      | 252      |\n",
      "|    time_elapsed    | 4348     |\n",
      "|    total_timesteps | 2540160  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2560320, episode_reward=-0.10 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0996      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2560320      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029083162 |\n",
      "|    clip_fraction        | 0.0385       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.16         |\n",
      "|    explained_variance   | 0.048        |\n",
      "|    learning_rate        | 0.00075      |\n",
      "|    loss                 | -0.00327     |\n",
      "|    n_updates            | 2530         |\n",
      "|    policy_gradient_loss | -0.000145    |\n",
      "|    std                  | 0.0139       |\n",
      "|    value_loss           | 7.06e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0907  |\n",
      "| time/              |          |\n",
      "|    fps             | 583      |\n",
      "|    iterations      | 254      |\n",
      "|    time_elapsed    | 4385     |\n",
      "|    total_timesteps | 2560320  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2580480, episode_reward=-0.11 +/- 0.07\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.106      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2580480     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006228295 |\n",
      "|    clip_fraction        | 0.0293      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.16        |\n",
      "|    explained_variance   | -0.0043     |\n",
      "|    learning_rate        | 0.000716    |\n",
      "|    loss                 | -0.00134    |\n",
      "|    n_updates            | 2550        |\n",
      "|    policy_gradient_loss | -0.000245   |\n",
      "|    std                  | 0.0137      |\n",
      "|    value_loss           | 1.72e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0897  |\n",
      "| time/              |          |\n",
      "|    fps             | 583      |\n",
      "|    iterations      | 256      |\n",
      "|    time_elapsed    | 4420     |\n",
      "|    total_timesteps | 2580480  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2600640, episode_reward=-0.09 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0919      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2600640      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030105433 |\n",
      "|    clip_fraction        | 0.0364       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.17         |\n",
      "|    explained_variance   | 0.00886      |\n",
      "|    learning_rate        | 0.000682     |\n",
      "|    loss                 | -0.00409     |\n",
      "|    n_updates            | 2570         |\n",
      "|    policy_gradient_loss | -0.00129     |\n",
      "|    std                  | 0.0136       |\n",
      "|    value_loss           | 2.27e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0893  |\n",
      "| time/              |          |\n",
      "|    fps             | 583      |\n",
      "|    iterations      | 258      |\n",
      "|    time_elapsed    | 4457     |\n",
      "|    total_timesteps | 2600640  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2620800, episode_reward=-0.09 +/- 0.07\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0918     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2620800     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007989899 |\n",
      "|    clip_fraction        | 0.039       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.19        |\n",
      "|    explained_variance   | 0.0138      |\n",
      "|    learning_rate        | 0.000649    |\n",
      "|    loss                 | 0.00471     |\n",
      "|    n_updates            | 2590        |\n",
      "|    policy_gradient_loss | -0.000451   |\n",
      "|    std                  | 0.0133      |\n",
      "|    value_loss           | 4.76e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0908  |\n",
      "| time/              |          |\n",
      "|    fps             | 583      |\n",
      "|    iterations      | 260      |\n",
      "|    time_elapsed    | 4495     |\n",
      "|    total_timesteps | 2620800  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2640960, episode_reward=-0.10 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1.26e+03      |\n",
      "|    mean_reward          | -0.1          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 2640960       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00028802012 |\n",
      "|    clip_fraction        | 0.0323        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | 6.2           |\n",
      "|    explained_variance   | 0.053         |\n",
      "|    learning_rate        | 0.000615      |\n",
      "|    loss                 | -0.000409     |\n",
      "|    n_updates            | 2610          |\n",
      "|    policy_gradient_loss | 0.0003        |\n",
      "|    std                  | 0.0133        |\n",
      "|    value_loss           | 9.14e-08      |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0879  |\n",
      "| time/              |          |\n",
      "|    fps             | 582      |\n",
      "|    iterations      | 262      |\n",
      "|    time_elapsed    | 4532     |\n",
      "|    total_timesteps | 2640960  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2661120, episode_reward=-0.10 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0984      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2661120      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024768296 |\n",
      "|    clip_fraction        | 0.0219       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.21         |\n",
      "|    explained_variance   | 0.0422       |\n",
      "|    learning_rate        | 0.000582     |\n",
      "|    loss                 | -0.000282    |\n",
      "|    n_updates            | 2630         |\n",
      "|    policy_gradient_loss | -0.000558    |\n",
      "|    std                  | 0.0132       |\n",
      "|    value_loss           | 7.17e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0886  |\n",
      "| time/              |          |\n",
      "|    fps             | 582      |\n",
      "|    iterations      | 264      |\n",
      "|    time_elapsed    | 4569     |\n",
      "|    total_timesteps | 2661120  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2681280, episode_reward=-0.09 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0904      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2681280      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045948382 |\n",
      "|    clip_fraction        | 0.0164       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.22         |\n",
      "|    explained_variance   | 0.044        |\n",
      "|    learning_rate        | 0.000548     |\n",
      "|    loss                 | -0.00094     |\n",
      "|    n_updates            | 2650         |\n",
      "|    policy_gradient_loss | -0.000472    |\n",
      "|    std                  | 0.0131       |\n",
      "|    value_loss           | 1.27e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0894  |\n",
      "| time/              |          |\n",
      "|    fps             | 582      |\n",
      "|    iterations      | 266      |\n",
      "|    time_elapsed    | 4605     |\n",
      "|    total_timesteps | 2681280  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2701440, episode_reward=-0.10 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.105       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2701440      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013787166 |\n",
      "|    clip_fraction        | 0.0244       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.22         |\n",
      "|    explained_variance   | 0.0627       |\n",
      "|    learning_rate        | 0.000514     |\n",
      "|    loss                 | -0.000965    |\n",
      "|    n_updates            | 2670         |\n",
      "|    policy_gradient_loss | -4.81e-05    |\n",
      "|    std                  | 0.013        |\n",
      "|    value_loss           | 6.74e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0883  |\n",
      "| time/              |          |\n",
      "|    fps             | 581      |\n",
      "|    iterations      | 268      |\n",
      "|    time_elapsed    | 4648     |\n",
      "|    total_timesteps | 2701440  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2721600, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0876     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2721600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005501253 |\n",
      "|    clip_fraction        | 0.0268      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.24        |\n",
      "|    explained_variance   | 0.077       |\n",
      "|    learning_rate        | 0.000481    |\n",
      "|    loss                 | -0.000505   |\n",
      "|    n_updates            | 2690        |\n",
      "|    policy_gradient_loss | -0.001      |\n",
      "|    std                  | 0.0128      |\n",
      "|    value_loss           | 9.99e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0868  |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 270      |\n",
      "|    time_elapsed    | 4694     |\n",
      "|    total_timesteps | 2721600  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2741760, episode_reward=-0.09 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0918     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2741760     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008577642 |\n",
      "|    clip_fraction        | 0.0354      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.26        |\n",
      "|    explained_variance   | 0.0178      |\n",
      "|    learning_rate        | 0.000447    |\n",
      "|    loss                 | -2.8e-05    |\n",
      "|    n_updates            | 2710        |\n",
      "|    policy_gradient_loss | -0.000333   |\n",
      "|    std                  | 0.0128      |\n",
      "|    value_loss           | 6.03e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0897  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 272      |\n",
      "|    time_elapsed    | 4739     |\n",
      "|    total_timesteps | 2741760  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2761920, episode_reward=-0.10 +/- 0.07\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.105       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2761920      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027130232 |\n",
      "|    clip_fraction        | 0.0141       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.27         |\n",
      "|    explained_variance   | 0.0442       |\n",
      "|    learning_rate        | 0.000414     |\n",
      "|    loss                 | -0.00183     |\n",
      "|    n_updates            | 2730         |\n",
      "|    policy_gradient_loss | -0.000638    |\n",
      "|    std                  | 0.0127       |\n",
      "|    value_loss           | 1.81e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0919  |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 274      |\n",
      "|    time_elapsed    | 4782     |\n",
      "|    total_timesteps | 2761920  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2782080, episode_reward=-0.09 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0863     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2782080     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004562276 |\n",
      "|    clip_fraction        | 0.0153      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.27        |\n",
      "|    explained_variance   | 0.0866      |\n",
      "|    learning_rate        | 0.00038     |\n",
      "|    loss                 | -0.00114    |\n",
      "|    n_updates            | 2750        |\n",
      "|    policy_gradient_loss | -0.000389   |\n",
      "|    std                  | 0.0127      |\n",
      "|    value_loss           | 7.37e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0913  |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 276      |\n",
      "|    time_elapsed    | 4817     |\n",
      "|    total_timesteps | 2782080  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2802240, episode_reward=-0.10 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0973     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2802240     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002772122 |\n",
      "|    clip_fraction        | 0.0102      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.27        |\n",
      "|    explained_variance   | 0.0665      |\n",
      "|    learning_rate        | 0.000346    |\n",
      "|    loss                 | 0.000939    |\n",
      "|    n_updates            | 2770        |\n",
      "|    policy_gradient_loss | -0.00035    |\n",
      "|    std                  | 0.0127      |\n",
      "|    value_loss           | 8.44e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0903  |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 278      |\n",
      "|    time_elapsed    | 4852     |\n",
      "|    total_timesteps | 2802240  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2822400, episode_reward=-0.10 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.103      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2822400     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005231475 |\n",
      "|    clip_fraction        | 0.0364      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.27        |\n",
      "|    explained_variance   | 0.0373      |\n",
      "|    learning_rate        | 0.000313    |\n",
      "|    loss                 | -0.00135    |\n",
      "|    n_updates            | 2790        |\n",
      "|    policy_gradient_loss | -0.00161    |\n",
      "|    std                  | 0.0126      |\n",
      "|    value_loss           | 2.45e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0923  |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 280      |\n",
      "|    time_elapsed    | 4887     |\n",
      "|    total_timesteps | 2822400  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2842560, episode_reward=-0.11 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.113     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2842560    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00158762 |\n",
      "|    clip_fraction        | 0.0071     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 6.28       |\n",
      "|    explained_variance   | 0.0425     |\n",
      "|    learning_rate        | 0.000279   |\n",
      "|    loss                 | -0.000336  |\n",
      "|    n_updates            | 2810       |\n",
      "|    policy_gradient_loss | -3.15e-05  |\n",
      "|    std                  | 0.0127     |\n",
      "|    value_loss           | 9.08e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0916  |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 282      |\n",
      "|    time_elapsed    | 4926     |\n",
      "|    total_timesteps | 2842560  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2862720, episode_reward=-0.09 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0862      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2862720      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021619205 |\n",
      "|    clip_fraction        | 0.00808      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.28         |\n",
      "|    explained_variance   | 0.0396       |\n",
      "|    learning_rate        | 0.000246     |\n",
      "|    loss                 | 0.00183      |\n",
      "|    n_updates            | 2830         |\n",
      "|    policy_gradient_loss | -0.000692    |\n",
      "|    std                  | 0.0127       |\n",
      "|    value_loss           | 8.16e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0876  |\n",
      "| time/              |          |\n",
      "|    fps             | 576      |\n",
      "|    iterations      | 284      |\n",
      "|    time_elapsed    | 4962     |\n",
      "|    total_timesteps | 2862720  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2882880, episode_reward=-0.08 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0817      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2882880      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027885055 |\n",
      "|    clip_fraction        | 0.0425       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.29         |\n",
      "|    explained_variance   | 0.0253       |\n",
      "|    learning_rate        | 0.000212     |\n",
      "|    loss                 | -0.00178     |\n",
      "|    n_updates            | 2850         |\n",
      "|    policy_gradient_loss | -0.00127     |\n",
      "|    std                  | 0.0126       |\n",
      "|    value_loss           | 2.39e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.087   |\n",
      "| time/              |          |\n",
      "|    fps             | 576      |\n",
      "|    iterations      | 286      |\n",
      "|    time_elapsed    | 5003     |\n",
      "|    total_timesteps | 2882880  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2903040, episode_reward=-0.10 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.1       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2903040    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00301865 |\n",
      "|    clip_fraction        | 0.0115     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 6.3        |\n",
      "|    explained_variance   | 0.0185     |\n",
      "|    learning_rate        | 0.000178   |\n",
      "|    loss                 | -0.00105   |\n",
      "|    n_updates            | 2870       |\n",
      "|    policy_gradient_loss | -0.000694  |\n",
      "|    std                  | 0.0126     |\n",
      "|    value_loss           | 8e-08      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0848  |\n",
      "| time/              |          |\n",
      "|    fps             | 575      |\n",
      "|    iterations      | 288      |\n",
      "|    time_elapsed    | 5043     |\n",
      "|    total_timesteps | 2903040  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2923200, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0755      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2923200      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018635849 |\n",
      "|    clip_fraction        | 0.00259      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.3          |\n",
      "|    explained_variance   | 0.0377       |\n",
      "|    learning_rate        | 0.000145     |\n",
      "|    loss                 | -0.00178     |\n",
      "|    n_updates            | 2890         |\n",
      "|    policy_gradient_loss | -0.000146    |\n",
      "|    std                  | 0.0126       |\n",
      "|    value_loss           | 2.74e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0887  |\n",
      "| time/              |          |\n",
      "|    fps             | 574      |\n",
      "|    iterations      | 290      |\n",
      "|    time_elapsed    | 5085     |\n",
      "|    total_timesteps | 2923200  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2943360, episode_reward=-0.11 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.106       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2943360      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025685113 |\n",
      "|    clip_fraction        | 0.000833     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.31         |\n",
      "|    explained_variance   | 0.0532       |\n",
      "|    learning_rate        | 0.000111     |\n",
      "|    loss                 | -0.000439    |\n",
      "|    n_updates            | 2910         |\n",
      "|    policy_gradient_loss | -8.36e-05    |\n",
      "|    std                  | 0.0125       |\n",
      "|    value_loss           | 8.66e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0868  |\n",
      "| time/              |          |\n",
      "|    fps             | 574      |\n",
      "|    iterations      | 292      |\n",
      "|    time_elapsed    | 5120     |\n",
      "|    total_timesteps | 2943360  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2963520, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0877      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2963520      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028280239 |\n",
      "|    clip_fraction        | 0.00478      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.31         |\n",
      "|    explained_variance   | 0.0452       |\n",
      "|    learning_rate        | 7.76e-05     |\n",
      "|    loss                 | -0.000319    |\n",
      "|    n_updates            | 2930         |\n",
      "|    policy_gradient_loss | -0.000328    |\n",
      "|    std                  | 0.0125       |\n",
      "|    value_loss           | 2.18e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0867  |\n",
      "| time/              |          |\n",
      "|    fps             | 574      |\n",
      "|    iterations      | 294      |\n",
      "|    time_elapsed    | 5160     |\n",
      "|    total_timesteps | 2963520  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2983680, episode_reward=-0.09 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0861     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2983680     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003437729 |\n",
      "|    clip_fraction        | 0.00642     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.31        |\n",
      "|    explained_variance   | -0.00863    |\n",
      "|    learning_rate        | 4.4e-05     |\n",
      "|    loss                 | 0.000957    |\n",
      "|    n_updates            | 2950        |\n",
      "|    policy_gradient_loss | -0.000506   |\n",
      "|    std                  | 0.0125      |\n",
      "|    value_loss           | 1.14e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0885  |\n",
      "| time/              |          |\n",
      "|    fps             | 573      |\n",
      "|    iterations      | 296      |\n",
      "|    time_elapsed    | 5203     |\n",
      "|    total_timesteps | 2983680  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3003840, episode_reward=-0.10 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.103       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3003840      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014613043 |\n",
      "|    clip_fraction        | 4.96e-05     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.31         |\n",
      "|    explained_variance   | 0.0633       |\n",
      "|    learning_rate        | 1.04e-05     |\n",
      "|    loss                 | -0.00144     |\n",
      "|    n_updates            | 2970         |\n",
      "|    policy_gradient_loss | -0.000582    |\n",
      "|    std                  | 0.0125       |\n",
      "|    value_loss           | 1.82e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0908  |\n",
      "| time/              |          |\n",
      "|    fps             | 572      |\n",
      "|    iterations      | 298      |\n",
      "|    time_elapsed    | 5247     |\n",
      "|    total_timesteps | 3003840  |\n",
      "---------------------------------\n",
      "Using cpu device\n",
      "Eval num_timesteps=4032, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0168     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4032        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023683982 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.72       |\n",
      "|    explained_variance   | 0.0347      |\n",
      "|    learning_rate        | 0.005       |\n",
      "|    loss                 | -0.00548    |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00615    |\n",
      "|    std                  | 0.917       |\n",
      "|    value_loss           | 0.00124     |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.156   |\n",
      "| time/              |          |\n",
      "|    fps             | 592      |\n",
      "|    iterations      | 2        |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 4032     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8064, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0237     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8064        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032985523 |\n",
      "|    clip_fraction        | 0.112       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.48       |\n",
      "|    explained_variance   | 0.028       |\n",
      "|    learning_rate        | 0.00499     |\n",
      "|    loss                 | -0.0279     |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0121     |\n",
      "|    std                  | 0.817       |\n",
      "|    value_loss           | 2.34e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.143   |\n",
      "| time/              |          |\n",
      "|    fps             | 580      |\n",
      "|    iterations      | 4        |\n",
      "|    time_elapsed    | 13       |\n",
      "|    total_timesteps | 8064     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=12096, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0167     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12096       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026872464 |\n",
      "|    clip_fraction        | 0.0979      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.17       |\n",
      "|    explained_variance   | 0.122       |\n",
      "|    learning_rate        | 0.00498     |\n",
      "|    loss                 | 0.00252     |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00901    |\n",
      "|    std                  | 0.705       |\n",
      "|    value_loss           | 1.6e-06     |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.139   |\n",
      "| time/              |          |\n",
      "|    fps             | 492      |\n",
      "|    iterations      | 6        |\n",
      "|    time_elapsed    | 24       |\n",
      "|    total_timesteps | 12096    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=16128, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0127     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 16128       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006636111 |\n",
      "|    clip_fraction        | 0.0293      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.06       |\n",
      "|    explained_variance   | -0.00124    |\n",
      "|    learning_rate        | 0.00498     |\n",
      "|    loss                 | -0.0082     |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00081    |\n",
      "|    std                  | 0.699       |\n",
      "|    value_loss           | 0.000204    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.136   |\n",
      "| time/              |          |\n",
      "|    fps             | 456      |\n",
      "|    iterations      | 8        |\n",
      "|    time_elapsed    | 35       |\n",
      "|    total_timesteps | 16128    |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Horizons\n",
    "\n",
    "# Default\n",
    "envs = VecMonitor(DummyVecEnv([\n",
    "    lambda: tradingEng(paths1,action = 'small-More-Trust', obs = 'xs'),\n",
    "    lambda: tradingEng(paths2,action = 'small-More-Trust', obs = 'xs')\n",
    "]),filename='eval_logs-Trainhugeb')\n",
    "ev_env = VecMonitor(DummyVecEnv([\n",
    "    lambda: tradingEng(paths_ev,action = 'small-More-Trust', obs = 'xs'),\n",
    "]))\n",
    "\n",
    "eval_callback = EvalCallback(\n",
    "    ev_env,\n",
    "    best_model_save_path='./logs/best_model-hugeb',\n",
    "    log_path='./logs/eval_logs/ev-hugeb',\n",
    "    eval_freq=252*8*5,\n",
    "    deterministic=True,\n",
    "    render=False,\n",
    "    verbose = True,\n",
    "    n_eval_episodes = 8\n",
    ")\n",
    "\n",
    "model = PPO(\"MlpPolicy\", envs, batch_size = 252*10*5, learning_rate=linear_schedule(0.005), policy_kwargs=policy_kwargs, n_steps=252*20*5, normalize_advantage=True, gamma = 0.9, verbose = 1) \n",
    "\n",
    "model.learn(total_timesteps=5e6, log_interval=2, callback=eval_callback) \n",
    "\n",
    "# 1 yr\n",
    "#envs = VecMonitor(DummyVecEnv([\n",
    "#    lambda: tradingEng(paths1,action = 'small-More-Trust', obs = 'xs', resetdate=1.0),\n",
    "#    lambda: tradingEng(paths2,action = 'small-More-Trust', obs = 'xs',resetdate=1.0)\n",
    "#]),filename='logs1-Train')\n",
    "#ev_env = VecMonitor(DummyVecEnv([\n",
    "#    lambda: tradingEng(paths_ev,action = 'small-More-Trust', obs = 'xs',resetdate=1.0),\n",
    "#]))\n",
    "#eval_callback = EvalCallback(\n",
    "#    ev_env,\n",
    "#    best_model_save_path='./logs/best_model1',\n",
    "#    log_path='./logs/eval_logs1/ev',\n",
    "#    eval_freq=252*8*1,\n",
    "#    deterministic=True,\n",
    "#    render=False,\n",
    "#    verbose = True,\n",
    "#    n_eval_episodes = 8\n",
    "#)\n",
    "\n",
    "# Instantiate the agent\n",
    "#model = PPO(\"MlpPolicy\", envs, batch_size = 252*2*1, learning_rate=linear_schedule(0.005), policy_kwargs=policy_kwargs, n_steps=252*4*1, normalize_advantage=True, gamma = 0.9, verbose = 1) \n",
    "#\n",
    "#model.learn(total_timesteps=3e6, log_interval=2, callback=eval_callback) \n",
    "\n",
    "# 20 yr\n",
    "#envs = VecMonitor(DummyVecEnv([\n",
    "#    lambda: tradingEng(paths1,action = 'small-More-Trust', obs = 'xs',resetdate=20.0),\n",
    "#    lambda: tradingEng(paths2,action = 'small-More-Trust', obs = 'xs',resetdate=20.0)\n",
    "#]),filename='logs20-Train')\n",
    "#ev_env = VecMonitor(DummyVecEnv([\n",
    "#    lambda: tradingEng(paths_ev,action = 'small-More-Trust', obs = 'xs',resetdate=20.0),\n",
    "#]))\n",
    "\n",
    "#eval_callback = EvalCallback(\n",
    "#    ev_env,\n",
    "#    best_model_save_path='./logs/best_model20',\n",
    "#    log_path='./logs/eval_logs20/ev',\n",
    "#    eval_freq=252*8*20,\n",
    "#    deterministic=True,\n",
    "#    render=False,\n",
    "#    verbose = True,\n",
    "#    n_eval_episodes = 8\n",
    "#)\n",
    "\n",
    "#model = PPO(\"MlpPolicy\", envs, batch_size = 252*2*20, learning_rate=linear_schedule(0.005), policy_kwargs=policy_kwargs, n_steps=252*4*20, normalize_advantage=True, gamma = 0.9, verbose = 1) \n",
    "\n",
    "#model.learn(total_timesteps=3e6, log_interval=2, callback=eval_callback) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=20160, episode_reward=-0.14 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.144      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 20160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015343782 |\n",
      "|    clip_fraction        | 0.0715      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.76       |\n",
      "|    explained_variance   | 0.0319      |\n",
      "|    learning_rate        | 0.00498     |\n",
      "|    loss                 | -0.00173    |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00248    |\n",
      "|    std                  | 0.935       |\n",
      "|    value_loss           | 0.0394      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -3.51    |\n",
      "| time/              |          |\n",
      "|    fps             | 613      |\n",
      "|    iterations      | 2        |\n",
      "|    time_elapsed    | 32       |\n",
      "|    total_timesteps | 20160    |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=40320, episode_reward=-0.18 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.182      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 40320       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022756876 |\n",
      "|    clip_fraction        | 0.075       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.52       |\n",
      "|    explained_variance   | 0.00474     |\n",
      "|    learning_rate        | 0.00495     |\n",
      "|    loss                 | -0.0107     |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00446    |\n",
      "|    std                  | 0.828       |\n",
      "|    value_loss           | 0.000389    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -3.39    |\n",
      "| time/              |          |\n",
      "|    fps             | 593      |\n",
      "|    iterations      | 4        |\n",
      "|    time_elapsed    | 67       |\n",
      "|    total_timesteps | 40320    |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=60480, episode_reward=-0.22 +/- 0.10\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.224      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 60480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027065812 |\n",
      "|    clip_fraction        | 0.131       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.11       |\n",
      "|    explained_variance   | 0.0489      |\n",
      "|    learning_rate        | 0.00492     |\n",
      "|    loss                 | -0.0186     |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00791    |\n",
      "|    std                  | 0.7         |\n",
      "|    value_loss           | 0.000165    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -3.12    |\n",
      "| time/              |          |\n",
      "|    fps             | 563      |\n",
      "|    iterations      | 6        |\n",
      "|    time_elapsed    | 107      |\n",
      "|    total_timesteps | 60480    |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=80640, episode_reward=-0.19 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.191      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 80640       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013627913 |\n",
      "|    clip_fraction        | 0.0529      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.8        |\n",
      "|    explained_variance   | 0.0303      |\n",
      "|    learning_rate        | 0.00488     |\n",
      "|    loss                 | -0.00362    |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00233    |\n",
      "|    std                  | 0.607       |\n",
      "|    value_loss           | 0.00128     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -3       |\n",
      "| time/              |          |\n",
      "|    fps             | 571      |\n",
      "|    iterations      | 8        |\n",
      "|    time_elapsed    | 141      |\n",
      "|    total_timesteps | 80640    |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=100800, episode_reward=-0.23 +/- 0.08\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.229      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 100800      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021178652 |\n",
      "|    clip_fraction        | 0.136       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0.105       |\n",
      "|    learning_rate        | 0.00485     |\n",
      "|    loss                 | -0.0108     |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00856    |\n",
      "|    std                  | 0.523       |\n",
      "|    value_loss           | 4.33e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.75    |\n",
      "| time/              |          |\n",
      "|    fps             | 571      |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 176      |\n",
      "|    total_timesteps | 100800   |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=120960, episode_reward=-0.14 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.136      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 120960      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023455735 |\n",
      "|    clip_fraction        | 0.106       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | 0.186       |\n",
      "|    learning_rate        | 0.00482     |\n",
      "|    loss                 | -0.00779    |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0066     |\n",
      "|    std                  | 0.444       |\n",
      "|    value_loss           | 9.99e-05    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.58    |\n",
      "| time/              |          |\n",
      "|    fps             | 574      |\n",
      "|    iterations      | 12       |\n",
      "|    time_elapsed    | 210      |\n",
      "|    total_timesteps | 120960   |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=141120, episode_reward=-0.12 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.123      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 141120      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018171137 |\n",
      "|    clip_fraction        | 0.183       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.636      |\n",
      "|    explained_variance   | 0.557       |\n",
      "|    learning_rate        | 0.00478     |\n",
      "|    loss                 | -0.0221     |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0175     |\n",
      "|    std                  | 0.371       |\n",
      "|    value_loss           | 5.77e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.22    |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 14       |\n",
      "|    time_elapsed    | 244      |\n",
      "|    total_timesteps | 141120   |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=161280, episode_reward=-0.11 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.112      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 161280      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024812015 |\n",
      "|    clip_fraction        | 0.174       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.283      |\n",
      "|    explained_variance   | 0.28        |\n",
      "|    learning_rate        | 0.00475     |\n",
      "|    loss                 | -0.019      |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0125     |\n",
      "|    std                  | 0.311       |\n",
      "|    value_loss           | 8.85e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 583      |\n",
      "|    iterations      | 16       |\n",
      "|    time_elapsed    | 276      |\n",
      "|    total_timesteps | 161280   |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=181440, episode_reward=-0.12 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.118      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 181440      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026276438 |\n",
      "|    clip_fraction        | 0.148       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.104       |\n",
      "|    explained_variance   | 0.155       |\n",
      "|    learning_rate        | 0.00471     |\n",
      "|    loss                 | -0.0155     |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0106     |\n",
      "|    std                  | 0.252       |\n",
      "|    value_loss           | 1.18e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.59    |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 18       |\n",
      "|    time_elapsed    | 313      |\n",
      "|    total_timesteps | 181440   |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=201600, episode_reward=-0.11 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.108      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 201600      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021281919 |\n",
      "|    clip_fraction        | 0.225       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.472       |\n",
      "|    explained_variance   | 0.506       |\n",
      "|    learning_rate        | 0.00468     |\n",
      "|    loss                 | -0.0254     |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0205     |\n",
      "|    std                  | 0.219       |\n",
      "|    value_loss           | 2.54e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.31    |\n",
      "| time/              |          |\n",
      "|    fps             | 574      |\n",
      "|    iterations      | 20       |\n",
      "|    time_elapsed    | 350      |\n",
      "|    total_timesteps | 201600   |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=221760, episode_reward=-0.10 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.101      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 221760      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023940744 |\n",
      "|    clip_fraction        | 0.166       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.808       |\n",
      "|    explained_variance   | 0.0664      |\n",
      "|    learning_rate        | 0.00465     |\n",
      "|    loss                 | -0.0174     |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0121     |\n",
      "|    std                  | 0.184       |\n",
      "|    value_loss           | 4.08e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.09    |\n",
      "| time/              |          |\n",
      "|    fps             | 570      |\n",
      "|    iterations      | 22       |\n",
      "|    time_elapsed    | 388      |\n",
      "|    total_timesteps | 221760   |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=241920, episode_reward=-0.12 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.123      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 241920      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026205098 |\n",
      "|    clip_fraction        | 0.136       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.19        |\n",
      "|    explained_variance   | 0.308       |\n",
      "|    learning_rate        | 0.00461     |\n",
      "|    loss                 | -0.013      |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0108     |\n",
      "|    std                  | 0.148       |\n",
      "|    value_loss           | 2.81e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.904   |\n",
      "| time/              |          |\n",
      "|    fps             | 569      |\n",
      "|    iterations      | 24       |\n",
      "|    time_elapsed    | 424      |\n",
      "|    total_timesteps | 241920   |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=262080, episode_reward=-0.09 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0856     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 262080      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025193976 |\n",
      "|    clip_fraction        | 0.166       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.62        |\n",
      "|    explained_variance   | 0.155       |\n",
      "|    learning_rate        | 0.00458     |\n",
      "|    loss                 | -0.0178     |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.0142     |\n",
      "|    std                  | 0.121       |\n",
      "|    value_loss           | 9.54e-07    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.766   |\n",
      "| time/              |          |\n",
      "|    fps             | 569      |\n",
      "|    iterations      | 26       |\n",
      "|    time_elapsed    | 460      |\n",
      "|    total_timesteps | 262080   |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=282240, episode_reward=-0.10 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0963     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 282240      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020650117 |\n",
      "|    clip_fraction        | 0.0895      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.94        |\n",
      "|    explained_variance   | 0.0981      |\n",
      "|    learning_rate        | 0.00455     |\n",
      "|    loss                 | -0.01       |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.00704    |\n",
      "|    std                  | 0.1         |\n",
      "|    value_loss           | 3.87e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.649   |\n",
      "| time/              |          |\n",
      "|    fps             | 568      |\n",
      "|    iterations      | 28       |\n",
      "|    time_elapsed    | 496      |\n",
      "|    total_timesteps | 282240   |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=302400, episode_reward=-0.10 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0961     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 302400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009914357 |\n",
      "|    clip_fraction        | 0.0449      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.28        |\n",
      "|    explained_variance   | 0.14        |\n",
      "|    learning_rate        | 0.00451     |\n",
      "|    loss                 | -0.0052     |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.00295    |\n",
      "|    std                  | 0.0867      |\n",
      "|    value_loss           | 8.09e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.543   |\n",
      "| time/              |          |\n",
      "|    fps             | 568      |\n",
      "|    iterations      | 30       |\n",
      "|    time_elapsed    | 531      |\n",
      "|    total_timesteps | 302400   |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=322560, episode_reward=-0.11 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.111     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 322560     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02201213 |\n",
      "|    clip_fraction        | 0.0905     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 2.62       |\n",
      "|    explained_variance   | 0.335      |\n",
      "|    learning_rate        | 0.00448    |\n",
      "|    loss                 | -0.00894   |\n",
      "|    n_updates            | 310        |\n",
      "|    policy_gradient_loss | -0.00701   |\n",
      "|    std                  | 0.0726     |\n",
      "|    value_loss           | 1.01e-06   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.455   |\n",
      "| time/              |          |\n",
      "|    fps             | 574      |\n",
      "|    iterations      | 32       |\n",
      "|    time_elapsed    | 561      |\n",
      "|    total_timesteps | 322560   |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=342720, episode_reward=-0.08 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0834     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 342720      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013713971 |\n",
      "|    clip_fraction        | 0.0719      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.97        |\n",
      "|    explained_variance   | 0.166       |\n",
      "|    learning_rate        | 0.00445     |\n",
      "|    loss                 | -0.00492    |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.00425    |\n",
      "|    std                  | 0.0615      |\n",
      "|    value_loss           | 1.52e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.385   |\n",
      "| time/              |          |\n",
      "|    fps             | 572      |\n",
      "|    iterations      | 34       |\n",
      "|    time_elapsed    | 598      |\n",
      "|    total_timesteps | 342720   |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=362880, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.093     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 362880     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01508754 |\n",
      "|    clip_fraction        | 0.0967     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 3.3        |\n",
      "|    explained_variance   | 0.329      |\n",
      "|    learning_rate        | 0.00441    |\n",
      "|    loss                 | -0.00366   |\n",
      "|    n_updates            | 350        |\n",
      "|    policy_gradient_loss | -0.00491   |\n",
      "|    std                  | 0.0517     |\n",
      "|    value_loss           | 5.57e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.325   |\n",
      "| time/              |          |\n",
      "|    fps             | 572      |\n",
      "|    iterations      | 36       |\n",
      "|    time_elapsed    | 634      |\n",
      "|    total_timesteps | 362880   |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=383040, episode_reward=-0.12 +/- 0.07\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.117       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 383040       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0111886365 |\n",
      "|    clip_fraction        | 0.106        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 3.6          |\n",
      "|    explained_variance   | 0.307        |\n",
      "|    learning_rate        | 0.00438      |\n",
      "|    loss                 | -0.0106      |\n",
      "|    n_updates            | 370          |\n",
      "|    policy_gradient_loss | -0.00558     |\n",
      "|    std                  | 0.0442       |\n",
      "|    value_loss           | 2.66e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.284   |\n",
      "| time/              |          |\n",
      "|    fps             | 572      |\n",
      "|    iterations      | 38       |\n",
      "|    time_elapsed    | 669      |\n",
      "|    total_timesteps | 383040   |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=403200, episode_reward=-0.10 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0968     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 403200      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021990787 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.91        |\n",
      "|    explained_variance   | 0.0599      |\n",
      "|    learning_rate        | 0.00434     |\n",
      "|    loss                 | -0.0118     |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.00923    |\n",
      "|    std                  | 0.0374      |\n",
      "|    value_loss           | 2.74e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.244   |\n",
      "| time/              |          |\n",
      "|    fps             | 573      |\n",
      "|    iterations      | 40       |\n",
      "|    time_elapsed    | 702      |\n",
      "|    total_timesteps | 403200   |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=423360, episode_reward=-0.10 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0993     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 423360      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010253478 |\n",
      "|    clip_fraction        | 0.1         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.13        |\n",
      "|    explained_variance   | 0.343       |\n",
      "|    learning_rate        | 0.00431     |\n",
      "|    loss                 | -0.00188    |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.000345   |\n",
      "|    std                  | 0.0344      |\n",
      "|    value_loss           | 4.05e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.207   |\n",
      "| time/              |          |\n",
      "|    fps             | 575      |\n",
      "|    iterations      | 42       |\n",
      "|    time_elapsed    | 736      |\n",
      "|    total_timesteps | 423360   |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=443520, episode_reward=-0.12 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.119      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 443520      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019580862 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.33        |\n",
      "|    explained_variance   | 0.0935      |\n",
      "|    learning_rate        | 0.00428     |\n",
      "|    loss                 | -0.00541    |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.00111    |\n",
      "|    std                  | 0.0314      |\n",
      "|    value_loss           | 5.37e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.185   |\n",
      "| time/              |          |\n",
      "|    fps             | 573      |\n",
      "|    iterations      | 44       |\n",
      "|    time_elapsed    | 773      |\n",
      "|    total_timesteps | 443520   |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=463680, episode_reward=-0.09 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0875     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 463680      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010271959 |\n",
      "|    clip_fraction        | 0.0792      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.52        |\n",
      "|    explained_variance   | 0.201       |\n",
      "|    learning_rate        | 0.00424     |\n",
      "|    loss                 | -0.00683    |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.00361    |\n",
      "|    std                  | 0.0281      |\n",
      "|    value_loss           | 4.16e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.166   |\n",
      "| time/              |          |\n",
      "|    fps             | 572      |\n",
      "|    iterations      | 46       |\n",
      "|    time_elapsed    | 810      |\n",
      "|    total_timesteps | 463680   |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=483840, episode_reward=-0.10 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.104      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 483840      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015522249 |\n",
      "|    clip_fraction        | 0.105       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.75        |\n",
      "|    explained_variance   | -0.0182     |\n",
      "|    learning_rate        | 0.00421     |\n",
      "|    loss                 | -0.00695    |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.0053     |\n",
      "|    std                  | 0.0253      |\n",
      "|    value_loss           | 1.43e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.155   |\n",
      "| time/              |          |\n",
      "|    fps             | 571      |\n",
      "|    iterations      | 48       |\n",
      "|    time_elapsed    | 847      |\n",
      "|    total_timesteps | 483840   |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=504000, episode_reward=-0.10 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0993      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 504000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0143324975 |\n",
      "|    clip_fraction        | 0.133        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.86         |\n",
      "|    explained_variance   | -0.0144      |\n",
      "|    learning_rate        | 0.00418      |\n",
      "|    loss                 | 0.00317      |\n",
      "|    n_updates            | 490          |\n",
      "|    policy_gradient_loss | 0.000578     |\n",
      "|    std                  | 0.0243       |\n",
      "|    value_loss           | 1.04e-06     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.148   |\n",
      "| time/              |          |\n",
      "|    fps             | 571      |\n",
      "|    iterations      | 50       |\n",
      "|    time_elapsed    | 881      |\n",
      "|    total_timesteps | 504000   |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=524160, episode_reward=-0.09 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0864     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 524160      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014204554 |\n",
      "|    clip_fraction        | 0.0858      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.01        |\n",
      "|    explained_variance   | 0.143       |\n",
      "|    learning_rate        | 0.00414     |\n",
      "|    loss                 | 0.00723     |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | 0.000132    |\n",
      "|    std                  | 0.0222      |\n",
      "|    value_loss           | 2.46e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.141   |\n",
      "| time/              |          |\n",
      "|    fps             | 570      |\n",
      "|    iterations      | 52       |\n",
      "|    time_elapsed    | 919      |\n",
      "|    total_timesteps | 524160   |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=544320, episode_reward=-0.08 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.082      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 544320      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011325307 |\n",
      "|    clip_fraction        | 0.0646      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.16        |\n",
      "|    explained_variance   | 0.142       |\n",
      "|    learning_rate        | 0.00411     |\n",
      "|    loss                 | 4.57e-05    |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | 0.000324    |\n",
      "|    std                  | 0.0208      |\n",
      "|    value_loss           | 4.31e-07    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.132   |\n",
      "| time/              |          |\n",
      "|    fps             | 571      |\n",
      "|    iterations      | 54       |\n",
      "|    time_elapsed    | 953      |\n",
      "|    total_timesteps | 544320   |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=564480, episode_reward=-0.09 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0925      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 564480       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050616534 |\n",
      "|    clip_fraction        | 0.0611       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.28         |\n",
      "|    explained_variance   | 0.122        |\n",
      "|    learning_rate        | 0.00408      |\n",
      "|    loss                 | 0.00295      |\n",
      "|    n_updates            | 550          |\n",
      "|    policy_gradient_loss | -0.000621    |\n",
      "|    std                  | 0.0195       |\n",
      "|    value_loss           | 1.33e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.124   |\n",
      "| time/              |          |\n",
      "|    fps             | 574      |\n",
      "|    iterations      | 56       |\n",
      "|    time_elapsed    | 982      |\n",
      "|    total_timesteps | 564480   |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=584640, episode_reward=-0.10 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0981     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 584640      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006959467 |\n",
      "|    clip_fraction        | 0.0682      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.45        |\n",
      "|    explained_variance   | 0.135       |\n",
      "|    learning_rate        | 0.00404     |\n",
      "|    loss                 | -0.00195    |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | -0.00148    |\n",
      "|    std                  | 0.0179      |\n",
      "|    value_loss           | 1.43e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.12    |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 58       |\n",
      "|    time_elapsed    | 1011     |\n",
      "|    total_timesteps | 584640   |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=604800, episode_reward=-0.09 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0866     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 604800      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014497215 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.55        |\n",
      "|    explained_variance   | 0.187       |\n",
      "|    learning_rate        | 0.00401     |\n",
      "|    loss                 | 0.0081      |\n",
      "|    n_updates            | 590         |\n",
      "|    policy_gradient_loss | 0.00138     |\n",
      "|    std                  | 0.0171      |\n",
      "|    value_loss           | 1.86e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.115   |\n",
      "| time/              |          |\n",
      "|    fps             | 581      |\n",
      "|    iterations      | 60       |\n",
      "|    time_elapsed    | 1040     |\n",
      "|    total_timesteps | 604800   |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=624960, episode_reward=-0.11 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.11       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 624960      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007061265 |\n",
      "|    clip_fraction        | 0.0502      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.69        |\n",
      "|    explained_variance   | -0.114      |\n",
      "|    learning_rate        | 0.00398     |\n",
      "|    loss                 | 0.00231     |\n",
      "|    n_updates            | 610         |\n",
      "|    policy_gradient_loss | -0.00143    |\n",
      "|    std                  | 0.0158      |\n",
      "|    value_loss           | 5.54e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.109   |\n",
      "| time/              |          |\n",
      "|    fps             | 584      |\n",
      "|    iterations      | 62       |\n",
      "|    time_elapsed    | 1068     |\n",
      "|    total_timesteps | 624960   |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=645120, episode_reward=-0.09 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0853     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 645120      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008407021 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.77        |\n",
      "|    explained_variance   | 0.356       |\n",
      "|    learning_rate        | 0.00394     |\n",
      "|    loss                 | -0.000159   |\n",
      "|    n_updates            | 630         |\n",
      "|    policy_gradient_loss | 0.000278    |\n",
      "|    std                  | 0.0155      |\n",
      "|    value_loss           | 2.11e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.105   |\n",
      "| time/              |          |\n",
      "|    fps             | 587      |\n",
      "|    iterations      | 64       |\n",
      "|    time_elapsed    | 1097     |\n",
      "|    total_timesteps | 645120   |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=665280, episode_reward=-0.11 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.108       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 665280       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018922293 |\n",
      "|    clip_fraction        | 0.146        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.81         |\n",
      "|    explained_variance   | 0.0322       |\n",
      "|    learning_rate        | 0.00391      |\n",
      "|    loss                 | -0.000329    |\n",
      "|    n_updates            | 650          |\n",
      "|    policy_gradient_loss | 0.00461      |\n",
      "|    std                  | 0.0152       |\n",
      "|    value_loss           | 9.49e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.102   |\n",
      "| time/              |          |\n",
      "|    fps             | 590      |\n",
      "|    iterations      | 66       |\n",
      "|    time_elapsed    | 1126     |\n",
      "|    total_timesteps | 665280   |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=685440, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0925      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 685440       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067784013 |\n",
      "|    clip_fraction        | 0.105        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.88         |\n",
      "|    explained_variance   | -0.156       |\n",
      "|    learning_rate        | 0.00387      |\n",
      "|    loss                 | -0.00384     |\n",
      "|    n_updates            | 670          |\n",
      "|    policy_gradient_loss | 0.00248      |\n",
      "|    std                  | 0.0146       |\n",
      "|    value_loss           | 1.9e-07      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.102   |\n",
      "| time/              |          |\n",
      "|    fps             | 593      |\n",
      "|    iterations      | 68       |\n",
      "|    time_elapsed    | 1155     |\n",
      "|    total_timesteps | 685440   |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=705600, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0846     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 705600      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012839217 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.94        |\n",
      "|    explained_variance   | 0.114       |\n",
      "|    learning_rate        | 0.00384     |\n",
      "|    loss                 | 0.00712     |\n",
      "|    n_updates            | 690         |\n",
      "|    policy_gradient_loss | 0.00321     |\n",
      "|    std                  | 0.0143      |\n",
      "|    value_loss           | 4.22e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.103   |\n",
      "| time/              |          |\n",
      "|    fps             | 595      |\n",
      "|    iterations      | 70       |\n",
      "|    time_elapsed    | 1184     |\n",
      "|    total_timesteps | 705600   |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=725760, episode_reward=-0.08 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0847     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 725760      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005376651 |\n",
      "|    clip_fraction        | 0.089       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6           |\n",
      "|    explained_variance   | 0.0948      |\n",
      "|    learning_rate        | 0.00381     |\n",
      "|    loss                 | -0.00123    |\n",
      "|    n_updates            | 710         |\n",
      "|    policy_gradient_loss | -7.83e-05   |\n",
      "|    std                  | 0.0139      |\n",
      "|    value_loss           | 2.31e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.106   |\n",
      "| time/              |          |\n",
      "|    fps             | 598      |\n",
      "|    iterations      | 72       |\n",
      "|    time_elapsed    | 1213     |\n",
      "|    total_timesteps | 725760   |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=745920, episode_reward=-0.09 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0879     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 745920      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004366426 |\n",
      "|    clip_fraction        | 0.265       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.04        |\n",
      "|    explained_variance   | -0.297      |\n",
      "|    learning_rate        | 0.00377     |\n",
      "|    loss                 | -0.000658   |\n",
      "|    n_updates            | 730         |\n",
      "|    policy_gradient_loss | 0.0135      |\n",
      "|    std                  | 0.0136      |\n",
      "|    value_loss           | 1.34e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.106   |\n",
      "| time/              |          |\n",
      "|    fps             | 600      |\n",
      "|    iterations      | 74       |\n",
      "|    time_elapsed    | 1243     |\n",
      "|    total_timesteps | 745920   |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=766080, episode_reward=-0.09 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0937      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 766080       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075597665 |\n",
      "|    clip_fraction        | 0.169        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.13         |\n",
      "|    explained_variance   | -0.0624      |\n",
      "|    learning_rate        | 0.00374      |\n",
      "|    loss                 | 0.00518      |\n",
      "|    n_updates            | 750          |\n",
      "|    policy_gradient_loss | 0.00513      |\n",
      "|    std                  | 0.013        |\n",
      "|    value_loss           | 8.58e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.105   |\n",
      "| time/              |          |\n",
      "|    fps             | 602      |\n",
      "|    iterations      | 76       |\n",
      "|    time_elapsed    | 1271     |\n",
      "|    total_timesteps | 766080   |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=786240, episode_reward=-0.11 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.112     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 786240     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01043112 |\n",
      "|    clip_fraction        | 0.121      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 6.2        |\n",
      "|    explained_variance   | -0.0692    |\n",
      "|    learning_rate        | 0.00371    |\n",
      "|    loss                 | 0.00588    |\n",
      "|    n_updates            | 770        |\n",
      "|    policy_gradient_loss | 0.00337    |\n",
      "|    std                  | 0.0126     |\n",
      "|    value_loss           | 1.28e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.106   |\n",
      "| time/              |          |\n",
      "|    fps             | 604      |\n",
      "|    iterations      | 78       |\n",
      "|    time_elapsed    | 1300     |\n",
      "|    total_timesteps | 786240   |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=806400, episode_reward=-0.09 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0888    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 806400     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02158076 |\n",
      "|    clip_fraction        | 0.196      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 6.26       |\n",
      "|    explained_variance   | 0.442      |\n",
      "|    learning_rate        | 0.00367    |\n",
      "|    loss                 | 0.0164     |\n",
      "|    n_updates            | 790        |\n",
      "|    policy_gradient_loss | 0.0065     |\n",
      "|    std                  | 0.0123     |\n",
      "|    value_loss           | 7.64e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.104   |\n",
      "| time/              |          |\n",
      "|    fps             | 606      |\n",
      "|    iterations      | 80       |\n",
      "|    time_elapsed    | 1329     |\n",
      "|    total_timesteps | 806400   |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=826560, episode_reward=-0.11 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.107      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 826560      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011214424 |\n",
      "|    clip_fraction        | 0.18        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.27        |\n",
      "|    explained_variance   | 0.16        |\n",
      "|    learning_rate        | 0.00364     |\n",
      "|    loss                 | 0.00103     |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | 0.00548     |\n",
      "|    std                  | 0.0122      |\n",
      "|    value_loss           | 1.49e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.107   |\n",
      "| time/              |          |\n",
      "|    fps             | 608      |\n",
      "|    iterations      | 82       |\n",
      "|    time_elapsed    | 1358     |\n",
      "|    total_timesteps | 826560   |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=846720, episode_reward=-0.11 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.113      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 846720      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004595723 |\n",
      "|    clip_fraction        | 0.156       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.34        |\n",
      "|    explained_variance   | 0.371       |\n",
      "|    learning_rate        | 0.00361     |\n",
      "|    loss                 | 0.00063     |\n",
      "|    n_updates            | 830         |\n",
      "|    policy_gradient_loss | 0.00249     |\n",
      "|    std                  | 0.0117      |\n",
      "|    value_loss           | 5.45e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.109   |\n",
      "| time/              |          |\n",
      "|    fps             | 610      |\n",
      "|    iterations      | 84       |\n",
      "|    time_elapsed    | 1387     |\n",
      "|    total_timesteps | 846720   |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=866880, episode_reward=-0.11 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.107      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 866880      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012183099 |\n",
      "|    clip_fraction        | 0.153       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.37        |\n",
      "|    explained_variance   | 0.416       |\n",
      "|    learning_rate        | 0.00357     |\n",
      "|    loss                 | -0.00256    |\n",
      "|    n_updates            | 850         |\n",
      "|    policy_gradient_loss | 0.00143     |\n",
      "|    std                  | 0.0115      |\n",
      "|    value_loss           | 1.38e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.109   |\n",
      "| time/              |          |\n",
      "|    fps             | 611      |\n",
      "|    iterations      | 86       |\n",
      "|    time_elapsed    | 1416     |\n",
      "|    total_timesteps | 866880   |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=887040, episode_reward=-0.08 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0823     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 887040      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010644476 |\n",
      "|    clip_fraction        | 0.222       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.42        |\n",
      "|    explained_variance   | -0.295      |\n",
      "|    learning_rate        | 0.00354     |\n",
      "|    loss                 | 0.000271    |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | 0.0101      |\n",
      "|    std                  | 0.0112      |\n",
      "|    value_loss           | 2.21e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.109   |\n",
      "| time/              |          |\n",
      "|    fps             | 613      |\n",
      "|    iterations      | 88       |\n",
      "|    time_elapsed    | 1445     |\n",
      "|    total_timesteps | 887040   |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=907200, episode_reward=-0.09 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0903      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 907200       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026398944 |\n",
      "|    clip_fraction        | 0.231        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.46         |\n",
      "|    explained_variance   | 0.267        |\n",
      "|    learning_rate        | 0.0035       |\n",
      "|    loss                 | -0.0005      |\n",
      "|    n_updates            | 890          |\n",
      "|    policy_gradient_loss | 0.0116       |\n",
      "|    std                  | 0.011        |\n",
      "|    value_loss           | 6.66e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.111   |\n",
      "| time/              |          |\n",
      "|    fps             | 615      |\n",
      "|    iterations      | 90       |\n",
      "|    time_elapsed    | 1474     |\n",
      "|    total_timesteps | 907200   |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=927360, episode_reward=-0.11 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.107      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 927360      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031293474 |\n",
      "|    clip_fraction        | 0.204       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.49        |\n",
      "|    explained_variance   | -0.0479     |\n",
      "|    learning_rate        | 0.00347     |\n",
      "|    loss                 | -0.0013     |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | 0.00932     |\n",
      "|    std                  | 0.0109      |\n",
      "|    value_loss           | 1.29e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.11    |\n",
      "| time/              |          |\n",
      "|    fps             | 616      |\n",
      "|    iterations      | 92       |\n",
      "|    time_elapsed    | 1503     |\n",
      "|    total_timesteps | 927360   |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=947520, episode_reward=-0.07 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.071      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 947520      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008346019 |\n",
      "|    clip_fraction        | 0.236       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.53        |\n",
      "|    explained_variance   | -0.0614     |\n",
      "|    learning_rate        | 0.00344     |\n",
      "|    loss                 | 0.00284     |\n",
      "|    n_updates            | 930         |\n",
      "|    policy_gradient_loss | 0.0135      |\n",
      "|    std                  | 0.0106      |\n",
      "|    value_loss           | 7.53e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.105   |\n",
      "| time/              |          |\n",
      "|    fps             | 617      |\n",
      "|    iterations      | 94       |\n",
      "|    time_elapsed    | 1535     |\n",
      "|    total_timesteps | 947520   |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=967680, episode_reward=-0.09 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0887     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 967680      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015597451 |\n",
      "|    clip_fraction        | 0.167       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.56        |\n",
      "|    explained_variance   | 0.179       |\n",
      "|    learning_rate        | 0.0034      |\n",
      "|    loss                 | 0.00565     |\n",
      "|    n_updates            | 950         |\n",
      "|    policy_gradient_loss | 0.0042      |\n",
      "|    std                  | 0.0105      |\n",
      "|    value_loss           | 2.69e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.102   |\n",
      "| time/              |          |\n",
      "|    fps             | 616      |\n",
      "|    iterations      | 96       |\n",
      "|    time_elapsed    | 1569     |\n",
      "|    total_timesteps | 967680   |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=987840, episode_reward=-0.12 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.12      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 987840     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02143491 |\n",
      "|    clip_fraction        | 0.157      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 6.59       |\n",
      "|    explained_variance   | 0.355      |\n",
      "|    learning_rate        | 0.00337    |\n",
      "|    loss                 | 0.00395    |\n",
      "|    n_updates            | 970        |\n",
      "|    policy_gradient_loss | 0.00569    |\n",
      "|    std                  | 0.0103     |\n",
      "|    value_loss           | 1.38e-05   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0989  |\n",
      "| time/              |          |\n",
      "|    fps             | 616      |\n",
      "|    iterations      | 98       |\n",
      "|    time_elapsed    | 1603     |\n",
      "|    total_timesteps | 987840   |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=1008000, episode_reward=-0.10 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.101      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1008000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005182137 |\n",
      "|    clip_fraction        | 0.211       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.64        |\n",
      "|    explained_variance   | 0.0326      |\n",
      "|    learning_rate        | 0.00334     |\n",
      "|    loss                 | -0.00279    |\n",
      "|    n_updates            | 990         |\n",
      "|    policy_gradient_loss | 0.00748     |\n",
      "|    std                  | 0.0102      |\n",
      "|    value_loss           | 2.76e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0987  |\n",
      "| time/              |          |\n",
      "|    fps             | 616      |\n",
      "|    iterations      | 100      |\n",
      "|    time_elapsed    | 1634     |\n",
      "|    total_timesteps | 1008000  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=1028160, episode_reward=-0.12 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.117     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1028160    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01948372 |\n",
      "|    clip_fraction        | 0.18       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 6.66       |\n",
      "|    explained_variance   | -0.356     |\n",
      "|    learning_rate        | 0.0033     |\n",
      "|    loss                 | 0.018      |\n",
      "|    n_updates            | 1010       |\n",
      "|    policy_gradient_loss | 0.0054     |\n",
      "|    std                  | 0.0101     |\n",
      "|    value_loss           | 7.4e-07    |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0976  |\n",
      "| time/              |          |\n",
      "|    fps             | 616      |\n",
      "|    iterations      | 102      |\n",
      "|    time_elapsed    | 1668     |\n",
      "|    total_timesteps | 1028160  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=1048320, episode_reward=-0.07 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0745     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1048320     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003163851 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.7         |\n",
      "|    explained_variance   | 0.12        |\n",
      "|    learning_rate        | 0.00327     |\n",
      "|    loss                 | -0.00162    |\n",
      "|    n_updates            | 1030        |\n",
      "|    policy_gradient_loss | -0.000762   |\n",
      "|    std                  | 0.00995     |\n",
      "|    value_loss           | 7.27e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0969  |\n",
      "| time/              |          |\n",
      "|    fps             | 616      |\n",
      "|    iterations      | 104      |\n",
      "|    time_elapsed    | 1699     |\n",
      "|    total_timesteps | 1048320  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=1068480, episode_reward=-0.09 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0852      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1068480      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049797604 |\n",
      "|    clip_fraction        | 0.149        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.76         |\n",
      "|    explained_variance   | -0.097       |\n",
      "|    learning_rate        | 0.00324      |\n",
      "|    loss                 | 0.000163     |\n",
      "|    n_updates            | 1050         |\n",
      "|    policy_gradient_loss | 0.00168      |\n",
      "|    std                  | 0.00959      |\n",
      "|    value_loss           | 2.13e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0986  |\n",
      "| time/              |          |\n",
      "|    fps             | 616      |\n",
      "|    iterations      | 106      |\n",
      "|    time_elapsed    | 1733     |\n",
      "|    total_timesteps | 1068480  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=1088640, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0855    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1088640    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01004198 |\n",
      "|    clip_fraction        | 0.199      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 6.82       |\n",
      "|    explained_variance   | 0.178      |\n",
      "|    learning_rate        | 0.0032     |\n",
      "|    loss                 | 0.0135     |\n",
      "|    n_updates            | 1070       |\n",
      "|    policy_gradient_loss | 0.00765    |\n",
      "|    std                  | 0.0093     |\n",
      "|    value_loss           | 4.05e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0934  |\n",
      "| time/              |          |\n",
      "|    fps             | 616      |\n",
      "|    iterations      | 108      |\n",
      "|    time_elapsed    | 1765     |\n",
      "|    total_timesteps | 1088640  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=1108800, episode_reward=-0.09 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0852      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1108800      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062474017 |\n",
      "|    clip_fraction        | 0.123        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.86         |\n",
      "|    explained_variance   | 0.218        |\n",
      "|    learning_rate        | 0.00317      |\n",
      "|    loss                 | -0.000887    |\n",
      "|    n_updates            | 1090         |\n",
      "|    policy_gradient_loss | 0.00214      |\n",
      "|    std                  | 0.00914      |\n",
      "|    value_loss           | 3.94e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0936  |\n",
      "| time/              |          |\n",
      "|    fps             | 616      |\n",
      "|    iterations      | 110      |\n",
      "|    time_elapsed    | 1798     |\n",
      "|    total_timesteps | 1108800  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=1128960, episode_reward=-0.11 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.113       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1128960      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073104994 |\n",
      "|    clip_fraction        | 0.132        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.91         |\n",
      "|    explained_variance   | -0.179       |\n",
      "|    learning_rate        | 0.00314      |\n",
      "|    loss                 | -0.000732    |\n",
      "|    n_updates            | 1110         |\n",
      "|    policy_gradient_loss | 0.00322      |\n",
      "|    std                  | 0.00892      |\n",
      "|    value_loss           | 1.37e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0947  |\n",
      "| time/              |          |\n",
      "|    fps             | 616      |\n",
      "|    iterations      | 112      |\n",
      "|    time_elapsed    | 1831     |\n",
      "|    total_timesteps | 1128960  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=1149120, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0943     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1149120     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011578424 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.93        |\n",
      "|    explained_variance   | 0.2         |\n",
      "|    learning_rate        | 0.0031      |\n",
      "|    loss                 | 0.0167      |\n",
      "|    n_updates            | 1130        |\n",
      "|    policy_gradient_loss | 0.00285     |\n",
      "|    std                  | 0.00888     |\n",
      "|    value_loss           | 2.04e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0943  |\n",
      "| time/              |          |\n",
      "|    fps             | 616      |\n",
      "|    iterations      | 114      |\n",
      "|    time_elapsed    | 1863     |\n",
      "|    total_timesteps | 1149120  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=1169280, episode_reward=-0.08 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0812    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1169280    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02048703 |\n",
      "|    clip_fraction        | 0.219      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 6.95       |\n",
      "|    explained_variance   | 0.2        |\n",
      "|    learning_rate        | 0.00307    |\n",
      "|    loss                 | 0.000532   |\n",
      "|    n_updates            | 1150       |\n",
      "|    policy_gradient_loss | 0.00705    |\n",
      "|    std                  | 0.00872    |\n",
      "|    value_loss           | 1.8e-07    |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0932  |\n",
      "| time/              |          |\n",
      "|    fps             | 616      |\n",
      "|    iterations      | 116      |\n",
      "|    time_elapsed    | 1896     |\n",
      "|    total_timesteps | 1169280  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=1189440, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0841    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1189440    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00698193 |\n",
      "|    clip_fraction        | 0.107      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 7          |\n",
      "|    explained_variance   | 0.0599     |\n",
      "|    learning_rate        | 0.00303    |\n",
      "|    loss                 | 0.00237    |\n",
      "|    n_updates            | 1170       |\n",
      "|    policy_gradient_loss | 0.000166   |\n",
      "|    std                  | 0.00851    |\n",
      "|    value_loss           | 2.81e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0934  |\n",
      "| time/              |          |\n",
      "|    fps             | 616      |\n",
      "|    iterations      | 118      |\n",
      "|    time_elapsed    | 1930     |\n",
      "|    total_timesteps | 1189440  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=1209600, episode_reward=-0.10 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0971      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1209600      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054042735 |\n",
      "|    clip_fraction        | 0.128        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 7.05         |\n",
      "|    explained_variance   | 0.0486       |\n",
      "|    learning_rate        | 0.003        |\n",
      "|    loss                 | 0.00147      |\n",
      "|    n_updates            | 1190         |\n",
      "|    policy_gradient_loss | 0.0042       |\n",
      "|    std                  | 0.00829      |\n",
      "|    value_loss           | 1.33e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0982  |\n",
      "| time/              |          |\n",
      "|    fps             | 616      |\n",
      "|    iterations      | 120      |\n",
      "|    time_elapsed    | 1961     |\n",
      "|    total_timesteps | 1209600  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=1229760, episode_reward=-0.11 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.107       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1229760      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040642144 |\n",
      "|    clip_fraction        | 0.0715       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 7.08         |\n",
      "|    explained_variance   | 0.25         |\n",
      "|    learning_rate        | 0.00297      |\n",
      "|    loss                 | -0.000548    |\n",
      "|    n_updates            | 1210         |\n",
      "|    policy_gradient_loss | 0.000149     |\n",
      "|    std                  | 0.00808      |\n",
      "|    value_loss           | 9.54e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.101   |\n",
      "| time/              |          |\n",
      "|    fps             | 616      |\n",
      "|    iterations      | 122      |\n",
      "|    time_elapsed    | 1994     |\n",
      "|    total_timesteps | 1229760  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=1249920, episode_reward=-0.10 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0985     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1249920     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004764148 |\n",
      "|    clip_fraction        | 0.123       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.15        |\n",
      "|    explained_variance   | 0.0417      |\n",
      "|    learning_rate        | 0.00293     |\n",
      "|    loss                 | -0.0015     |\n",
      "|    n_updates            | 1230        |\n",
      "|    policy_gradient_loss | 8.81e-05    |\n",
      "|    std                  | 0.00782     |\n",
      "|    value_loss           | 5.25e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0995  |\n",
      "| time/              |          |\n",
      "|    fps             | 612      |\n",
      "|    iterations      | 124      |\n",
      "|    time_elapsed    | 2040     |\n",
      "|    total_timesteps | 1249920  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=1270080, episode_reward=-0.10 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0976     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1270080     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023933757 |\n",
      "|    clip_fraction        | 0.168       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.18        |\n",
      "|    explained_variance   | 0.0692      |\n",
      "|    learning_rate        | 0.0029      |\n",
      "|    loss                 | 0.0173      |\n",
      "|    n_updates            | 1250        |\n",
      "|    policy_gradient_loss | 0.00579     |\n",
      "|    std                  | 0.0077      |\n",
      "|    value_loss           | 8.7e-08     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0979  |\n",
      "| time/              |          |\n",
      "|    fps             | 609      |\n",
      "|    iterations      | 126      |\n",
      "|    time_elapsed    | 2083     |\n",
      "|    total_timesteps | 1270080  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=1290240, episode_reward=-0.12 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.122     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1290240    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03180426 |\n",
      "|    clip_fraction        | 0.243      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 7.21       |\n",
      "|    explained_variance   | 0.0811     |\n",
      "|    learning_rate        | 0.00287    |\n",
      "|    loss                 | 0.012      |\n",
      "|    n_updates            | 1270       |\n",
      "|    policy_gradient_loss | 0.0147     |\n",
      "|    std                  | 0.00765    |\n",
      "|    value_loss           | 6.62e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.098   |\n",
      "| time/              |          |\n",
      "|    fps             | 607      |\n",
      "|    iterations      | 128      |\n",
      "|    time_elapsed    | 2122     |\n",
      "|    total_timesteps | 1290240  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=1310400, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0897      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1310400      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009560783 |\n",
      "|    clip_fraction        | 0.145        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 7.24         |\n",
      "|    explained_variance   | -0.0254      |\n",
      "|    learning_rate        | 0.00283      |\n",
      "|    loss                 | 0.000926     |\n",
      "|    n_updates            | 1290         |\n",
      "|    policy_gradient_loss | 0.00222      |\n",
      "|    std                  | 0.00755      |\n",
      "|    value_loss           | 2.38e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.096   |\n",
      "| time/              |          |\n",
      "|    fps             | 606      |\n",
      "|    iterations      | 130      |\n",
      "|    time_elapsed    | 2161     |\n",
      "|    total_timesteps | 1310400  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=1330560, episode_reward=-0.09 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0903    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1330560    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01424801 |\n",
      "|    clip_fraction        | 0.26       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 7.28       |\n",
      "|    explained_variance   | 0.0504     |\n",
      "|    learning_rate        | 0.0028     |\n",
      "|    loss                 | -0.000384  |\n",
      "|    n_updates            | 1310       |\n",
      "|    policy_gradient_loss | 0.0119     |\n",
      "|    std                  | 0.00744    |\n",
      "|    value_loss           | 9.15e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0966  |\n",
      "| time/              |          |\n",
      "|    fps             | 604      |\n",
      "|    iterations      | 132      |\n",
      "|    time_elapsed    | 2201     |\n",
      "|    total_timesteps | 1330560  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=1350720, episode_reward=-0.10 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0967     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1350720     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.051494315 |\n",
      "|    clip_fraction        | 0.201       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.3         |\n",
      "|    explained_variance   | 0.224       |\n",
      "|    learning_rate        | 0.00277     |\n",
      "|    loss                 | 0.0291      |\n",
      "|    n_updates            | 1330        |\n",
      "|    policy_gradient_loss | 0.00838     |\n",
      "|    std                  | 0.00732     |\n",
      "|    value_loss           | 1.12e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.092   |\n",
      "| time/              |          |\n",
      "|    fps             | 602      |\n",
      "|    iterations      | 134      |\n",
      "|    time_elapsed    | 2241     |\n",
      "|    total_timesteps | 1350720  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=1370880, episode_reward=-0.10 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0963      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1370880      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047503337 |\n",
      "|    clip_fraction        | 0.093        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 7.33         |\n",
      "|    explained_variance   | 0.0943       |\n",
      "|    learning_rate        | 0.00273      |\n",
      "|    loss                 | -0.00204     |\n",
      "|    n_updates            | 1350         |\n",
      "|    policy_gradient_loss | 0.000334     |\n",
      "|    std                  | 0.00719      |\n",
      "|    value_loss           | 5.06e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0932  |\n",
      "| time/              |          |\n",
      "|    fps             | 601      |\n",
      "|    iterations      | 136      |\n",
      "|    time_elapsed    | 2280     |\n",
      "|    total_timesteps | 1370880  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=1391040, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0752      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1391040      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033341567 |\n",
      "|    clip_fraction        | 0.168        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 7.36         |\n",
      "|    explained_variance   | -0.0425      |\n",
      "|    learning_rate        | 0.0027       |\n",
      "|    loss                 | 0.00139      |\n",
      "|    n_updates            | 1370         |\n",
      "|    policy_gradient_loss | 0.00732      |\n",
      "|    std                  | 0.00714      |\n",
      "|    value_loss           | 1.56e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0942  |\n",
      "| time/              |          |\n",
      "|    fps             | 599      |\n",
      "|    iterations      | 138      |\n",
      "|    time_elapsed    | 2320     |\n",
      "|    total_timesteps | 1391040  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=1411200, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0907     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1411200     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009650187 |\n",
      "|    clip_fraction        | 0.15        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.37        |\n",
      "|    explained_variance   | 0.117       |\n",
      "|    learning_rate        | 0.00266     |\n",
      "|    loss                 | -0.00249    |\n",
      "|    n_updates            | 1390        |\n",
      "|    policy_gradient_loss | 0.00169     |\n",
      "|    std                  | 0.00705     |\n",
      "|    value_loss           | 3.64e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0946  |\n",
      "| time/              |          |\n",
      "|    fps             | 598      |\n",
      "|    iterations      | 140      |\n",
      "|    time_elapsed    | 2359     |\n",
      "|    total_timesteps | 1411200  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=1431360, episode_reward=-0.11 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.108      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1431360     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008647596 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.38        |\n",
      "|    explained_variance   | 0.156       |\n",
      "|    learning_rate        | 0.00263     |\n",
      "|    loss                 | 0.00247     |\n",
      "|    n_updates            | 1410        |\n",
      "|    policy_gradient_loss | 0.00369     |\n",
      "|    std                  | 0.00707     |\n",
      "|    value_loss           | 1.9e-05     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0952  |\n",
      "| time/              |          |\n",
      "|    fps             | 595      |\n",
      "|    iterations      | 142      |\n",
      "|    time_elapsed    | 2404     |\n",
      "|    total_timesteps | 1431360  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=1451520, episode_reward=-0.12 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.118     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1451520    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01433434 |\n",
      "|    clip_fraction        | 0.168      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 7.39       |\n",
      "|    explained_variance   | -2.89      |\n",
      "|    learning_rate        | 0.0026     |\n",
      "|    loss                 | 0.0062     |\n",
      "|    n_updates            | 1430       |\n",
      "|    policy_gradient_loss | 0.00403    |\n",
      "|    std                  | 0.007      |\n",
      "|    value_loss           | 1.03e-05   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0947  |\n",
      "| time/              |          |\n",
      "|    fps             | 593      |\n",
      "|    iterations      | 144      |\n",
      "|    time_elapsed    | 2445     |\n",
      "|    total_timesteps | 1451520  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=1471680, episode_reward=-0.12 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.119      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1471680     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020371009 |\n",
      "|    clip_fraction        | 0.147       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.4         |\n",
      "|    explained_variance   | 0.294       |\n",
      "|    learning_rate        | 0.00256     |\n",
      "|    loss                 | 0.0114      |\n",
      "|    n_updates            | 1450        |\n",
      "|    policy_gradient_loss | 0.00321     |\n",
      "|    std                  | 0.00701     |\n",
      "|    value_loss           | 0.000208    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0908  |\n",
      "| time/              |          |\n",
      "|    fps             | 592      |\n",
      "|    iterations      | 146      |\n",
      "|    time_elapsed    | 2484     |\n",
      "|    total_timesteps | 1471680  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=1491840, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0854     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1491840     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031158835 |\n",
      "|    clip_fraction        | 0.312       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.43        |\n",
      "|    explained_variance   | 0.0482      |\n",
      "|    learning_rate        | 0.00253     |\n",
      "|    loss                 | 0.021       |\n",
      "|    n_updates            | 1470        |\n",
      "|    policy_gradient_loss | 0.0164      |\n",
      "|    std                  | 0.00692     |\n",
      "|    value_loss           | 1.02e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0948  |\n",
      "| time/              |          |\n",
      "|    fps             | 591      |\n",
      "|    iterations      | 148      |\n",
      "|    time_elapsed    | 2523     |\n",
      "|    total_timesteps | 1491840  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=1512000, episode_reward=-0.10 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0963     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1512000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012390937 |\n",
      "|    clip_fraction        | 0.247       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.44        |\n",
      "|    explained_variance   | 0.143       |\n",
      "|    learning_rate        | 0.0025      |\n",
      "|    loss                 | 0.00502     |\n",
      "|    n_updates            | 1490        |\n",
      "|    policy_gradient_loss | 0.0118      |\n",
      "|    std                  | 0.00692     |\n",
      "|    value_loss           | 1.66e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0939  |\n",
      "| time/              |          |\n",
      "|    fps             | 590      |\n",
      "|    iterations      | 150      |\n",
      "|    time_elapsed    | 2562     |\n",
      "|    total_timesteps | 1512000  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=1532160, episode_reward=-0.08 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0776     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1532160     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011442878 |\n",
      "|    clip_fraction        | 0.239       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.47        |\n",
      "|    explained_variance   | 0.101       |\n",
      "|    learning_rate        | 0.00246     |\n",
      "|    loss                 | 0.00748     |\n",
      "|    n_updates            | 1510        |\n",
      "|    policy_gradient_loss | 0.0112      |\n",
      "|    std                  | 0.0068      |\n",
      "|    value_loss           | 1.58e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.095   |\n",
      "| time/              |          |\n",
      "|    fps             | 588      |\n",
      "|    iterations      | 152      |\n",
      "|    time_elapsed    | 2602     |\n",
      "|    total_timesteps | 1532160  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=1552320, episode_reward=-0.10 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.102      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1552320     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007293866 |\n",
      "|    clip_fraction        | 0.192       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.5         |\n",
      "|    explained_variance   | 0.0316      |\n",
      "|    learning_rate        | 0.00243     |\n",
      "|    loss                 | -0.00266    |\n",
      "|    n_updates            | 1530        |\n",
      "|    policy_gradient_loss | 0.00437     |\n",
      "|    std                  | 0.00679     |\n",
      "|    value_loss           | 8.79e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.094   |\n",
      "| time/              |          |\n",
      "|    fps             | 588      |\n",
      "|    iterations      | 154      |\n",
      "|    time_elapsed    | 2638     |\n",
      "|    total_timesteps | 1552320  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=1572480, episode_reward=-0.07 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0716      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1572480      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030876568 |\n",
      "|    clip_fraction        | 0.283        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 7.5          |\n",
      "|    explained_variance   | 0.0738       |\n",
      "|    learning_rate        | 0.0024       |\n",
      "|    loss                 | 0.000259     |\n",
      "|    n_updates            | 1550         |\n",
      "|    policy_gradient_loss | 0.019        |\n",
      "|    std                  | 0.00677      |\n",
      "|    value_loss           | 1.27e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0966  |\n",
      "| time/              |          |\n",
      "|    fps             | 589      |\n",
      "|    iterations      | 156      |\n",
      "|    time_elapsed    | 2666     |\n",
      "|    total_timesteps | 1572480  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=1592640, episode_reward=-0.11 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.112       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1592640      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052756188 |\n",
      "|    clip_fraction        | 0.143        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 7.51         |\n",
      "|    explained_variance   | 0.0516       |\n",
      "|    learning_rate        | 0.00236      |\n",
      "|    loss                 | 0.00359      |\n",
      "|    n_updates            | 1570         |\n",
      "|    policy_gradient_loss | 0.0037       |\n",
      "|    std                  | 0.00672      |\n",
      "|    value_loss           | 1.71e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0973  |\n",
      "| time/              |          |\n",
      "|    fps             | 591      |\n",
      "|    iterations      | 158      |\n",
      "|    time_elapsed    | 2694     |\n",
      "|    total_timesteps | 1592640  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=1612800, episode_reward=-0.08 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0773      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1612800      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069602425 |\n",
      "|    clip_fraction        | 0.0829       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 7.54         |\n",
      "|    explained_variance   | 0.0355       |\n",
      "|    learning_rate        | 0.00233      |\n",
      "|    loss                 | -0.00282     |\n",
      "|    n_updates            | 1590         |\n",
      "|    policy_gradient_loss | 0.00207      |\n",
      "|    std                  | 0.00663      |\n",
      "|    value_loss           | 8.97e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0928  |\n",
      "| time/              |          |\n",
      "|    fps             | 592      |\n",
      "|    iterations      | 160      |\n",
      "|    time_elapsed    | 2722     |\n",
      "|    total_timesteps | 1612800  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=1632960, episode_reward=-0.10 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.103       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1632960      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053960583 |\n",
      "|    clip_fraction        | 0.116        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 7.59         |\n",
      "|    explained_variance   | 0.115        |\n",
      "|    learning_rate        | 0.0023       |\n",
      "|    loss                 | 0.000183     |\n",
      "|    n_updates            | 1610         |\n",
      "|    policy_gradient_loss | -0.000117    |\n",
      "|    std                  | 0.00646      |\n",
      "|    value_loss           | 7.52e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0951  |\n",
      "| time/              |          |\n",
      "|    fps             | 593      |\n",
      "|    iterations      | 162      |\n",
      "|    time_elapsed    | 2751     |\n",
      "|    total_timesteps | 1632960  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=1653120, episode_reward=-0.10 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1653120     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013295906 |\n",
      "|    clip_fraction        | 0.211       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.64        |\n",
      "|    explained_variance   | 0.166       |\n",
      "|    learning_rate        | 0.00226     |\n",
      "|    loss                 | 0.0109      |\n",
      "|    n_updates            | 1630        |\n",
      "|    policy_gradient_loss | 0.0104      |\n",
      "|    std                  | 0.00623     |\n",
      "|    value_loss           | 1.13e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0968  |\n",
      "| time/              |          |\n",
      "|    fps             | 594      |\n",
      "|    iterations      | 164      |\n",
      "|    time_elapsed    | 2779     |\n",
      "|    total_timesteps | 1653120  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=1673280, episode_reward=-0.12 +/- 0.07\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.12       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1673280     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011573563 |\n",
      "|    clip_fraction        | 0.149       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.69        |\n",
      "|    explained_variance   | 0.222       |\n",
      "|    learning_rate        | 0.00223     |\n",
      "|    loss                 | -0.00036    |\n",
      "|    n_updates            | 1650        |\n",
      "|    policy_gradient_loss | 0.00296     |\n",
      "|    std                  | 0.0061      |\n",
      "|    value_loss           | 3.64e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0975  |\n",
      "| time/              |          |\n",
      "|    fps             | 595      |\n",
      "|    iterations      | 166      |\n",
      "|    time_elapsed    | 2808     |\n",
      "|    total_timesteps | 1673280  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=1693440, episode_reward=-0.10 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0966     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1693440     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014841464 |\n",
      "|    clip_fraction        | 0.288       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.72        |\n",
      "|    explained_variance   | -0.0798     |\n",
      "|    learning_rate        | 0.00219     |\n",
      "|    loss                 | -0.00067    |\n",
      "|    n_updates            | 1670        |\n",
      "|    policy_gradient_loss | 0.0153      |\n",
      "|    std                  | 0.006       |\n",
      "|    value_loss           | 1.16e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.096   |\n",
      "| time/              |          |\n",
      "|    fps             | 596      |\n",
      "|    iterations      | 168      |\n",
      "|    time_elapsed    | 2837     |\n",
      "|    total_timesteps | 1693440  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=1713600, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0836     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1713600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013088069 |\n",
      "|    clip_fraction        | 0.249       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.74        |\n",
      "|    explained_variance   | 0.242       |\n",
      "|    learning_rate        | 0.00216     |\n",
      "|    loss                 | 0.00123     |\n",
      "|    n_updates            | 1690        |\n",
      "|    policy_gradient_loss | 0.0124      |\n",
      "|    std                  | 0.00593     |\n",
      "|    value_loss           | 1.16e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0966  |\n",
      "| time/              |          |\n",
      "|    fps             | 597      |\n",
      "|    iterations      | 170      |\n",
      "|    time_elapsed    | 2865     |\n",
      "|    total_timesteps | 1713600  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=1733760, episode_reward=-0.10 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.103      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1733760     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009957866 |\n",
      "|    clip_fraction        | 0.114       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.74        |\n",
      "|    explained_variance   | 0.0247      |\n",
      "|    learning_rate        | 0.00213     |\n",
      "|    loss                 | 0.00309     |\n",
      "|    n_updates            | 1710        |\n",
      "|    policy_gradient_loss | 0.00372     |\n",
      "|    std                  | 0.00592     |\n",
      "|    value_loss           | 7.8e-08     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0925  |\n",
      "| time/              |          |\n",
      "|    fps             | 599      |\n",
      "|    iterations      | 172      |\n",
      "|    time_elapsed    | 2894     |\n",
      "|    total_timesteps | 1733760  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=1753920, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0946      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1753920      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072788796 |\n",
      "|    clip_fraction        | 0.123        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 7.75         |\n",
      "|    explained_variance   | 0.472        |\n",
      "|    learning_rate        | 0.00209      |\n",
      "|    loss                 | 0.00522      |\n",
      "|    n_updates            | 1730         |\n",
      "|    policy_gradient_loss | 0.00296      |\n",
      "|    std                  | 0.00592      |\n",
      "|    value_loss           | 1.79e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0928  |\n",
      "| time/              |          |\n",
      "|    fps             | 600      |\n",
      "|    iterations      | 174      |\n",
      "|    time_elapsed    | 2922     |\n",
      "|    total_timesteps | 1753920  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=1774080, episode_reward=-0.12 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.116     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1774080    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00838342 |\n",
      "|    clip_fraction        | 0.145      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 7.8        |\n",
      "|    explained_variance   | 0.215      |\n",
      "|    learning_rate        | 0.00206    |\n",
      "|    loss                 | -0.000432  |\n",
      "|    n_updates            | 1750       |\n",
      "|    policy_gradient_loss | 0.00469    |\n",
      "|    std                  | 0.00577    |\n",
      "|    value_loss           | 1.04e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0916  |\n",
      "| time/              |          |\n",
      "|    fps             | 601      |\n",
      "|    iterations      | 176      |\n",
      "|    time_elapsed    | 2951     |\n",
      "|    total_timesteps | 1774080  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=1794240, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0891     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1794240     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008417903 |\n",
      "|    clip_fraction        | 0.137       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.83        |\n",
      "|    explained_variance   | 0.0442      |\n",
      "|    learning_rate        | 0.00203     |\n",
      "|    loss                 | 0.000462    |\n",
      "|    n_updates            | 1770        |\n",
      "|    policy_gradient_loss | 0.00493     |\n",
      "|    std                  | 0.00572     |\n",
      "|    value_loss           | 1.19e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0904  |\n",
      "| time/              |          |\n",
      "|    fps             | 602      |\n",
      "|    iterations      | 178      |\n",
      "|    time_elapsed    | 2979     |\n",
      "|    total_timesteps | 1794240  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=1814400, episode_reward=-0.12 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.121      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1814400     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012529274 |\n",
      "|    clip_fraction        | 0.249       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.88        |\n",
      "|    explained_variance   | 0.206       |\n",
      "|    learning_rate        | 0.00199     |\n",
      "|    loss                 | 0.00337     |\n",
      "|    n_updates            | 1790        |\n",
      "|    policy_gradient_loss | 0.00606     |\n",
      "|    std                  | 0.00557     |\n",
      "|    value_loss           | 9.25e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0932  |\n",
      "| time/              |          |\n",
      "|    fps             | 603      |\n",
      "|    iterations      | 180      |\n",
      "|    time_elapsed    | 3007     |\n",
      "|    total_timesteps | 1814400  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=1834560, episode_reward=-0.10 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0968     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1834560     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005737156 |\n",
      "|    clip_fraction        | 0.13        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.89        |\n",
      "|    explained_variance   | 0.0591      |\n",
      "|    learning_rate        | 0.00196     |\n",
      "|    loss                 | -0.000348   |\n",
      "|    n_updates            | 1810        |\n",
      "|    policy_gradient_loss | 0.00419     |\n",
      "|    std                  | 0.00556     |\n",
      "|    value_loss           | 6.6e-08     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0923  |\n",
      "| time/              |          |\n",
      "|    fps             | 604      |\n",
      "|    iterations      | 182      |\n",
      "|    time_elapsed    | 3035     |\n",
      "|    total_timesteps | 1834560  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=1854720, episode_reward=-0.09 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.087     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1854720    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02537182 |\n",
      "|    clip_fraction        | 0.264      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 7.87       |\n",
      "|    explained_variance   | 0.127      |\n",
      "|    learning_rate        | 0.00193    |\n",
      "|    loss                 | 0.0316     |\n",
      "|    n_updates            | 1830       |\n",
      "|    policy_gradient_loss | 0.018      |\n",
      "|    std                  | 0.00558    |\n",
      "|    value_loss           | 1.11e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.096   |\n",
      "| time/              |          |\n",
      "|    fps             | 605      |\n",
      "|    iterations      | 184      |\n",
      "|    time_elapsed    | 3064     |\n",
      "|    total_timesteps | 1854720  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=1874880, episode_reward=-0.11 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.105      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1874880     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038895696 |\n",
      "|    clip_fraction        | 0.295       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.87        |\n",
      "|    explained_variance   | 0.0934      |\n",
      "|    learning_rate        | 0.00189     |\n",
      "|    loss                 | 0.0155      |\n",
      "|    n_updates            | 1850        |\n",
      "|    policy_gradient_loss | 0.00714     |\n",
      "|    std                  | 0.00555     |\n",
      "|    value_loss           | 8.36e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0969  |\n",
      "| time/              |          |\n",
      "|    fps             | 606      |\n",
      "|    iterations      | 186      |\n",
      "|    time_elapsed    | 3092     |\n",
      "|    total_timesteps | 1874880  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=1895040, episode_reward=-0.10 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.104      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1895040     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017546054 |\n",
      "|    clip_fraction        | 0.136       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.9         |\n",
      "|    explained_variance   | 0.0165      |\n",
      "|    learning_rate        | 0.00186     |\n",
      "|    loss                 | 0.0113      |\n",
      "|    n_updates            | 1870        |\n",
      "|    policy_gradient_loss | 0.00349     |\n",
      "|    std                  | 0.00543     |\n",
      "|    value_loss           | 8.14e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0953  |\n",
      "| time/              |          |\n",
      "|    fps             | 607      |\n",
      "|    iterations      | 188      |\n",
      "|    time_elapsed    | 3121     |\n",
      "|    total_timesteps | 1895040  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=1915200, episode_reward=-0.11 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.11       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1915200     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011648539 |\n",
      "|    clip_fraction        | 0.243       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.92        |\n",
      "|    explained_variance   | 0.377       |\n",
      "|    learning_rate        | 0.00182     |\n",
      "|    loss                 | 0.0125      |\n",
      "|    n_updates            | 1890        |\n",
      "|    policy_gradient_loss | 0.0104      |\n",
      "|    std                  | 0.00535     |\n",
      "|    value_loss           | 7.42e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0959  |\n",
      "| time/              |          |\n",
      "|    fps             | 608      |\n",
      "|    iterations      | 190      |\n",
      "|    time_elapsed    | 3149     |\n",
      "|    total_timesteps | 1915200  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=1935360, episode_reward=-0.10 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0984     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1935360     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007713711 |\n",
      "|    clip_fraction        | 0.133       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.94        |\n",
      "|    explained_variance   | 0.0947      |\n",
      "|    learning_rate        | 0.00179     |\n",
      "|    loss                 | 0.000378    |\n",
      "|    n_updates            | 1910        |\n",
      "|    policy_gradient_loss | 0.00451     |\n",
      "|    std                  | 0.00533     |\n",
      "|    value_loss           | 9.89e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.093   |\n",
      "| time/              |          |\n",
      "|    fps             | 609      |\n",
      "|    iterations      | 192      |\n",
      "|    time_elapsed    | 3177     |\n",
      "|    total_timesteps | 1935360  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=1955520, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0872     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1955520     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015472406 |\n",
      "|    clip_fraction        | 0.14        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.95        |\n",
      "|    explained_variance   | 0.457       |\n",
      "|    learning_rate        | 0.00176     |\n",
      "|    loss                 | 0.000679    |\n",
      "|    n_updates            | 1930        |\n",
      "|    policy_gradient_loss | 0.00241     |\n",
      "|    std                  | 0.00532     |\n",
      "|    value_loss           | 5.68e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.096   |\n",
      "| time/              |          |\n",
      "|    fps             | 610      |\n",
      "|    iterations      | 194      |\n",
      "|    time_elapsed    | 3205     |\n",
      "|    total_timesteps | 1955520  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=1975680, episode_reward=-0.11 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.108      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1975680     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009823608 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.96        |\n",
      "|    explained_variance   | 0.16        |\n",
      "|    learning_rate        | 0.00172     |\n",
      "|    loss                 | 0.00147     |\n",
      "|    n_updates            | 1950        |\n",
      "|    policy_gradient_loss | 0.00242     |\n",
      "|    std                  | 0.00522     |\n",
      "|    value_loss           | 8.25e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0917  |\n",
      "| time/              |          |\n",
      "|    fps             | 611      |\n",
      "|    iterations      | 196      |\n",
      "|    time_elapsed    | 3233     |\n",
      "|    total_timesteps | 1975680  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=1995840, episode_reward=-0.09 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0925     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1995840     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015101915 |\n",
      "|    clip_fraction        | 0.193       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.98        |\n",
      "|    explained_variance   | -0.0368     |\n",
      "|    learning_rate        | 0.00169     |\n",
      "|    loss                 | -0.00219    |\n",
      "|    n_updates            | 1970        |\n",
      "|    policy_gradient_loss | 0.00699     |\n",
      "|    std                  | 0.0052      |\n",
      "|    value_loss           | 6.65e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0873  |\n",
      "| time/              |          |\n",
      "|    fps             | 611      |\n",
      "|    iterations      | 198      |\n",
      "|    time_elapsed    | 3261     |\n",
      "|    total_timesteps | 1995840  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=2016000, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0811      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2016000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016463787 |\n",
      "|    clip_fraction        | 0.0853       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 8.01         |\n",
      "|    explained_variance   | 0.135        |\n",
      "|    learning_rate        | 0.00166      |\n",
      "|    loss                 | 0.00225      |\n",
      "|    n_updates            | 1990         |\n",
      "|    policy_gradient_loss | 0.00241      |\n",
      "|    std                  | 0.00507      |\n",
      "|    value_loss           | 9.14e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0898  |\n",
      "| time/              |          |\n",
      "|    fps             | 612      |\n",
      "|    iterations      | 200      |\n",
      "|    time_elapsed    | 3289     |\n",
      "|    total_timesteps | 2016000  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=2036160, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.089      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2036160     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012478001 |\n",
      "|    clip_fraction        | 0.0912      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.02        |\n",
      "|    explained_variance   | 0.247       |\n",
      "|    learning_rate        | 0.00162     |\n",
      "|    loss                 | -0.0019     |\n",
      "|    n_updates            | 2010        |\n",
      "|    policy_gradient_loss | 0.00182     |\n",
      "|    std                  | 0.00508     |\n",
      "|    value_loss           | 5.77e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0888  |\n",
      "| time/              |          |\n",
      "|    fps             | 613      |\n",
      "|    iterations      | 202      |\n",
      "|    time_elapsed    | 3318     |\n",
      "|    total_timesteps | 2036160  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=2056320, episode_reward=-0.08 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0784     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2056320     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006204796 |\n",
      "|    clip_fraction        | 0.0728      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.02        |\n",
      "|    explained_variance   | 0.107       |\n",
      "|    learning_rate        | 0.00159     |\n",
      "|    loss                 | 0.0141      |\n",
      "|    n_updates            | 2030        |\n",
      "|    policy_gradient_loss | 0.00213     |\n",
      "|    std                  | 0.00503     |\n",
      "|    value_loss           | 3.03e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.089   |\n",
      "| time/              |          |\n",
      "|    fps             | 614      |\n",
      "|    iterations      | 204      |\n",
      "|    time_elapsed    | 3346     |\n",
      "|    total_timesteps | 2056320  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=2076480, episode_reward=-0.10 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.104      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2076480     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008991029 |\n",
      "|    clip_fraction        | 0.081       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.05        |\n",
      "|    explained_variance   | -0.107      |\n",
      "|    learning_rate        | 0.00156     |\n",
      "|    loss                 | 0.000955    |\n",
      "|    n_updates            | 2050        |\n",
      "|    policy_gradient_loss | 0.000918    |\n",
      "|    std                  | 0.005       |\n",
      "|    value_loss           | 2.75e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0868  |\n",
      "| time/              |          |\n",
      "|    fps             | 615      |\n",
      "|    iterations      | 206      |\n",
      "|    time_elapsed    | 3374     |\n",
      "|    total_timesteps | 2076480  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=2096640, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0801     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2096640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010581894 |\n",
      "|    clip_fraction        | 0.141       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.06        |\n",
      "|    explained_variance   | 0.0146      |\n",
      "|    learning_rate        | 0.00152     |\n",
      "|    loss                 | 0.0147      |\n",
      "|    n_updates            | 2070        |\n",
      "|    policy_gradient_loss | 0.00453     |\n",
      "|    std                  | 0.00503     |\n",
      "|    value_loss           | 1.71e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0879  |\n",
      "| time/              |          |\n",
      "|    fps             | 616      |\n",
      "|    iterations      | 208      |\n",
      "|    time_elapsed    | 3403     |\n",
      "|    total_timesteps | 2096640  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=2116800, episode_reward=-0.08 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0814     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2116800     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009521208 |\n",
      "|    clip_fraction        | 0.209       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.05        |\n",
      "|    explained_variance   | 0.00155     |\n",
      "|    learning_rate        | 0.00149     |\n",
      "|    loss                 | 0.0112      |\n",
      "|    n_updates            | 2090        |\n",
      "|    policy_gradient_loss | 0.00863     |\n",
      "|    std                  | 0.005       |\n",
      "|    value_loss           | 1.3e-07     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.089   |\n",
      "| time/              |          |\n",
      "|    fps             | 616      |\n",
      "|    iterations      | 210      |\n",
      "|    time_elapsed    | 3431     |\n",
      "|    total_timesteps | 2116800  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=2136960, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0887     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2136960     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009967184 |\n",
      "|    clip_fraction        | 0.207       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.07        |\n",
      "|    explained_variance   | 0.331       |\n",
      "|    learning_rate        | 0.00146     |\n",
      "|    loss                 | 0.00829     |\n",
      "|    n_updates            | 2110        |\n",
      "|    policy_gradient_loss | 0.00782     |\n",
      "|    std                  | 0.00491     |\n",
      "|    value_loss           | 3.34e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0929  |\n",
      "| time/              |          |\n",
      "|    fps             | 617      |\n",
      "|    iterations      | 212      |\n",
      "|    time_elapsed    | 3460     |\n",
      "|    total_timesteps | 2136960  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=2157120, episode_reward=-0.08 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0823     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2157120     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003785673 |\n",
      "|    clip_fraction        | 0.0857      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.09        |\n",
      "|    explained_variance   | 0.19        |\n",
      "|    learning_rate        | 0.00142     |\n",
      "|    loss                 | 0.000902    |\n",
      "|    n_updates            | 2130        |\n",
      "|    policy_gradient_loss | 0.00181     |\n",
      "|    std                  | 0.00486     |\n",
      "|    value_loss           | 2.82e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0944  |\n",
      "| time/              |          |\n",
      "|    fps             | 618      |\n",
      "|    iterations      | 214      |\n",
      "|    time_elapsed    | 3488     |\n",
      "|    total_timesteps | 2157120  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=2177280, episode_reward=-0.12 +/- 0.07\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.118      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2177280     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003781303 |\n",
      "|    clip_fraction        | 0.2         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.1         |\n",
      "|    explained_variance   | 0.00818     |\n",
      "|    learning_rate        | 0.00139     |\n",
      "|    loss                 | 0.00216     |\n",
      "|    n_updates            | 2150        |\n",
      "|    policy_gradient_loss | 0.00858     |\n",
      "|    std                  | 0.00483     |\n",
      "|    value_loss           | 1.07e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0923  |\n",
      "| time/              |          |\n",
      "|    fps             | 619      |\n",
      "|    iterations      | 216      |\n",
      "|    time_elapsed    | 3516     |\n",
      "|    total_timesteps | 2177280  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=2197440, episode_reward=-0.08 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0774      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2197440      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025062398 |\n",
      "|    clip_fraction        | 0.14         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 8.11         |\n",
      "|    explained_variance   | 0.0734       |\n",
      "|    learning_rate        | 0.00135      |\n",
      "|    loss                 | -0.00104     |\n",
      "|    n_updates            | 2170         |\n",
      "|    policy_gradient_loss | 0.00485      |\n",
      "|    std                  | 0.00477      |\n",
      "|    value_loss           | 1.41e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0954  |\n",
      "| time/              |          |\n",
      "|    fps             | 619      |\n",
      "|    iterations      | 218      |\n",
      "|    time_elapsed    | 3544     |\n",
      "|    total_timesteps | 2197440  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=2217600, episode_reward=-0.07 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0683     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2217600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009429471 |\n",
      "|    clip_fraction        | 0.123       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.14        |\n",
      "|    explained_variance   | 0.102       |\n",
      "|    learning_rate        | 0.00132     |\n",
      "|    loss                 | 0.00741     |\n",
      "|    n_updates            | 2190        |\n",
      "|    policy_gradient_loss | 0.0037      |\n",
      "|    std                  | 0.00472     |\n",
      "|    value_loss           | 6.16e-08    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0944  |\n",
      "| time/              |          |\n",
      "|    fps             | 620      |\n",
      "|    iterations      | 220      |\n",
      "|    time_elapsed    | 3572     |\n",
      "|    total_timesteps | 2217600  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=2237760, episode_reward=-0.12 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.122       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2237760      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062372806 |\n",
      "|    clip_fraction        | 0.111        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 8.15         |\n",
      "|    explained_variance   | 0.299        |\n",
      "|    learning_rate        | 0.00129      |\n",
      "|    loss                 | 0.00308      |\n",
      "|    n_updates            | 2210         |\n",
      "|    policy_gradient_loss | 0.00356      |\n",
      "|    std                  | 0.00467      |\n",
      "|    value_loss           | 9.15e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0956  |\n",
      "| time/              |          |\n",
      "|    fps             | 621      |\n",
      "|    iterations      | 222      |\n",
      "|    time_elapsed    | 3599     |\n",
      "|    total_timesteps | 2237760  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=2257920, episode_reward=-0.09 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0935      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2257920      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026820975 |\n",
      "|    clip_fraction        | 0.137        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 8.16         |\n",
      "|    explained_variance   | 0.265        |\n",
      "|    learning_rate        | 0.00125      |\n",
      "|    loss                 | 0.00142      |\n",
      "|    n_updates            | 2230         |\n",
      "|    policy_gradient_loss | 0.00282      |\n",
      "|    std                  | 0.0047       |\n",
      "|    value_loss           | 9.41e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0927  |\n",
      "| time/              |          |\n",
      "|    fps             | 622      |\n",
      "|    iterations      | 224      |\n",
      "|    time_elapsed    | 3627     |\n",
      "|    total_timesteps | 2257920  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=2278080, episode_reward=-0.12 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.12       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2278080     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012565967 |\n",
      "|    clip_fraction        | 0.14        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.18        |\n",
      "|    explained_variance   | 0.246       |\n",
      "|    learning_rate        | 0.00122     |\n",
      "|    loss                 | 0.0101      |\n",
      "|    n_updates            | 2250        |\n",
      "|    policy_gradient_loss | 0.00346     |\n",
      "|    std                  | 0.0046      |\n",
      "|    value_loss           | 2e-07       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0899  |\n",
      "| time/              |          |\n",
      "|    fps             | 622      |\n",
      "|    iterations      | 226      |\n",
      "|    time_elapsed    | 3656     |\n",
      "|    total_timesteps | 2278080  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=2298240, episode_reward=-0.11 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.109       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2298240      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036155817 |\n",
      "|    clip_fraction        | 0.125        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 8.21         |\n",
      "|    explained_variance   | -0.0811      |\n",
      "|    learning_rate        | 0.00119      |\n",
      "|    loss                 | 0.00318      |\n",
      "|    n_updates            | 2270         |\n",
      "|    policy_gradient_loss | 0.00218      |\n",
      "|    std                  | 0.00458      |\n",
      "|    value_loss           | 2.64e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0924  |\n",
      "| time/              |          |\n",
      "|    fps             | 623      |\n",
      "|    iterations      | 228      |\n",
      "|    time_elapsed    | 3684     |\n",
      "|    total_timesteps | 2298240  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=2318400, episode_reward=-0.07 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0656     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2318400     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008093113 |\n",
      "|    clip_fraction        | 0.13        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.24        |\n",
      "|    explained_variance   | 0.278       |\n",
      "|    learning_rate        | 0.00115     |\n",
      "|    loss                 | 0.0132      |\n",
      "|    n_updates            | 2290        |\n",
      "|    policy_gradient_loss | 0.00411     |\n",
      "|    std                  | 0.00452     |\n",
      "|    value_loss           | 1.16e-07    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0939  |\n",
      "| time/              |          |\n",
      "|    fps             | 624      |\n",
      "|    iterations      | 230      |\n",
      "|    time_elapsed    | 3712     |\n",
      "|    total_timesteps | 2318400  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=2338560, episode_reward=-0.09 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0883     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2338560     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008323023 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.24        |\n",
      "|    explained_variance   | 0.186       |\n",
      "|    learning_rate        | 0.00112     |\n",
      "|    loss                 | 0.0012      |\n",
      "|    n_updates            | 2310        |\n",
      "|    policy_gradient_loss | 0.0026      |\n",
      "|    std                  | 0.00449     |\n",
      "|    value_loss           | 5.25e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0965  |\n",
      "| time/              |          |\n",
      "|    fps             | 625      |\n",
      "|    iterations      | 232      |\n",
      "|    time_elapsed    | 3740     |\n",
      "|    total_timesteps | 2338560  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=2358720, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0933    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2358720    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02619164 |\n",
      "|    clip_fraction        | 0.129      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 8.26       |\n",
      "|    explained_variance   | 0.235      |\n",
      "|    learning_rate        | 0.00109    |\n",
      "|    loss                 | 0.00183    |\n",
      "|    n_updates            | 2330       |\n",
      "|    policy_gradient_loss | 0.00344    |\n",
      "|    std                  | 0.00444    |\n",
      "|    value_loss           | 1.7e-07    |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0981  |\n",
      "| time/              |          |\n",
      "|    fps             | 625      |\n",
      "|    iterations      | 234      |\n",
      "|    time_elapsed    | 3769     |\n",
      "|    total_timesteps | 2358720  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=2378880, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0867     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2378880     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003669391 |\n",
      "|    clip_fraction        | 0.0852      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.28        |\n",
      "|    explained_variance   | 0.197       |\n",
      "|    learning_rate        | 0.00105     |\n",
      "|    loss                 | 0.000286    |\n",
      "|    n_updates            | 2350        |\n",
      "|    policy_gradient_loss | 0.00273     |\n",
      "|    std                  | 0.00438     |\n",
      "|    value_loss           | 1.23e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 626      |\n",
      "|    iterations      | 236      |\n",
      "|    time_elapsed    | 3796     |\n",
      "|    total_timesteps | 2378880  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=2399040, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.085      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2399040     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012966529 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.3         |\n",
      "|    explained_variance   | 0.0453      |\n",
      "|    learning_rate        | 0.00102     |\n",
      "|    loss                 | 0.0111      |\n",
      "|    n_updates            | 2370        |\n",
      "|    policy_gradient_loss | 0.00232     |\n",
      "|    std                  | 0.00435     |\n",
      "|    value_loss           | 1e-07       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.102   |\n",
      "| time/              |          |\n",
      "|    fps             | 627      |\n",
      "|    iterations      | 238      |\n",
      "|    time_elapsed    | 3824     |\n",
      "|    total_timesteps | 2399040  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=2419200, episode_reward=-0.09 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0908     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2419200     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009518209 |\n",
      "|    clip_fraction        | 0.0432      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.3         |\n",
      "|    explained_variance   | 0.0724      |\n",
      "|    learning_rate        | 0.000985    |\n",
      "|    loss                 | 0.00564     |\n",
      "|    n_updates            | 2390        |\n",
      "|    policy_gradient_loss | 0.0011      |\n",
      "|    std                  | 0.00438     |\n",
      "|    value_loss           | 1.79e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.102   |\n",
      "| time/              |          |\n",
      "|    fps             | 627      |\n",
      "|    iterations      | 240      |\n",
      "|    time_elapsed    | 3853     |\n",
      "|    total_timesteps | 2419200  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=2439360, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0823     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2439360     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014362239 |\n",
      "|    clip_fraction        | 0.0774      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.32        |\n",
      "|    explained_variance   | 0.133       |\n",
      "|    learning_rate        | 0.000951    |\n",
      "|    loss                 | 0.00499     |\n",
      "|    n_updates            | 2410        |\n",
      "|    policy_gradient_loss | 0.000776    |\n",
      "|    std                  | 0.00431     |\n",
      "|    value_loss           | 9.28e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.101   |\n",
      "| time/              |          |\n",
      "|    fps             | 628      |\n",
      "|    iterations      | 242      |\n",
      "|    time_elapsed    | 3881     |\n",
      "|    total_timesteps | 2439360  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=2459520, episode_reward=-0.10 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.101      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2459520     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002658813 |\n",
      "|    clip_fraction        | 0.0509      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.34        |\n",
      "|    explained_variance   | 0.196       |\n",
      "|    learning_rate        | 0.000918    |\n",
      "|    loss                 | 0.00475     |\n",
      "|    n_updates            | 2430        |\n",
      "|    policy_gradient_loss | 0.00148     |\n",
      "|    std                  | 0.00428     |\n",
      "|    value_loss           | 1.68e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0979  |\n",
      "| time/              |          |\n",
      "|    fps             | 629      |\n",
      "|    iterations      | 244      |\n",
      "|    time_elapsed    | 3908     |\n",
      "|    total_timesteps | 2459520  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=2479680, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0929    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2479680    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00657101 |\n",
      "|    clip_fraction        | 0.0683     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 8.34       |\n",
      "|    explained_variance   | 0.354      |\n",
      "|    learning_rate        | 0.000884   |\n",
      "|    loss                 | -0.000875  |\n",
      "|    n_updates            | 2450       |\n",
      "|    policy_gradient_loss | 0.000926   |\n",
      "|    std                  | 0.00429    |\n",
      "|    value_loss           | 1.72e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.097   |\n",
      "| time/              |          |\n",
      "|    fps             | 629      |\n",
      "|    iterations      | 246      |\n",
      "|    time_elapsed    | 3936     |\n",
      "|    total_timesteps | 2479680  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=2499840, episode_reward=-0.07 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.073       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2499840      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048797475 |\n",
      "|    clip_fraction        | 0.147        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 8.35         |\n",
      "|    explained_variance   | 0.0181       |\n",
      "|    learning_rate        | 0.00085      |\n",
      "|    loss                 | 0.00263      |\n",
      "|    n_updates            | 2470         |\n",
      "|    policy_gradient_loss | 0.00454      |\n",
      "|    std                  | 0.00427      |\n",
      "|    value_loss           | 1.24e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.094   |\n",
      "| time/              |          |\n",
      "|    fps             | 630      |\n",
      "|    iterations      | 248      |\n",
      "|    time_elapsed    | 3964     |\n",
      "|    total_timesteps | 2499840  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=2520000, episode_reward=-0.10 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.104       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2520000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028265542 |\n",
      "|    clip_fraction        | 0.112        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 8.39         |\n",
      "|    explained_variance   | 0.391        |\n",
      "|    learning_rate        | 0.000817     |\n",
      "|    loss                 | -0.0031      |\n",
      "|    n_updates            | 2490         |\n",
      "|    policy_gradient_loss | 0.00267      |\n",
      "|    std                  | 0.00421      |\n",
      "|    value_loss           | 2.62e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0935  |\n",
      "| time/              |          |\n",
      "|    fps             | 631      |\n",
      "|    iterations      | 250      |\n",
      "|    time_elapsed    | 3992     |\n",
      "|    total_timesteps | 2520000  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=2540160, episode_reward=-0.10 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.103       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2540160      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043970495 |\n",
      "|    clip_fraction        | 0.0481       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 8.42         |\n",
      "|    explained_variance   | 0.0991       |\n",
      "|    learning_rate        | 0.000783     |\n",
      "|    loss                 | -0.00173     |\n",
      "|    n_updates            | 2510         |\n",
      "|    policy_gradient_loss | 0.000645     |\n",
      "|    std                  | 0.00409      |\n",
      "|    value_loss           | 7.65e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0897  |\n",
      "| time/              |          |\n",
      "|    fps             | 631      |\n",
      "|    iterations      | 252      |\n",
      "|    time_elapsed    | 4021     |\n",
      "|    total_timesteps | 2540160  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=2560320, episode_reward=-0.09 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0859     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2560320     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007634949 |\n",
      "|    clip_fraction        | 0.0835      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.45        |\n",
      "|    explained_variance   | 0.457       |\n",
      "|    learning_rate        | 0.00075     |\n",
      "|    loss                 | 0.00728     |\n",
      "|    n_updates            | 2530        |\n",
      "|    policy_gradient_loss | 0.000789    |\n",
      "|    std                  | 0.00402     |\n",
      "|    value_loss           | 4.11e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0923  |\n",
      "| time/              |          |\n",
      "|    fps             | 632      |\n",
      "|    iterations      | 254      |\n",
      "|    time_elapsed    | 4048     |\n",
      "|    total_timesteps | 2560320  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=2580480, episode_reward=-0.12 +/- 0.07\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.118       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2580480      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024016136 |\n",
      "|    clip_fraction        | 0.0848       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 8.47         |\n",
      "|    explained_variance   | 0.208        |\n",
      "|    learning_rate        | 0.000716     |\n",
      "|    loss                 | -0.001       |\n",
      "|    n_updates            | 2550         |\n",
      "|    policy_gradient_loss | 0.00134      |\n",
      "|    std                  | 0.00403      |\n",
      "|    value_loss           | 1.49e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.095   |\n",
      "| time/              |          |\n",
      "|    fps             | 633      |\n",
      "|    iterations      | 256      |\n",
      "|    time_elapsed    | 4076     |\n",
      "|    total_timesteps | 2580480  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=2600640, episode_reward=-0.09 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0875      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2600640      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047131698 |\n",
      "|    clip_fraction        | 0.0812       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 8.48         |\n",
      "|    explained_variance   | 0.141        |\n",
      "|    learning_rate        | 0.000682     |\n",
      "|    loss                 | -0.000593    |\n",
      "|    n_updates            | 2570         |\n",
      "|    policy_gradient_loss | 0.00175      |\n",
      "|    std                  | 0.00398      |\n",
      "|    value_loss           | 1.46e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0977  |\n",
      "| time/              |          |\n",
      "|    fps             | 633      |\n",
      "|    iterations      | 258      |\n",
      "|    time_elapsed    | 4104     |\n",
      "|    total_timesteps | 2600640  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=2620800, episode_reward=-0.11 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.107      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2620800     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007520198 |\n",
      "|    clip_fraction        | 0.0626      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.5         |\n",
      "|    explained_variance   | 0.308       |\n",
      "|    learning_rate        | 0.000649    |\n",
      "|    loss                 | 0.00528     |\n",
      "|    n_updates            | 2590        |\n",
      "|    policy_gradient_loss | -0.000144   |\n",
      "|    std                  | 0.00394     |\n",
      "|    value_loss           | 4.2e-07     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.101   |\n",
      "| time/              |          |\n",
      "|    fps             | 633      |\n",
      "|    iterations      | 260      |\n",
      "|    time_elapsed    | 4133     |\n",
      "|    total_timesteps | 2620800  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=2640960, episode_reward=-0.10 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.101      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2640960     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016005171 |\n",
      "|    clip_fraction        | 0.0904      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.52        |\n",
      "|    explained_variance   | 0.218       |\n",
      "|    learning_rate        | 0.000615    |\n",
      "|    loss                 | -0.00304    |\n",
      "|    n_updates            | 2610        |\n",
      "|    policy_gradient_loss | 0.00112     |\n",
      "|    std                  | 0.00388     |\n",
      "|    value_loss           | 8.78e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 634      |\n",
      "|    iterations      | 262      |\n",
      "|    time_elapsed    | 4161     |\n",
      "|    total_timesteps | 2640960  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=2661120, episode_reward=-0.07 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0743     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2661120     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003890645 |\n",
      "|    clip_fraction        | 0.0851      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.56        |\n",
      "|    explained_variance   | 0.365       |\n",
      "|    learning_rate        | 0.000582    |\n",
      "|    loss                 | 9.19e-05    |\n",
      "|    n_updates            | 2630        |\n",
      "|    policy_gradient_loss | 0.00141     |\n",
      "|    std                  | 0.00384     |\n",
      "|    value_loss           | 1.58e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.103   |\n",
      "| time/              |          |\n",
      "|    fps             | 635      |\n",
      "|    iterations      | 264      |\n",
      "|    time_elapsed    | 4189     |\n",
      "|    total_timesteps | 2661120  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=2681280, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1.26e+03      |\n",
      "|    mean_reward          | -0.0825       |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 2681280       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00045139797 |\n",
      "|    clip_fraction        | 0.0172        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | 8.56          |\n",
      "|    explained_variance   | 0.175         |\n",
      "|    learning_rate        | 0.000548      |\n",
      "|    loss                 | 0.00118       |\n",
      "|    n_updates            | 2650          |\n",
      "|    policy_gradient_loss | -0.00019      |\n",
      "|    std                  | 0.00383       |\n",
      "|    value_loss           | 6.57e-08      |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.102   |\n",
      "| time/              |          |\n",
      "|    fps             | 635      |\n",
      "|    iterations      | 266      |\n",
      "|    time_elapsed    | 4217     |\n",
      "|    total_timesteps | 2681280  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=2701440, episode_reward=-0.09 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0882      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2701440      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074648424 |\n",
      "|    clip_fraction        | 0.0619       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 8.57         |\n",
      "|    explained_variance   | 0.0142       |\n",
      "|    learning_rate        | 0.000514     |\n",
      "|    loss                 | 0.00711      |\n",
      "|    n_updates            | 2670         |\n",
      "|    policy_gradient_loss | 0.000502     |\n",
      "|    std                  | 0.0038       |\n",
      "|    value_loss           | 1.47e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0954  |\n",
      "| time/              |          |\n",
      "|    fps             | 636      |\n",
      "|    iterations      | 268      |\n",
      "|    time_elapsed    | 4244     |\n",
      "|    total_timesteps | 2701440  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=2721600, episode_reward=-0.10 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0952      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2721600      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075137173 |\n",
      "|    clip_fraction        | 0.0834       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 8.59         |\n",
      "|    explained_variance   | 0.148        |\n",
      "|    learning_rate        | 0.000481     |\n",
      "|    loss                 | -0.00173     |\n",
      "|    n_updates            | 2690         |\n",
      "|    policy_gradient_loss | 0.000602     |\n",
      "|    std                  | 0.00375      |\n",
      "|    value_loss           | 8.28e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0952  |\n",
      "| time/              |          |\n",
      "|    fps             | 636      |\n",
      "|    iterations      | 270      |\n",
      "|    time_elapsed    | 4273     |\n",
      "|    total_timesteps | 2721600  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=2741760, episode_reward=-0.11 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.109      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2741760     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003589048 |\n",
      "|    clip_fraction        | 0.134       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.6         |\n",
      "|    explained_variance   | 0.29        |\n",
      "|    learning_rate        | 0.000447    |\n",
      "|    loss                 | 0.00376     |\n",
      "|    n_updates            | 2710        |\n",
      "|    policy_gradient_loss | 0.00397     |\n",
      "|    std                  | 0.00376     |\n",
      "|    value_loss           | 1.28e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0937  |\n",
      "| time/              |          |\n",
      "|    fps             | 637      |\n",
      "|    iterations      | 272      |\n",
      "|    time_elapsed    | 4300     |\n",
      "|    total_timesteps | 2741760  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=2761920, episode_reward=-0.10 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.103       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2761920      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035682134 |\n",
      "|    clip_fraction        | 0.0267       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 8.6          |\n",
      "|    explained_variance   | -0.104       |\n",
      "|    learning_rate        | 0.000414     |\n",
      "|    loss                 | -0.0043      |\n",
      "|    n_updates            | 2730         |\n",
      "|    policy_gradient_loss | -0.000939    |\n",
      "|    std                  | 0.00373      |\n",
      "|    value_loss           | 8.8e-08      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0941  |\n",
      "| time/              |          |\n",
      "|    fps             | 638      |\n",
      "|    iterations      | 274      |\n",
      "|    time_elapsed    | 4328     |\n",
      "|    total_timesteps | 2761920  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=2782080, episode_reward=-0.09 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0891     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2782080     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004376794 |\n",
      "|    clip_fraction        | 0.116       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.61        |\n",
      "|    explained_variance   | 0.469       |\n",
      "|    learning_rate        | 0.00038     |\n",
      "|    loss                 | -0.000416   |\n",
      "|    n_updates            | 2750        |\n",
      "|    policy_gradient_loss | 0.00265     |\n",
      "|    std                  | 0.00372     |\n",
      "|    value_loss           | 1.73e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.093   |\n",
      "| time/              |          |\n",
      "|    fps             | 638      |\n",
      "|    iterations      | 276      |\n",
      "|    time_elapsed    | 4356     |\n",
      "|    total_timesteps | 2782080  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=2802240, episode_reward=-0.11 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.109       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2802240      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061813965 |\n",
      "|    clip_fraction        | 0.0279       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 8.62         |\n",
      "|    explained_variance   | 0.202        |\n",
      "|    learning_rate        | 0.000346     |\n",
      "|    loss                 | 0.00474      |\n",
      "|    n_updates            | 2770         |\n",
      "|    policy_gradient_loss | 0.000762     |\n",
      "|    std                  | 0.00371      |\n",
      "|    value_loss           | 7.95e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0942  |\n",
      "| time/              |          |\n",
      "|    fps             | 639      |\n",
      "|    iterations      | 278      |\n",
      "|    time_elapsed    | 4385     |\n",
      "|    total_timesteps | 2802240  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=2822400, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.081      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2822400     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003807958 |\n",
      "|    clip_fraction        | 0.0222      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.61        |\n",
      "|    explained_variance   | 0.181       |\n",
      "|    learning_rate        | 0.000313    |\n",
      "|    loss                 | 0.000903    |\n",
      "|    n_updates            | 2790        |\n",
      "|    policy_gradient_loss | -0.000288   |\n",
      "|    std                  | 0.0037      |\n",
      "|    value_loss           | 8.59e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0946  |\n",
      "| time/              |          |\n",
      "|    fps             | 639      |\n",
      "|    iterations      | 280      |\n",
      "|    time_elapsed    | 4412     |\n",
      "|    total_timesteps | 2822400  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=2842560, episode_reward=-0.11 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.115       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2842560      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069515747 |\n",
      "|    clip_fraction        | 0.0429       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 8.63         |\n",
      "|    explained_variance   | 0.111        |\n",
      "|    learning_rate        | 0.000279     |\n",
      "|    loss                 | 0.00354      |\n",
      "|    n_updates            | 2810         |\n",
      "|    policy_gradient_loss | -0.00144     |\n",
      "|    std                  | 0.00366      |\n",
      "|    value_loss           | 8.03e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0932  |\n",
      "| time/              |          |\n",
      "|    fps             | 639      |\n",
      "|    iterations      | 282      |\n",
      "|    time_elapsed    | 4442     |\n",
      "|    total_timesteps | 2842560  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=2862720, episode_reward=-0.10 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0982     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2862720     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001344237 |\n",
      "|    clip_fraction        | 0.0169      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.64        |\n",
      "|    explained_variance   | 0.177       |\n",
      "|    learning_rate        | 0.000246    |\n",
      "|    loss                 | -2.51e-05   |\n",
      "|    n_updates            | 2830        |\n",
      "|    policy_gradient_loss | -0.000393   |\n",
      "|    std                  | 0.00365     |\n",
      "|    value_loss           | 4.5e-08     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0909  |\n",
      "| time/              |          |\n",
      "|    fps             | 640      |\n",
      "|    iterations      | 284      |\n",
      "|    time_elapsed    | 4469     |\n",
      "|    total_timesteps | 2862720  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=2882880, episode_reward=-0.10 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0987      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2882880      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030510877 |\n",
      "|    clip_fraction        | 0.0433       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 8.65         |\n",
      "|    explained_variance   | 0.272        |\n",
      "|    learning_rate        | 0.000212     |\n",
      "|    loss                 | -7.57e-06    |\n",
      "|    n_updates            | 2850         |\n",
      "|    policy_gradient_loss | -0.001       |\n",
      "|    std                  | 0.00363      |\n",
      "|    value_loss           | 9.21e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.09    |\n",
      "| time/              |          |\n",
      "|    fps             | 641      |\n",
      "|    iterations      | 286      |\n",
      "|    time_elapsed    | 4497     |\n",
      "|    total_timesteps | 2882880  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=2903040, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0802    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2903040    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00489607 |\n",
      "|    clip_fraction        | 0.0205     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 8.66       |\n",
      "|    explained_variance   | 0.183      |\n",
      "|    learning_rate        | 0.000178   |\n",
      "|    loss                 | 0.00264    |\n",
      "|    n_updates            | 2870       |\n",
      "|    policy_gradient_loss | -0.000268  |\n",
      "|    std                  | 0.00363    |\n",
      "|    value_loss           | 1.26e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0917  |\n",
      "| time/              |          |\n",
      "|    fps             | 641      |\n",
      "|    iterations      | 288      |\n",
      "|    time_elapsed    | 4524     |\n",
      "|    total_timesteps | 2903040  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=2923200, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0845      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2923200      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036306775 |\n",
      "|    clip_fraction        | 0.0247       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 8.66         |\n",
      "|    explained_variance   | 0.176        |\n",
      "|    learning_rate        | 0.000145     |\n",
      "|    loss                 | -0.0034      |\n",
      "|    n_updates            | 2890         |\n",
      "|    policy_gradient_loss | -0.00109     |\n",
      "|    std                  | 0.00361      |\n",
      "|    value_loss           | 9.08e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0926  |\n",
      "| time/              |          |\n",
      "|    fps             | 642      |\n",
      "|    iterations      | 290      |\n",
      "|    time_elapsed    | 4552     |\n",
      "|    total_timesteps | 2923200  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=2943360, episode_reward=-0.10 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0974      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2943360      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032354805 |\n",
      "|    clip_fraction        | 0.024        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 8.67         |\n",
      "|    explained_variance   | -0.0674      |\n",
      "|    learning_rate        | 0.000111     |\n",
      "|    loss                 | -0.000962    |\n",
      "|    n_updates            | 2910         |\n",
      "|    policy_gradient_loss | -0.00129     |\n",
      "|    std                  | 0.0036       |\n",
      "|    value_loss           | 5.64e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0913  |\n",
      "| time/              |          |\n",
      "|    fps             | 642      |\n",
      "|    iterations      | 292      |\n",
      "|    time_elapsed    | 4581     |\n",
      "|    total_timesteps | 2943360  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=2963520, episode_reward=-0.10 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2963520      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015727595 |\n",
      "|    clip_fraction        | 0.00443      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 8.67         |\n",
      "|    explained_variance   | 0.177        |\n",
      "|    learning_rate        | 7.76e-05     |\n",
      "|    loss                 | 0.000824     |\n",
      "|    n_updates            | 2930         |\n",
      "|    policy_gradient_loss | 2.24e-05     |\n",
      "|    std                  | 0.0036       |\n",
      "|    value_loss           | 6.83e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0871  |\n",
      "| time/              |          |\n",
      "|    fps             | 643      |\n",
      "|    iterations      | 294      |\n",
      "|    time_elapsed    | 4608     |\n",
      "|    total_timesteps | 2963520  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=2983680, episode_reward=-0.12 +/- 0.07\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.119       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2983680      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027150407 |\n",
      "|    clip_fraction        | 0.00744      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 8.68         |\n",
      "|    explained_variance   | 0.324        |\n",
      "|    learning_rate        | 4.4e-05      |\n",
      "|    loss                 | 0.000169     |\n",
      "|    n_updates            | 2950         |\n",
      "|    policy_gradient_loss | -8.92e-05    |\n",
      "|    std                  | 0.00359      |\n",
      "|    value_loss           | 4.07e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0906  |\n",
      "| time/              |          |\n",
      "|    fps             | 643      |\n",
      "|    iterations      | 296      |\n",
      "|    time_elapsed    | 4636     |\n",
      "|    total_timesteps | 2983680  |\n",
      "---------------------------------\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "4.997023218892638\n",
      "Eval num_timesteps=3003840, episode_reward=-0.09 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0869      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3003840      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017481189 |\n",
      "|    clip_fraction        | 8.93e-05     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 8.68         |\n",
      "|    explained_variance   | 0.162        |\n",
      "|    learning_rate        | 1.04e-05     |\n",
      "|    loss                 | 0.000332     |\n",
      "|    n_updates            | 2970         |\n",
      "|    policy_gradient_loss | -0.000235    |\n",
      "|    std                  | 0.00359      |\n",
      "|    value_loss           | 1.03e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0926  |\n",
      "| time/              |          |\n",
      "|    fps             | 643      |\n",
      "|    iterations      | 298      |\n",
      "|    time_elapsed    | 4664     |\n",
      "|    total_timesteps | 3003840  |\n",
      "---------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x280dc3a50d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Big observation\n",
    "envs = VecMonitor(DummyVecEnv([\n",
    "    lambda: tradingEng(paths1,action = 'small-More-Trust', obs = 'small'),\n",
    "    lambda: tradingEng(paths2,action = 'small-More-Trust', obs = 'small')\n",
    "]),filename='logsBigO-train')\n",
    "ev_env = VecMonitor(DummyVecEnv([\n",
    "    lambda: tradingEng(paths_ev,action = 'small-More-Trust', obs = 'small'),\n",
    "]))\n",
    "\n",
    "eval_callback = EvalCallback(\n",
    "    ev_env,\n",
    "    best_model_save_path='./logs/best_modelBigObs',\n",
    "    log_path='./logs/eval_logsBigO/ev',\n",
    "    eval_freq=252*8*5,\n",
    "    deterministic=True,\n",
    "    render=False,\n",
    "    verbose = True,\n",
    "    n_eval_episodes = 8\n",
    ")\n",
    "model = PPO(\"MlpPolicy\", envs, batch_size = 252*2*5, learning_rate=linear_schedule(0.005), policy_kwargs=policy_kwargs, n_steps=252*4*5, normalize_advantage=True, gamma = 0.9, verbose = 1) \n",
    "\n",
    "model.learn(total_timesteps=3e6, log_interval=2, callback=eval_callback) \n",
    "\n",
    "try:\n",
    "    # Auto Encoder Observation\n",
    "    envs = VecMonitor(DummyVecEnv([\n",
    "        lambda: tradingEng(paths1,action = 'small-More-Trust', obs = 'auto'),\n",
    "        lambda: tradingEng(paths2,action = 'small-More-Trust', obs = 'auto')\n",
    "    ]),filename='logsAuto-train')\n",
    "    ev_env = VecMonitor(DummyVecEnv([\n",
    "        lambda: tradingEng(paths_ev,action = 'small-More-Trust', obs = 'auto'),\n",
    "    ]))\n",
    "\n",
    "    eval_callback = EvalCallback(\n",
    "        ev_env,\n",
    "        best_model_save_path='./logs/best_modelAutoObs',\n",
    "        log_path='./logs/eval_logsAutoO/ev',\n",
    "        eval_freq=252*8*20,\n",
    "        deterministic=True,\n",
    "        render=False,\n",
    "        verbose = True,\n",
    "        n_eval_episodes = 3\n",
    "    )\n",
    "    model = PPO(\"MlpPolicy\", envs, batch_size = 252*2*5, learning_rate=linear_schedule(0.005), policy_kwargs=policy_kwargs, n_steps=252*4*5, normalize_advantage=True, gamma = 0.9, verbose = 1) \n",
    "\n",
    "    model.learn(total_timesteps=3e6, log_interval=2, callback=eval_callback) \n",
    "except:\n",
    "    print(\"Autoencoder failed, continuing\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Eval num_timesteps=20160, episode_reward=-0.09 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0884     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 20160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009682716 |\n",
      "|    clip_fraction        | 0.147       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -56.6       |\n",
      "|    explained_variance   | 0.0316      |\n",
      "|    learning_rate        | 0.00498     |\n",
      "|    loss                 | -0.00138    |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00673    |\n",
      "|    std                  | 0.996       |\n",
      "|    value_loss           | 0.00274     |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -3.38    |\n",
      "| time/              |          |\n",
      "|    fps             | 735      |\n",
      "|    iterations      | 2        |\n",
      "|    time_elapsed    | 27       |\n",
      "|    total_timesteps | 20160    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=40320, episode_reward=-0.12 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.118      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 40320       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009854713 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -56.5       |\n",
      "|    explained_variance   | 0.156       |\n",
      "|    learning_rate        | 0.00495     |\n",
      "|    loss                 | -0.0115     |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00701    |\n",
      "|    std                  | 0.992       |\n",
      "|    value_loss           | 6.39e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -3.12    |\n",
      "| time/              |          |\n",
      "|    fps             | 704      |\n",
      "|    iterations      | 4        |\n",
      "|    time_elapsed    | 57       |\n",
      "|    total_timesteps | 40320    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=60480, episode_reward=-0.13 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.13       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 60480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009355842 |\n",
      "|    clip_fraction        | 0.136       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -56.4       |\n",
      "|    explained_variance   | 0.0227      |\n",
      "|    learning_rate        | 0.00492     |\n",
      "|    loss                 | -0.00669    |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00567    |\n",
      "|    std                  | 0.991       |\n",
      "|    value_loss           | 0.000174    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -3.05    |\n",
      "| time/              |          |\n",
      "|    fps             | 696      |\n",
      "|    iterations      | 6        |\n",
      "|    time_elapsed    | 86       |\n",
      "|    total_timesteps | 60480    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=80640, episode_reward=-0.12 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.123      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 80640       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009050426 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -56.2       |\n",
      "|    explained_variance   | 0.0242      |\n",
      "|    learning_rate        | 0.00488     |\n",
      "|    loss                 | -0.00542    |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00502    |\n",
      "|    std                  | 0.987       |\n",
      "|    value_loss           | 0.000402    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -3.06    |\n",
      "| time/              |          |\n",
      "|    fps             | 692      |\n",
      "|    iterations      | 8        |\n",
      "|    time_elapsed    | 116      |\n",
      "|    total_timesteps | 80640    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=100800, episode_reward=-0.11 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.113      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 100800      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009416141 |\n",
      "|    clip_fraction        | 0.111       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -56         |\n",
      "|    explained_variance   | 0.169       |\n",
      "|    learning_rate        | 0.00485     |\n",
      "|    loss                 | -0.00594    |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00728    |\n",
      "|    std                  | 0.982       |\n",
      "|    value_loss           | 4.03e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -3.03    |\n",
      "| time/              |          |\n",
      "|    fps             | 690      |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 145      |\n",
      "|    total_timesteps | 100800   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=120960, episode_reward=-0.19 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.188     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 120960     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00975954 |\n",
      "|    clip_fraction        | 0.13       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -55.9      |\n",
      "|    explained_variance   | 0.0472     |\n",
      "|    learning_rate        | 0.00482    |\n",
      "|    loss                 | -0.00969   |\n",
      "|    n_updates            | 110        |\n",
      "|    policy_gradient_loss | -0.00678   |\n",
      "|    std                  | 0.978      |\n",
      "|    value_loss           | 8.13e-05   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -3.02    |\n",
      "| time/              |          |\n",
      "|    fps             | 689      |\n",
      "|    iterations      | 12       |\n",
      "|    time_elapsed    | 175      |\n",
      "|    total_timesteps | 120960   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=141120, episode_reward=-0.13 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.132       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 141120       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0106584085 |\n",
      "|    clip_fraction        | 0.14         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -55.6        |\n",
      "|    explained_variance   | 0.055        |\n",
      "|    learning_rate        | 0.00478      |\n",
      "|    loss                 | -0.0037      |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.00613     |\n",
      "|    std                  | 0.971        |\n",
      "|    value_loss           | 0.000236     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.97    |\n",
      "| time/              |          |\n",
      "|    fps             | 688      |\n",
      "|    iterations      | 14       |\n",
      "|    time_elapsed    | 205      |\n",
      "|    total_timesteps | 141120   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=161280, episode_reward=-0.21 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.209       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 161280       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0100344755 |\n",
      "|    clip_fraction        | 0.152        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -55.4        |\n",
      "|    explained_variance   | 0.0815       |\n",
      "|    learning_rate        | 0.00475      |\n",
      "|    loss                 | -0.0101      |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.006       |\n",
      "|    std                  | 0.967        |\n",
      "|    value_loss           | 0.000174     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.98    |\n",
      "| time/              |          |\n",
      "|    fps             | 686      |\n",
      "|    iterations      | 16       |\n",
      "|    time_elapsed    | 235      |\n",
      "|    total_timesteps | 161280   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=181440, episode_reward=-0.19 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.191      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 181440      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009213639 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -55.1       |\n",
      "|    explained_variance   | 0.152       |\n",
      "|    learning_rate        | 0.00471     |\n",
      "|    loss                 | -0.00208    |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.00535    |\n",
      "|    std                  | 0.96        |\n",
      "|    value_loss           | 0.000121    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.96    |\n",
      "| time/              |          |\n",
      "|    fps             | 685      |\n",
      "|    iterations      | 18       |\n",
      "|    time_elapsed    | 264      |\n",
      "|    total_timesteps | 181440   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=201600, episode_reward=-0.21 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.215      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 201600      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009979226 |\n",
      "|    clip_fraction        | 0.133       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -55.1       |\n",
      "|    explained_variance   | 0.084       |\n",
      "|    learning_rate        | 0.00468     |\n",
      "|    loss                 | -0.00504    |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.00587    |\n",
      "|    std                  | 0.96        |\n",
      "|    value_loss           | 0.000147    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.93    |\n",
      "| time/              |          |\n",
      "|    fps             | 685      |\n",
      "|    iterations      | 20       |\n",
      "|    time_elapsed    | 294      |\n",
      "|    total_timesteps | 201600   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=221760, episode_reward=-0.19 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.187      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 221760      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010259621 |\n",
      "|    clip_fraction        | 0.131       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -54.8       |\n",
      "|    explained_variance   | 0.156       |\n",
      "|    learning_rate        | 0.00465     |\n",
      "|    loss                 | -0.00743    |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.00505    |\n",
      "|    std                  | 0.953       |\n",
      "|    value_loss           | 0.00011     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.89    |\n",
      "| time/              |          |\n",
      "|    fps             | 684      |\n",
      "|    iterations      | 22       |\n",
      "|    time_elapsed    | 323      |\n",
      "|    total_timesteps | 221760   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=241920, episode_reward=-0.22 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.22       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 241920      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010827123 |\n",
      "|    clip_fraction        | 0.134       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -54.7       |\n",
      "|    explained_variance   | 0.0746      |\n",
      "|    learning_rate        | 0.00461     |\n",
      "|    loss                 | -0.0087     |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.00571    |\n",
      "|    std                  | 0.951       |\n",
      "|    value_loss           | 0.000239    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.93    |\n",
      "| time/              |          |\n",
      "|    fps             | 684      |\n",
      "|    iterations      | 24       |\n",
      "|    time_elapsed    | 353      |\n",
      "|    total_timesteps | 241920   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=262080, episode_reward=-0.22 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.217      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 262080      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008588422 |\n",
      "|    clip_fraction        | 0.136       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -54.3       |\n",
      "|    explained_variance   | 0.145       |\n",
      "|    learning_rate        | 0.00458     |\n",
      "|    loss                 | -0.00909    |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.00674    |\n",
      "|    std                  | 0.943       |\n",
      "|    value_loss           | 0.000131    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.91    |\n",
      "| time/              |          |\n",
      "|    fps             | 684      |\n",
      "|    iterations      | 26       |\n",
      "|    time_elapsed    | 382      |\n",
      "|    total_timesteps | 262080   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=282240, episode_reward=-0.20 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.204      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 282240      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010785978 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -54.2       |\n",
      "|    explained_variance   | 0.195       |\n",
      "|    learning_rate        | 0.00455     |\n",
      "|    loss                 | -0.00707    |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.0062     |\n",
      "|    std                  | 0.94        |\n",
      "|    value_loss           | 8.12e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 683      |\n",
      "|    iterations      | 28       |\n",
      "|    time_elapsed    | 413      |\n",
      "|    total_timesteps | 282240   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=302400, episode_reward=-0.22 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.223      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 302400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008849729 |\n",
      "|    clip_fraction        | 0.133       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -54         |\n",
      "|    explained_variance   | 0.232       |\n",
      "|    learning_rate        | 0.00451     |\n",
      "|    loss                 | -0.00417    |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.0052     |\n",
      "|    std                  | 0.935       |\n",
      "|    value_loss           | 0.000185    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.87    |\n",
      "| time/              |          |\n",
      "|    fps             | 682      |\n",
      "|    iterations      | 30       |\n",
      "|    time_elapsed    | 443      |\n",
      "|    total_timesteps | 302400   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=322560, episode_reward=-0.23 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.23       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 322560      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012176675 |\n",
      "|    clip_fraction        | 0.164       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -53.7       |\n",
      "|    explained_variance   | 0.0484      |\n",
      "|    learning_rate        | 0.00448     |\n",
      "|    loss                 | -0.00813    |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.00475    |\n",
      "|    std                  | 0.931       |\n",
      "|    value_loss           | 0.000768    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.89    |\n",
      "| time/              |          |\n",
      "|    fps             | 682      |\n",
      "|    iterations      | 32       |\n",
      "|    time_elapsed    | 472      |\n",
      "|    total_timesteps | 322560   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=342720, episode_reward=-0.31 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.309      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 342720      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010360447 |\n",
      "|    clip_fraction        | 0.164       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -53.5       |\n",
      "|    explained_variance   | 0.142       |\n",
      "|    learning_rate        | 0.00445     |\n",
      "|    loss                 | -0.00883    |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.00663    |\n",
      "|    std                  | 0.926       |\n",
      "|    value_loss           | 0.000309    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.91    |\n",
      "| time/              |          |\n",
      "|    fps             | 682      |\n",
      "|    iterations      | 34       |\n",
      "|    time_elapsed    | 502      |\n",
      "|    total_timesteps | 342720   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=362880, episode_reward=-0.26 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.257      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 362880      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008934909 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -53.3       |\n",
      "|    explained_variance   | 0.413       |\n",
      "|    learning_rate        | 0.00441     |\n",
      "|    loss                 | -0.00499    |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.00615    |\n",
      "|    std                  | 0.921       |\n",
      "|    value_loss           | 3.81e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.87    |\n",
      "| time/              |          |\n",
      "|    fps             | 682      |\n",
      "|    iterations      | 36       |\n",
      "|    time_elapsed    | 531      |\n",
      "|    total_timesteps | 362880   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=383040, episode_reward=-0.23 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.228     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 383040     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01013996 |\n",
      "|    clip_fraction        | 0.136      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -53.1      |\n",
      "|    explained_variance   | 0.254      |\n",
      "|    learning_rate        | 0.00438    |\n",
      "|    loss                 | -0.0089    |\n",
      "|    n_updates            | 370        |\n",
      "|    policy_gradient_loss | -0.00621   |\n",
      "|    std                  | 0.917      |\n",
      "|    value_loss           | 8.63e-05   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.82    |\n",
      "| time/              |          |\n",
      "|    fps             | 682      |\n",
      "|    iterations      | 38       |\n",
      "|    time_elapsed    | 561      |\n",
      "|    total_timesteps | 383040   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=403200, episode_reward=-0.28 +/- 0.09\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.276      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 403200      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011559847 |\n",
      "|    clip_fraction        | 0.155       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -52.8       |\n",
      "|    explained_variance   | 0.204       |\n",
      "|    learning_rate        | 0.00434     |\n",
      "|    loss                 | -0.0078     |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.00636    |\n",
      "|    std                  | 0.911       |\n",
      "|    value_loss           | 0.000198    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 681      |\n",
      "|    iterations      | 40       |\n",
      "|    time_elapsed    | 591      |\n",
      "|    total_timesteps | 403200   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=423360, episode_reward=-0.28 +/- 0.08\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.279      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 423360      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014050762 |\n",
      "|    clip_fraction        | 0.174       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -52.6       |\n",
      "|    explained_variance   | 0.0171      |\n",
      "|    learning_rate        | 0.00431     |\n",
      "|    loss                 | -0.00656    |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.00557    |\n",
      "|    std                  | 0.906       |\n",
      "|    value_loss           | 0.00127     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.79    |\n",
      "| time/              |          |\n",
      "|    fps             | 681      |\n",
      "|    iterations      | 42       |\n",
      "|    time_elapsed    | 620      |\n",
      "|    total_timesteps | 423360   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=443520, episode_reward=-0.29 +/- 0.11\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.288      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 443520      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011007156 |\n",
      "|    clip_fraction        | 0.139       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -52.2       |\n",
      "|    explained_variance   | 0.207       |\n",
      "|    learning_rate        | 0.00428     |\n",
      "|    loss                 | -0.0102     |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.00708    |\n",
      "|    std                  | 0.898       |\n",
      "|    value_loss           | 5.85e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.74    |\n",
      "| time/              |          |\n",
      "|    fps             | 681      |\n",
      "|    iterations      | 44       |\n",
      "|    time_elapsed    | 650      |\n",
      "|    total_timesteps | 443520   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=463680, episode_reward=-0.22 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.219     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 463680     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00949449 |\n",
      "|    clip_fraction        | 0.145      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -51.9      |\n",
      "|    explained_variance   | 0.126      |\n",
      "|    learning_rate        | 0.00424    |\n",
      "|    loss                 | -0.0063    |\n",
      "|    n_updates            | 450        |\n",
      "|    policy_gradient_loss | -0.00547   |\n",
      "|    std                  | 0.892      |\n",
      "|    value_loss           | 0.000299   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.76    |\n",
      "| time/              |          |\n",
      "|    fps             | 682      |\n",
      "|    iterations      | 46       |\n",
      "|    time_elapsed    | 679      |\n",
      "|    total_timesteps | 463680   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=483840, episode_reward=-0.20 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.204      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 483840      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009086529 |\n",
      "|    clip_fraction        | 0.126       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -51.7       |\n",
      "|    explained_variance   | 0.269       |\n",
      "|    learning_rate        | 0.00421     |\n",
      "|    loss                 | -0.0102     |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.00588    |\n",
      "|    std                  | 0.889       |\n",
      "|    value_loss           | 5.92e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.77    |\n",
      "| time/              |          |\n",
      "|    fps             | 682      |\n",
      "|    iterations      | 48       |\n",
      "|    time_elapsed    | 709      |\n",
      "|    total_timesteps | 483840   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=504000, episode_reward=-0.19 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.188      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 504000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009715034 |\n",
      "|    clip_fraction        | 0.131       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -51.5       |\n",
      "|    explained_variance   | 0.143       |\n",
      "|    learning_rate        | 0.00418     |\n",
      "|    loss                 | -0.00574    |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | -0.00557    |\n",
      "|    std                  | 0.886       |\n",
      "|    value_loss           | 0.000146    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.76    |\n",
      "| time/              |          |\n",
      "|    fps             | 682      |\n",
      "|    iterations      | 50       |\n",
      "|    time_elapsed    | 738      |\n",
      "|    total_timesteps | 504000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=524160, episode_reward=-0.19 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.19      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 524160     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01416482 |\n",
      "|    clip_fraction        | 0.191      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -51.3      |\n",
      "|    explained_variance   | 0.203      |\n",
      "|    learning_rate        | 0.00414    |\n",
      "|    loss                 | -0.00701   |\n",
      "|    n_updates            | 510        |\n",
      "|    policy_gradient_loss | -0.00707   |\n",
      "|    std                  | 0.881      |\n",
      "|    value_loss           | 0.000233   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.78    |\n",
      "| time/              |          |\n",
      "|    fps             | 681      |\n",
      "|    iterations      | 52       |\n",
      "|    time_elapsed    | 768      |\n",
      "|    total_timesteps | 524160   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=544320, episode_reward=-0.21 +/- 0.07\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.206      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 544320      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009085706 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -51         |\n",
      "|    explained_variance   | 0.376       |\n",
      "|    learning_rate        | 0.00411     |\n",
      "|    loss                 | -0.00863    |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | -0.00635    |\n",
      "|    std                  | 0.876       |\n",
      "|    value_loss           | 3.09e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.72    |\n",
      "| time/              |          |\n",
      "|    fps             | 682      |\n",
      "|    iterations      | 54       |\n",
      "|    time_elapsed    | 798      |\n",
      "|    total_timesteps | 544320   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=564480, episode_reward=-0.23 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.226      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 564480      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009147963 |\n",
      "|    clip_fraction        | 0.124       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -50.8       |\n",
      "|    explained_variance   | 0.141       |\n",
      "|    learning_rate        | 0.00408     |\n",
      "|    loss                 | -0.00389    |\n",
      "|    n_updates            | 550         |\n",
      "|    policy_gradient_loss | -0.00582    |\n",
      "|    std                  | 0.873       |\n",
      "|    value_loss           | 9.76e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.76    |\n",
      "| time/              |          |\n",
      "|    fps             | 681      |\n",
      "|    iterations      | 56       |\n",
      "|    time_elapsed    | 827      |\n",
      "|    total_timesteps | 564480   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=584640, episode_reward=-0.19 +/- 0.07\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.19       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 584640      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011224518 |\n",
      "|    clip_fraction        | 0.155       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -50.6       |\n",
      "|    explained_variance   | 0.0855      |\n",
      "|    learning_rate        | 0.00404     |\n",
      "|    loss                 | -0.00544    |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | -0.00552    |\n",
      "|    std                  | 0.869       |\n",
      "|    value_loss           | 0.000284    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.75    |\n",
      "| time/              |          |\n",
      "|    fps             | 681      |\n",
      "|    iterations      | 58       |\n",
      "|    time_elapsed    | 857      |\n",
      "|    total_timesteps | 584640   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=604800, episode_reward=-0.24 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.245      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 604800      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014164507 |\n",
      "|    clip_fraction        | 0.192       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -50.6       |\n",
      "|    explained_variance   | 0.147       |\n",
      "|    learning_rate        | 0.00401     |\n",
      "|    loss                 | -0.00695    |\n",
      "|    n_updates            | 590         |\n",
      "|    policy_gradient_loss | -0.00678    |\n",
      "|    std                  | 0.87        |\n",
      "|    value_loss           | 0.000471    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.75    |\n",
      "| time/              |          |\n",
      "|    fps             | 681      |\n",
      "|    iterations      | 60       |\n",
      "|    time_elapsed    | 886      |\n",
      "|    total_timesteps | 604800   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=624960, episode_reward=-0.24 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.243      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 624960      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009847343 |\n",
      "|    clip_fraction        | 0.135       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -50.2       |\n",
      "|    explained_variance   | 0.255       |\n",
      "|    learning_rate        | 0.00398     |\n",
      "|    loss                 | -0.00537    |\n",
      "|    n_updates            | 610         |\n",
      "|    policy_gradient_loss | -0.0069     |\n",
      "|    std                  | 0.86        |\n",
      "|    value_loss           | 6.83e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.71    |\n",
      "| time/              |          |\n",
      "|    fps             | 682      |\n",
      "|    iterations      | 62       |\n",
      "|    time_elapsed    | 916      |\n",
      "|    total_timesteps | 624960   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=645120, episode_reward=-0.23 +/- 0.07\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.229      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 645120      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014941919 |\n",
      "|    clip_fraction        | 0.174       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -49.8       |\n",
      "|    explained_variance   | 0.0274      |\n",
      "|    learning_rate        | 0.00394     |\n",
      "|    loss                 | -0.0051     |\n",
      "|    n_updates            | 630         |\n",
      "|    policy_gradient_loss | -0.00545    |\n",
      "|    std                  | 0.852       |\n",
      "|    value_loss           | 0.002       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.69    |\n",
      "| time/              |          |\n",
      "|    fps             | 681      |\n",
      "|    iterations      | 64       |\n",
      "|    time_elapsed    | 946      |\n",
      "|    total_timesteps | 645120   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=665280, episode_reward=-0.16 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.161      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 665280      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009458582 |\n",
      "|    clip_fraction        | 0.141       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -49.4       |\n",
      "|    explained_variance   | 0.238       |\n",
      "|    learning_rate        | 0.00391     |\n",
      "|    loss                 | -0.00857    |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | -0.00595    |\n",
      "|    std                  | 0.846       |\n",
      "|    value_loss           | 8.2e-05     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.64    |\n",
      "| time/              |          |\n",
      "|    fps             | 681      |\n",
      "|    iterations      | 66       |\n",
      "|    time_elapsed    | 975      |\n",
      "|    total_timesteps | 665280   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=685440, episode_reward=-0.20 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.204      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 685440      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009081196 |\n",
      "|    clip_fraction        | 0.116       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -49.1       |\n",
      "|    explained_variance   | 0.307       |\n",
      "|    learning_rate        | 0.00387     |\n",
      "|    loss                 | -0.00865    |\n",
      "|    n_updates            | 670         |\n",
      "|    policy_gradient_loss | -0.00719    |\n",
      "|    std                  | 0.84        |\n",
      "|    value_loss           | 2.39e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.62    |\n",
      "| time/              |          |\n",
      "|    fps             | 681      |\n",
      "|    iterations      | 68       |\n",
      "|    time_elapsed    | 1005     |\n",
      "|    total_timesteps | 685440   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=705600, episode_reward=-0.16 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.163      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 705600      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009925518 |\n",
      "|    clip_fraction        | 0.133       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -48.8       |\n",
      "|    explained_variance   | 0.489       |\n",
      "|    learning_rate        | 0.00384     |\n",
      "|    loss                 | -0.00837    |\n",
      "|    n_updates            | 690         |\n",
      "|    policy_gradient_loss | -0.00774    |\n",
      "|    std                  | 0.834       |\n",
      "|    value_loss           | 4.88e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.54    |\n",
      "| time/              |          |\n",
      "|    fps             | 681      |\n",
      "|    iterations      | 70       |\n",
      "|    time_elapsed    | 1034     |\n",
      "|    total_timesteps | 705600   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=725760, episode_reward=-0.17 +/- 0.07\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.173      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 725760      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010221669 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -48.4       |\n",
      "|    explained_variance   | 0.1         |\n",
      "|    learning_rate        | 0.00381     |\n",
      "|    loss                 | -0.00282    |\n",
      "|    n_updates            | 710         |\n",
      "|    policy_gradient_loss | -0.00545    |\n",
      "|    std                  | 0.827       |\n",
      "|    value_loss           | 0.000141    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.47    |\n",
      "| time/              |          |\n",
      "|    fps             | 681      |\n",
      "|    iterations      | 72       |\n",
      "|    time_elapsed    | 1064     |\n",
      "|    total_timesteps | 725760   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=745920, episode_reward=-0.21 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.211      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 745920      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009006853 |\n",
      "|    clip_fraction        | 0.116       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -48.1       |\n",
      "|    explained_variance   | 0.479       |\n",
      "|    learning_rate        | 0.00377     |\n",
      "|    loss                 | -0.00955    |\n",
      "|    n_updates            | 730         |\n",
      "|    policy_gradient_loss | -0.00707    |\n",
      "|    std                  | 0.822       |\n",
      "|    value_loss           | 3.02e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.44    |\n",
      "| time/              |          |\n",
      "|    fps             | 681      |\n",
      "|    iterations      | 74       |\n",
      "|    time_elapsed    | 1094     |\n",
      "|    total_timesteps | 745920   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=766080, episode_reward=-0.24 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.241      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 766080      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010121455 |\n",
      "|    clip_fraction        | 0.151       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -47.8       |\n",
      "|    explained_variance   | 0.212       |\n",
      "|    learning_rate        | 0.00374     |\n",
      "|    loss                 | -0.0059     |\n",
      "|    n_updates            | 750         |\n",
      "|    policy_gradient_loss | -0.00754    |\n",
      "|    std                  | 0.817       |\n",
      "|    value_loss           | 8.14e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.43    |\n",
      "| time/              |          |\n",
      "|    fps             | 681      |\n",
      "|    iterations      | 76       |\n",
      "|    time_elapsed    | 1124     |\n",
      "|    total_timesteps | 766080   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=786240, episode_reward=-0.19 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.192      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 786240      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013282727 |\n",
      "|    clip_fraction        | 0.157       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -47.7       |\n",
      "|    explained_variance   | 0.0622      |\n",
      "|    learning_rate        | 0.00371     |\n",
      "|    loss                 | -0.00832    |\n",
      "|    n_updates            | 770         |\n",
      "|    policy_gradient_loss | -0.00689    |\n",
      "|    std                  | 0.814       |\n",
      "|    value_loss           | 0.000222    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.46    |\n",
      "| time/              |          |\n",
      "|    fps             | 681      |\n",
      "|    iterations      | 78       |\n",
      "|    time_elapsed    | 1153     |\n",
      "|    total_timesteps | 786240   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=806400, episode_reward=-0.15 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.15       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 806400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009972886 |\n",
      "|    clip_fraction        | 0.151       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -47.5       |\n",
      "|    explained_variance   | 0.117       |\n",
      "|    learning_rate        | 0.00367     |\n",
      "|    loss                 | -0.00846    |\n",
      "|    n_updates            | 790         |\n",
      "|    policy_gradient_loss | -0.00629    |\n",
      "|    std                  | 0.812       |\n",
      "|    value_loss           | 0.000196    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.43    |\n",
      "| time/              |          |\n",
      "|    fps             | 681      |\n",
      "|    iterations      | 80       |\n",
      "|    time_elapsed    | 1183     |\n",
      "|    total_timesteps | 806400   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=826560, episode_reward=-0.17 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.167      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 826560      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010709739 |\n",
      "|    clip_fraction        | 0.13        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -47.3       |\n",
      "|    explained_variance   | 0.308       |\n",
      "|    learning_rate        | 0.00364     |\n",
      "|    loss                 | -0.00994    |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | -0.00707    |\n",
      "|    std                  | 0.807       |\n",
      "|    value_loss           | 4.03e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.43    |\n",
      "| time/              |          |\n",
      "|    fps             | 681      |\n",
      "|    iterations      | 82       |\n",
      "|    time_elapsed    | 1213     |\n",
      "|    total_timesteps | 826560   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=846720, episode_reward=-0.17 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.168      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 846720      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016157191 |\n",
      "|    clip_fraction        | 0.171       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -46.9       |\n",
      "|    explained_variance   | 0.0542      |\n",
      "|    learning_rate        | 0.00361     |\n",
      "|    loss                 | -0.00719    |\n",
      "|    n_updates            | 830         |\n",
      "|    policy_gradient_loss | -0.00662    |\n",
      "|    std                  | 0.799       |\n",
      "|    value_loss           | 0.000535    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.45    |\n",
      "| time/              |          |\n",
      "|    fps             | 681      |\n",
      "|    iterations      | 84       |\n",
      "|    time_elapsed    | 1242     |\n",
      "|    total_timesteps | 846720   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=866880, episode_reward=-0.23 +/- 0.08\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.228      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 866880      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010007827 |\n",
      "|    clip_fraction        | 0.126       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -46.7       |\n",
      "|    explained_variance   | 0.212       |\n",
      "|    learning_rate        | 0.00357     |\n",
      "|    loss                 | -0.00361    |\n",
      "|    n_updates            | 850         |\n",
      "|    policy_gradient_loss | -0.00577    |\n",
      "|    std                  | 0.798       |\n",
      "|    value_loss           | 9.85e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.49    |\n",
      "| time/              |          |\n",
      "|    fps             | 681      |\n",
      "|    iterations      | 86       |\n",
      "|    time_elapsed    | 1272     |\n",
      "|    total_timesteps | 866880   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=887040, episode_reward=-0.12 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.12      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 887040     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01056627 |\n",
      "|    clip_fraction        | 0.133      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -46.5      |\n",
      "|    explained_variance   | 0.154      |\n",
      "|    learning_rate        | 0.00354    |\n",
      "|    loss                 | -0.00619   |\n",
      "|    n_updates            | 870        |\n",
      "|    policy_gradient_loss | -0.006     |\n",
      "|    std                  | 0.796      |\n",
      "|    value_loss           | 0.000111   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.49    |\n",
      "| time/              |          |\n",
      "|    fps             | 681      |\n",
      "|    iterations      | 88       |\n",
      "|    time_elapsed    | 1302     |\n",
      "|    total_timesteps | 887040   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=907200, episode_reward=-0.17 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.167      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 907200      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009819564 |\n",
      "|    clip_fraction        | 0.123       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -46.2       |\n",
      "|    explained_variance   | 0.222       |\n",
      "|    learning_rate        | 0.0035      |\n",
      "|    loss                 | -0.00824    |\n",
      "|    n_updates            | 890         |\n",
      "|    policy_gradient_loss | -0.00585    |\n",
      "|    std                  | 0.79        |\n",
      "|    value_loss           | 8.49e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.49    |\n",
      "| time/              |          |\n",
      "|    fps             | 681      |\n",
      "|    iterations      | 90       |\n",
      "|    time_elapsed    | 1332     |\n",
      "|    total_timesteps | 907200   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=927360, episode_reward=-0.16 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.163      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 927360      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008957136 |\n",
      "|    clip_fraction        | 0.109       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.9       |\n",
      "|    explained_variance   | 0.216       |\n",
      "|    learning_rate        | 0.00347     |\n",
      "|    loss                 | -0.00274    |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | -0.00629    |\n",
      "|    std                  | 0.785       |\n",
      "|    value_loss           | 3.34e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.42    |\n",
      "| time/              |          |\n",
      "|    fps             | 681      |\n",
      "|    iterations      | 92       |\n",
      "|    time_elapsed    | 1361     |\n",
      "|    total_timesteps | 927360   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=947520, episode_reward=-0.17 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.174      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 947520      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009600607 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.7       |\n",
      "|    explained_variance   | 0.378       |\n",
      "|    learning_rate        | 0.00344     |\n",
      "|    loss                 | -0.0102     |\n",
      "|    n_updates            | 930         |\n",
      "|    policy_gradient_loss | -0.0072     |\n",
      "|    std                  | 0.781       |\n",
      "|    value_loss           | 2.84e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.37    |\n",
      "| time/              |          |\n",
      "|    fps             | 680      |\n",
      "|    iterations      | 94       |\n",
      "|    time_elapsed    | 1391     |\n",
      "|    total_timesteps | 947520   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=967680, episode_reward=-0.26 +/- 0.11\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.264       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 967680       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0101746265 |\n",
      "|    clip_fraction        | 0.136        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -45.4        |\n",
      "|    explained_variance   | 0.173        |\n",
      "|    learning_rate        | 0.0034       |\n",
      "|    loss                 | -0.00935     |\n",
      "|    n_updates            | 950          |\n",
      "|    policy_gradient_loss | -0.00649     |\n",
      "|    std                  | 0.777        |\n",
      "|    value_loss           | 8.89e-05     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.36    |\n",
      "| time/              |          |\n",
      "|    fps             | 681      |\n",
      "|    iterations      | 96       |\n",
      "|    time_elapsed    | 1420     |\n",
      "|    total_timesteps | 967680   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=987840, episode_reward=-0.18 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.179      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 987840      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010219496 |\n",
      "|    clip_fraction        | 0.125       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.2       |\n",
      "|    explained_variance   | 0.0972      |\n",
      "|    learning_rate        | 0.00337     |\n",
      "|    loss                 | -0.00442    |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | -0.00514    |\n",
      "|    std                  | 0.775       |\n",
      "|    value_loss           | 0.000158    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.33    |\n",
      "| time/              |          |\n",
      "|    fps             | 680      |\n",
      "|    iterations      | 98       |\n",
      "|    time_elapsed    | 1450     |\n",
      "|    total_timesteps | 987840   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1008000, episode_reward=-0.17 +/- 0.08\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.17      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1008000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00978373 |\n",
      "|    clip_fraction        | 0.138      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -44.8      |\n",
      "|    explained_variance   | 0.208      |\n",
      "|    learning_rate        | 0.00334    |\n",
      "|    loss                 | -0.00792   |\n",
      "|    n_updates            | 990        |\n",
      "|    policy_gradient_loss | -0.00701   |\n",
      "|    std                  | 0.768      |\n",
      "|    value_loss           | 3.72e-05   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.28    |\n",
      "| time/              |          |\n",
      "|    fps             | 680      |\n",
      "|    iterations      | 100      |\n",
      "|    time_elapsed    | 1480     |\n",
      "|    total_timesteps | 1008000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1028160, episode_reward=-0.16 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.159      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1028160     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012730019 |\n",
      "|    clip_fraction        | 0.173       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.6       |\n",
      "|    explained_variance   | 0.108       |\n",
      "|    learning_rate        | 0.0033      |\n",
      "|    loss                 | -0.00724    |\n",
      "|    n_updates            | 1010        |\n",
      "|    policy_gradient_loss | -0.00691    |\n",
      "|    std                  | 0.764       |\n",
      "|    value_loss           | 0.000218    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.24    |\n",
      "| time/              |          |\n",
      "|    fps             | 680      |\n",
      "|    iterations      | 102      |\n",
      "|    time_elapsed    | 1510     |\n",
      "|    total_timesteps | 1028160  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1048320, episode_reward=-0.14 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.144      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1048320     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009909568 |\n",
      "|    clip_fraction        | 0.131       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.2       |\n",
      "|    explained_variance   | 0.168       |\n",
      "|    learning_rate        | 0.00327     |\n",
      "|    loss                 | -0.0132     |\n",
      "|    n_updates            | 1030        |\n",
      "|    policy_gradient_loss | -0.00795    |\n",
      "|    std                  | 0.758       |\n",
      "|    value_loss           | 2.74e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.25    |\n",
      "| time/              |          |\n",
      "|    fps             | 680      |\n",
      "|    iterations      | 104      |\n",
      "|    time_elapsed    | 1540     |\n",
      "|    total_timesteps | 1048320  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1068480, episode_reward=-0.15 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.146      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1068480     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008709909 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.8       |\n",
      "|    explained_variance   | 0.265       |\n",
      "|    learning_rate        | 0.00324     |\n",
      "|    loss                 | -0.0099     |\n",
      "|    n_updates            | 1050        |\n",
      "|    policy_gradient_loss | -0.00683    |\n",
      "|    std                  | 0.752       |\n",
      "|    value_loss           | 2.24e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.23    |\n",
      "| time/              |          |\n",
      "|    fps             | 680      |\n",
      "|    iterations      | 106      |\n",
      "|    time_elapsed    | 1569     |\n",
      "|    total_timesteps | 1068480  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1088640, episode_reward=-0.17 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.172      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1088640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008887848 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.5       |\n",
      "|    explained_variance   | 0.32        |\n",
      "|    learning_rate        | 0.0032      |\n",
      "|    loss                 | -0.0112     |\n",
      "|    n_updates            | 1070        |\n",
      "|    policy_gradient_loss | -0.00732    |\n",
      "|    std                  | 0.748       |\n",
      "|    value_loss           | 2.01e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 680      |\n",
      "|    iterations      | 108      |\n",
      "|    time_elapsed    | 1599     |\n",
      "|    total_timesteps | 1088640  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1108800, episode_reward=-0.13 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.126      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1108800     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008490067 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.1       |\n",
      "|    explained_variance   | 0.379       |\n",
      "|    learning_rate        | 0.00317     |\n",
      "|    loss                 | -0.013      |\n",
      "|    n_updates            | 1090        |\n",
      "|    policy_gradient_loss | -0.00741    |\n",
      "|    std                  | 0.741       |\n",
      "|    value_loss           | 1.72e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.15    |\n",
      "| time/              |          |\n",
      "|    fps             | 680      |\n",
      "|    iterations      | 110      |\n",
      "|    time_elapsed    | 1628     |\n",
      "|    total_timesteps | 1108800  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1128960, episode_reward=-0.15 +/- 0.07\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.152      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1128960     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008977653 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.7       |\n",
      "|    explained_variance   | 0.381       |\n",
      "|    learning_rate        | 0.00314     |\n",
      "|    loss                 | -0.0076     |\n",
      "|    n_updates            | 1110        |\n",
      "|    policy_gradient_loss | -0.00698    |\n",
      "|    std                  | 0.735       |\n",
      "|    value_loss           | 3.54e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.16    |\n",
      "| time/              |          |\n",
      "|    fps             | 680      |\n",
      "|    iterations      | 112      |\n",
      "|    time_elapsed    | 1658     |\n",
      "|    total_timesteps | 1128960  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1149120, episode_reward=-0.15 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.147      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1149120     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014031097 |\n",
      "|    clip_fraction        | 0.159       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.4       |\n",
      "|    explained_variance   | 0.0473      |\n",
      "|    learning_rate        | 0.0031      |\n",
      "|    loss                 | -0.00708    |\n",
      "|    n_updates            | 1130        |\n",
      "|    policy_gradient_loss | -0.00575    |\n",
      "|    std                  | 0.73        |\n",
      "|    value_loss           | 0.000342    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.15    |\n",
      "| time/              |          |\n",
      "|    fps             | 680      |\n",
      "|    iterations      | 114      |\n",
      "|    time_elapsed    | 1688     |\n",
      "|    total_timesteps | 1149120  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1169280, episode_reward=-0.13 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.129     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1169280    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00872379 |\n",
      "|    clip_fraction        | 0.11       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -42.2      |\n",
      "|    explained_variance   | 0.45       |\n",
      "|    learning_rate        | 0.00307    |\n",
      "|    loss                 | -0.00589   |\n",
      "|    n_updates            | 1150       |\n",
      "|    policy_gradient_loss | -0.00688   |\n",
      "|    std                  | 0.728      |\n",
      "|    value_loss           | 2.23e-05   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.15    |\n",
      "| time/              |          |\n",
      "|    fps             | 680      |\n",
      "|    iterations      | 116      |\n",
      "|    time_elapsed    | 1718     |\n",
      "|    total_timesteps | 1169280  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1189440, episode_reward=-0.11 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.111      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1189440     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009973219 |\n",
      "|    clip_fraction        | 0.138       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.8       |\n",
      "|    explained_variance   | 0.177       |\n",
      "|    learning_rate        | 0.00303     |\n",
      "|    loss                 | -0.00597    |\n",
      "|    n_updates            | 1170        |\n",
      "|    policy_gradient_loss | -0.00624    |\n",
      "|    std                  | 0.721       |\n",
      "|    value_loss           | 7.12e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.13    |\n",
      "| time/              |          |\n",
      "|    fps             | 680      |\n",
      "|    iterations      | 118      |\n",
      "|    time_elapsed    | 1748     |\n",
      "|    total_timesteps | 1189440  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1209600, episode_reward=-0.11 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.114      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1209600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013266256 |\n",
      "|    clip_fraction        | 0.159       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.5       |\n",
      "|    explained_variance   | 0.185       |\n",
      "|    learning_rate        | 0.003       |\n",
      "|    loss                 | -0.00746    |\n",
      "|    n_updates            | 1190        |\n",
      "|    policy_gradient_loss | -0.00681    |\n",
      "|    std                  | 0.715       |\n",
      "|    value_loss           | 0.000129    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.11    |\n",
      "| time/              |          |\n",
      "|    fps             | 680      |\n",
      "|    iterations      | 120      |\n",
      "|    time_elapsed    | 1777     |\n",
      "|    total_timesteps | 1209600  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1229760, episode_reward=-0.12 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.118     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1229760    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01110748 |\n",
      "|    clip_fraction        | 0.135      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -41.2      |\n",
      "|    explained_variance   | 0.322      |\n",
      "|    learning_rate        | 0.00297    |\n",
      "|    loss                 | -0.00853   |\n",
      "|    n_updates            | 1210       |\n",
      "|    policy_gradient_loss | -0.00658   |\n",
      "|    std                  | 0.71       |\n",
      "|    value_loss           | 6.37e-05   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.12    |\n",
      "| time/              |          |\n",
      "|    fps             | 680      |\n",
      "|    iterations      | 122      |\n",
      "|    time_elapsed    | 1807     |\n",
      "|    total_timesteps | 1229760  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1249920, episode_reward=-0.11 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.113      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1249920     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008448817 |\n",
      "|    clip_fraction        | 0.111       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -40.9       |\n",
      "|    explained_variance   | 0.302       |\n",
      "|    learning_rate        | 0.00293     |\n",
      "|    loss                 | -0.00837    |\n",
      "|    n_updates            | 1230        |\n",
      "|    policy_gradient_loss | -0.00594    |\n",
      "|    std                  | 0.706       |\n",
      "|    value_loss           | 2.05e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 680      |\n",
      "|    iterations      | 124      |\n",
      "|    time_elapsed    | 1837     |\n",
      "|    total_timesteps | 1249920  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1270080, episode_reward=-0.11 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.109     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1270080    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01071712 |\n",
      "|    clip_fraction        | 0.128      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -40.6      |\n",
      "|    explained_variance   | 0.107      |\n",
      "|    learning_rate        | 0.0029     |\n",
      "|    loss                 | -0.00812   |\n",
      "|    n_updates            | 1250       |\n",
      "|    policy_gradient_loss | -0.00541   |\n",
      "|    std                  | 0.7        |\n",
      "|    value_loss           | 0.000145   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.08    |\n",
      "| time/              |          |\n",
      "|    fps             | 680      |\n",
      "|    iterations      | 126      |\n",
      "|    time_elapsed    | 1867     |\n",
      "|    total_timesteps | 1270080  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1290240, episode_reward=-0.09 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0915     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1290240     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009046707 |\n",
      "|    clip_fraction        | 0.107       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -40.2       |\n",
      "|    explained_variance   | 0.521       |\n",
      "|    learning_rate        | 0.00287     |\n",
      "|    loss                 | -0.0105     |\n",
      "|    n_updates            | 1270        |\n",
      "|    policy_gradient_loss | -0.00619    |\n",
      "|    std                  | 0.692       |\n",
      "|    value_loss           | 1.75e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.02    |\n",
      "| time/              |          |\n",
      "|    fps             | 679      |\n",
      "|    iterations      | 128      |\n",
      "|    time_elapsed    | 1897     |\n",
      "|    total_timesteps | 1290240  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1310400, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0916     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1310400     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010664428 |\n",
      "|    clip_fraction        | 0.125       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -39.9       |\n",
      "|    explained_variance   | 0.159       |\n",
      "|    learning_rate        | 0.00283     |\n",
      "|    loss                 | -0.00755    |\n",
      "|    n_updates            | 1290        |\n",
      "|    policy_gradient_loss | -0.00601    |\n",
      "|    std                  | 0.688       |\n",
      "|    value_loss           | 0.000109    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.03    |\n",
      "| time/              |          |\n",
      "|    fps             | 679      |\n",
      "|    iterations      | 130      |\n",
      "|    time_elapsed    | 1927     |\n",
      "|    total_timesteps | 1310400  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1330560, episode_reward=-0.08 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0778     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1330560     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011474512 |\n",
      "|    clip_fraction        | 0.136       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -39.6       |\n",
      "|    explained_variance   | 0.0979      |\n",
      "|    learning_rate        | 0.0028      |\n",
      "|    loss                 | -0.00903    |\n",
      "|    n_updates            | 1310        |\n",
      "|    policy_gradient_loss | -0.00576    |\n",
      "|    std                  | 0.685       |\n",
      "|    value_loss           | 7.63e-05    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.04    |\n",
      "| time/              |          |\n",
      "|    fps             | 679      |\n",
      "|    iterations      | 132      |\n",
      "|    time_elapsed    | 1957     |\n",
      "|    total_timesteps | 1330560  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1350720, episode_reward=-0.10 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0995     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1350720     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011611998 |\n",
      "|    clip_fraction        | 0.138       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -39.4       |\n",
      "|    explained_variance   | 0.158       |\n",
      "|    learning_rate        | 0.00277     |\n",
      "|    loss                 | -0.00919    |\n",
      "|    n_updates            | 1330        |\n",
      "|    policy_gradient_loss | -0.0051     |\n",
      "|    std                  | 0.681       |\n",
      "|    value_loss           | 0.000137    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.04    |\n",
      "| time/              |          |\n",
      "|    fps             | 679      |\n",
      "|    iterations      | 134      |\n",
      "|    time_elapsed    | 1987     |\n",
      "|    total_timesteps | 1350720  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1370880, episode_reward=-0.08 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0773      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1370880      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0090326015 |\n",
      "|    clip_fraction        | 0.113        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -39.1        |\n",
      "|    explained_variance   | 0.328        |\n",
      "|    learning_rate        | 0.00273      |\n",
      "|    loss                 | -0.00426     |\n",
      "|    n_updates            | 1350         |\n",
      "|    policy_gradient_loss | -0.0064      |\n",
      "|    std                  | 0.677        |\n",
      "|    value_loss           | 2.79e-05     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 679      |\n",
      "|    iterations      | 136      |\n",
      "|    time_elapsed    | 2016     |\n",
      "|    total_timesteps | 1370880  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1391040, episode_reward=-0.09 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0906     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1391040     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011052641 |\n",
      "|    clip_fraction        | 0.138       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -38.9       |\n",
      "|    explained_variance   | 0.155       |\n",
      "|    learning_rate        | 0.0027      |\n",
      "|    loss                 | -0.00821    |\n",
      "|    n_updates            | 1370        |\n",
      "|    policy_gradient_loss | -0.00609    |\n",
      "|    std                  | 0.675       |\n",
      "|    value_loss           | 7.61e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.96    |\n",
      "| time/              |          |\n",
      "|    fps             | 679      |\n",
      "|    iterations      | 138      |\n",
      "|    time_elapsed    | 2046     |\n",
      "|    total_timesteps | 1391040  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1411200, episode_reward=-0.11 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.11        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1411200      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074806483 |\n",
      "|    clip_fraction        | 0.102        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -38.6        |\n",
      "|    explained_variance   | 0.334        |\n",
      "|    learning_rate        | 0.00266      |\n",
      "|    loss                 | -0.00616     |\n",
      "|    n_updates            | 1390         |\n",
      "|    policy_gradient_loss | -0.00576     |\n",
      "|    std                  | 0.671        |\n",
      "|    value_loss           | 1.9e-05      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.98    |\n",
      "| time/              |          |\n",
      "|    fps             | 679      |\n",
      "|    iterations      | 140      |\n",
      "|    time_elapsed    | 2076     |\n",
      "|    total_timesteps | 1411200  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1431360, episode_reward=-0.11 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.112      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1431360     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008930108 |\n",
      "|    clip_fraction        | 0.124       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -38.2       |\n",
      "|    explained_variance   | 0.251       |\n",
      "|    learning_rate        | 0.00263     |\n",
      "|    loss                 | -0.0101     |\n",
      "|    n_updates            | 1410        |\n",
      "|    policy_gradient_loss | -0.00687    |\n",
      "|    std                  | 0.664       |\n",
      "|    value_loss           | 2.62e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.96    |\n",
      "| time/              |          |\n",
      "|    fps             | 679      |\n",
      "|    iterations      | 142      |\n",
      "|    time_elapsed    | 2105     |\n",
      "|    total_timesteps | 1431360  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1451520, episode_reward=-0.09 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0909     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1451520     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009761313 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -37.9       |\n",
      "|    explained_variance   | 0.21        |\n",
      "|    learning_rate        | 0.0026      |\n",
      "|    loss                 | -0.00747    |\n",
      "|    n_updates            | 1430        |\n",
      "|    policy_gradient_loss | -0.00559    |\n",
      "|    std                  | 0.659       |\n",
      "|    value_loss           | 6.6e-05     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.93    |\n",
      "| time/              |          |\n",
      "|    fps             | 679      |\n",
      "|    iterations      | 144      |\n",
      "|    time_elapsed    | 2135     |\n",
      "|    total_timesteps | 1451520  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1471680, episode_reward=-0.09 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0889     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1471680     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009104094 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -37.4       |\n",
      "|    explained_variance   | 0.321       |\n",
      "|    learning_rate        | 0.00256     |\n",
      "|    loss                 | -0.00488    |\n",
      "|    n_updates            | 1450        |\n",
      "|    policy_gradient_loss | -0.00649    |\n",
      "|    std                  | 0.652       |\n",
      "|    value_loss           | 2.3e-05     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.89    |\n",
      "| time/              |          |\n",
      "|    fps             | 679      |\n",
      "|    iterations      | 146      |\n",
      "|    time_elapsed    | 2165     |\n",
      "|    total_timesteps | 1471680  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1491840, episode_reward=-0.10 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0991     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1491840     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009538107 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -37         |\n",
      "|    explained_variance   | 0.151       |\n",
      "|    learning_rate        | 0.00253     |\n",
      "|    loss                 | -0.0082     |\n",
      "|    n_updates            | 1470        |\n",
      "|    policy_gradient_loss | -0.00627    |\n",
      "|    std                  | 0.647       |\n",
      "|    value_loss           | 3.62e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.87    |\n",
      "| time/              |          |\n",
      "|    fps             | 678      |\n",
      "|    iterations      | 148      |\n",
      "|    time_elapsed    | 2197     |\n",
      "|    total_timesteps | 1491840  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1512000, episode_reward=-0.10 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0987     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1512000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009021915 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -36.7       |\n",
      "|    explained_variance   | 0.266       |\n",
      "|    learning_rate        | 0.0025      |\n",
      "|    loss                 | -0.00871    |\n",
      "|    n_updates            | 1490        |\n",
      "|    policy_gradient_loss | -0.00551    |\n",
      "|    std                  | 0.642       |\n",
      "|    value_loss           | 4.93e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.87    |\n",
      "| time/              |          |\n",
      "|    fps             | 678      |\n",
      "|    iterations      | 150      |\n",
      "|    time_elapsed    | 2229     |\n",
      "|    total_timesteps | 1512000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1532160, episode_reward=-0.11 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.112      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1532160     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011084644 |\n",
      "|    clip_fraction        | 0.145       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -36.4       |\n",
      "|    explained_variance   | 0.171       |\n",
      "|    learning_rate        | 0.00246     |\n",
      "|    loss                 | -0.0101     |\n",
      "|    n_updates            | 1510        |\n",
      "|    policy_gradient_loss | -0.0068     |\n",
      "|    std                  | 0.638       |\n",
      "|    value_loss           | 7.02e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.86    |\n",
      "| time/              |          |\n",
      "|    fps             | 676      |\n",
      "|    iterations      | 152      |\n",
      "|    time_elapsed    | 2264     |\n",
      "|    total_timesteps | 1532160  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1552320, episode_reward=-0.11 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.112      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1552320     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009738391 |\n",
      "|    clip_fraction        | 0.126       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -36.1       |\n",
      "|    explained_variance   | 0.401       |\n",
      "|    learning_rate        | 0.00243     |\n",
      "|    loss                 | -0.00611    |\n",
      "|    n_updates            | 1530        |\n",
      "|    policy_gradient_loss | -0.00597    |\n",
      "|    std                  | 0.633       |\n",
      "|    value_loss           | 5.43e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.86    |\n",
      "| time/              |          |\n",
      "|    fps             | 673      |\n",
      "|    iterations      | 154      |\n",
      "|    time_elapsed    | 2303     |\n",
      "|    total_timesteps | 1552320  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1572480, episode_reward=-0.10 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0973     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1572480     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009764703 |\n",
      "|    clip_fraction        | 0.124       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -35.9       |\n",
      "|    explained_variance   | 0.141       |\n",
      "|    learning_rate        | 0.0024      |\n",
      "|    loss                 | -0.00789    |\n",
      "|    n_updates            | 1550        |\n",
      "|    policy_gradient_loss | -0.00587    |\n",
      "|    std                  | 0.631       |\n",
      "|    value_loss           | 0.00011     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.84    |\n",
      "| time/              |          |\n",
      "|    fps             | 672      |\n",
      "|    iterations      | 156      |\n",
      "|    time_elapsed    | 2339     |\n",
      "|    total_timesteps | 1572480  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1592640, episode_reward=-0.12 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.117      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1592640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010686531 |\n",
      "|    clip_fraction        | 0.142       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -35.4       |\n",
      "|    explained_variance   | 0.132       |\n",
      "|    learning_rate        | 0.00236     |\n",
      "|    loss                 | -0.00935    |\n",
      "|    n_updates            | 1570        |\n",
      "|    policy_gradient_loss | -0.00714    |\n",
      "|    std                  | 0.624       |\n",
      "|    value_loss           | 5.14e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.84    |\n",
      "| time/              |          |\n",
      "|    fps             | 671      |\n",
      "|    iterations      | 158      |\n",
      "|    time_elapsed    | 2370     |\n",
      "|    total_timesteps | 1592640  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1612800, episode_reward=-0.12 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.117      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1612800     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013055037 |\n",
      "|    clip_fraction        | 0.14        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -35.1       |\n",
      "|    explained_variance   | 0.079       |\n",
      "|    learning_rate        | 0.00233     |\n",
      "|    loss                 | -0.0071     |\n",
      "|    n_updates            | 1590        |\n",
      "|    policy_gradient_loss | -0.00579    |\n",
      "|    std                  | 0.619       |\n",
      "|    value_loss           | 0.000276    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.88    |\n",
      "| time/              |          |\n",
      "|    fps             | 671      |\n",
      "|    iterations      | 160      |\n",
      "|    time_elapsed    | 2401     |\n",
      "|    total_timesteps | 1612800  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1632960, episode_reward=-0.12 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.118      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1632960     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009267346 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -34.9       |\n",
      "|    explained_variance   | 0.453       |\n",
      "|    learning_rate        | 0.0023      |\n",
      "|    loss                 | -0.00774    |\n",
      "|    n_updates            | 1610        |\n",
      "|    policy_gradient_loss | -0.00646    |\n",
      "|    std                  | 0.617       |\n",
      "|    value_loss           | 2.94e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.88    |\n",
      "| time/              |          |\n",
      "|    fps             | 671      |\n",
      "|    iterations      | 162      |\n",
      "|    time_elapsed    | 2433     |\n",
      "|    total_timesteps | 1632960  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1653120, episode_reward=-0.13 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.131      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1653120     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007263256 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -34.5       |\n",
      "|    explained_variance   | 0.34        |\n",
      "|    learning_rate        | 0.00226     |\n",
      "|    loss                 | -0.009      |\n",
      "|    n_updates            | 1630        |\n",
      "|    policy_gradient_loss | -0.00703    |\n",
      "|    std                  | 0.612       |\n",
      "|    value_loss           | 1.07e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.86    |\n",
      "| time/              |          |\n",
      "|    fps             | 670      |\n",
      "|    iterations      | 164      |\n",
      "|    time_elapsed    | 2464     |\n",
      "|    total_timesteps | 1653120  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1673280, episode_reward=-0.09 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0897     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1673280     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012072489 |\n",
      "|    clip_fraction        | 0.144       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -34.3       |\n",
      "|    explained_variance   | 0.0268      |\n",
      "|    learning_rate        | 0.00223     |\n",
      "|    loss                 | -0.00506    |\n",
      "|    n_updates            | 1650        |\n",
      "|    policy_gradient_loss | -0.00467    |\n",
      "|    std                  | 0.609       |\n",
      "|    value_loss           | 0.000802    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.85    |\n",
      "| time/              |          |\n",
      "|    fps             | 670      |\n",
      "|    iterations      | 166      |\n",
      "|    time_elapsed    | 2495     |\n",
      "|    total_timesteps | 1673280  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1693440, episode_reward=-0.12 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.116      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1693440     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009588803 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -33.9       |\n",
      "|    explained_variance   | 0.265       |\n",
      "|    learning_rate        | 0.00219     |\n",
      "|    loss                 | -0.0091     |\n",
      "|    n_updates            | 1670        |\n",
      "|    policy_gradient_loss | -0.00645    |\n",
      "|    std                  | 0.604       |\n",
      "|    value_loss           | 4.6e-05     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.85    |\n",
      "| time/              |          |\n",
      "|    fps             | 670      |\n",
      "|    iterations      | 168      |\n",
      "|    time_elapsed    | 2526     |\n",
      "|    total_timesteps | 1693440  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1713600, episode_reward=-0.12 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.12        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1713600      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0120333955 |\n",
      "|    clip_fraction        | 0.149        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -33.7        |\n",
      "|    explained_variance   | 0.0404       |\n",
      "|    learning_rate        | 0.00216      |\n",
      "|    loss                 | -0.00514     |\n",
      "|    n_updates            | 1690         |\n",
      "|    policy_gradient_loss | -0.00647     |\n",
      "|    std                  | 0.602        |\n",
      "|    value_loss           | 0.000125     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.85    |\n",
      "| time/              |          |\n",
      "|    fps             | 670      |\n",
      "|    iterations      | 170      |\n",
      "|    time_elapsed    | 2557     |\n",
      "|    total_timesteps | 1713600  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1733760, episode_reward=-0.14 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.144      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1733760     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010732353 |\n",
      "|    clip_fraction        | 0.125       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -33.5       |\n",
      "|    explained_variance   | 0.119       |\n",
      "|    learning_rate        | 0.00213     |\n",
      "|    loss                 | -0.0101     |\n",
      "|    n_updates            | 1710        |\n",
      "|    policy_gradient_loss | -0.00541    |\n",
      "|    std                  | 0.599       |\n",
      "|    value_loss           | 7.43e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.82    |\n",
      "| time/              |          |\n",
      "|    fps             | 669      |\n",
      "|    iterations      | 172      |\n",
      "|    time_elapsed    | 2588     |\n",
      "|    total_timesteps | 1733760  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1753920, episode_reward=-0.16 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.16       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1753920     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014473841 |\n",
      "|    clip_fraction        | 0.163       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -33.2       |\n",
      "|    explained_variance   | 0.0196      |\n",
      "|    learning_rate        | 0.00209     |\n",
      "|    loss                 | -0.00759    |\n",
      "|    n_updates            | 1730        |\n",
      "|    policy_gradient_loss | -0.00629    |\n",
      "|    std                  | 0.593       |\n",
      "|    value_loss           | 0.000665    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.83    |\n",
      "| time/              |          |\n",
      "|    fps             | 669      |\n",
      "|    iterations      | 174      |\n",
      "|    time_elapsed    | 2619     |\n",
      "|    total_timesteps | 1753920  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1774080, episode_reward=-0.14 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.139      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1774080     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011285733 |\n",
      "|    clip_fraction        | 0.134       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -32.7       |\n",
      "|    explained_variance   | 0.0998      |\n",
      "|    learning_rate        | 0.00206     |\n",
      "|    loss                 | -0.0113     |\n",
      "|    n_updates            | 1750        |\n",
      "|    policy_gradient_loss | -0.00615    |\n",
      "|    std                  | 0.588       |\n",
      "|    value_loss           | 0.000121    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.87    |\n",
      "| time/              |          |\n",
      "|    fps             | 669      |\n",
      "|    iterations      | 176      |\n",
      "|    time_elapsed    | 2650     |\n",
      "|    total_timesteps | 1774080  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1794240, episode_reward=-0.13 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.132     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1794240    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00891403 |\n",
      "|    clip_fraction        | 0.106      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -32.5      |\n",
      "|    explained_variance   | 0.184      |\n",
      "|    learning_rate        | 0.00203    |\n",
      "|    loss                 | -0.00614   |\n",
      "|    n_updates            | 1770       |\n",
      "|    policy_gradient_loss | -0.00529   |\n",
      "|    std                  | 0.586      |\n",
      "|    value_loss           | 4.64e-05   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.82    |\n",
      "| time/              |          |\n",
      "|    fps             | 669      |\n",
      "|    iterations      | 178      |\n",
      "|    time_elapsed    | 2681     |\n",
      "|    total_timesteps | 1794240  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1814400, episode_reward=-0.15 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.154      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1814400     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009995913 |\n",
      "|    clip_fraction        | 0.112       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -32.3       |\n",
      "|    explained_variance   | 0.152       |\n",
      "|    learning_rate        | 0.00199     |\n",
      "|    loss                 | -0.00768    |\n",
      "|    n_updates            | 1790        |\n",
      "|    policy_gradient_loss | -0.00561    |\n",
      "|    std                  | 0.583       |\n",
      "|    value_loss           | 8.31e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.83    |\n",
      "| time/              |          |\n",
      "|    fps             | 668      |\n",
      "|    iterations      | 180      |\n",
      "|    time_elapsed    | 2712     |\n",
      "|    total_timesteps | 1814400  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1834560, episode_reward=-0.14 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.142      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1834560     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012383725 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -32         |\n",
      "|    explained_variance   | 0.067       |\n",
      "|    learning_rate        | 0.00196     |\n",
      "|    loss                 | -0.00772    |\n",
      "|    n_updates            | 1810        |\n",
      "|    policy_gradient_loss | -0.00529    |\n",
      "|    std                  | 0.58        |\n",
      "|    value_loss           | 0.000176    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.87    |\n",
      "| time/              |          |\n",
      "|    fps             | 668      |\n",
      "|    iterations      | 182      |\n",
      "|    time_elapsed    | 2743     |\n",
      "|    total_timesteps | 1834560  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1854720, episode_reward=-0.11 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.113      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1854720     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011036063 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -31.8       |\n",
      "|    explained_variance   | 0.045       |\n",
      "|    learning_rate        | 0.00193     |\n",
      "|    loss                 | -0.00657    |\n",
      "|    n_updates            | 1830        |\n",
      "|    policy_gradient_loss | -0.00619    |\n",
      "|    std                  | 0.578       |\n",
      "|    value_loss           | 4.84e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.82    |\n",
      "| time/              |          |\n",
      "|    fps             | 668      |\n",
      "|    iterations      | 184      |\n",
      "|    time_elapsed    | 2775     |\n",
      "|    total_timesteps | 1854720  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1874880, episode_reward=-0.13 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.133      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1874880     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013772836 |\n",
      "|    clip_fraction        | 0.134       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -31.7       |\n",
      "|    explained_variance   | 0.0669      |\n",
      "|    learning_rate        | 0.00189     |\n",
      "|    loss                 | -0.00826    |\n",
      "|    n_updates            | 1850        |\n",
      "|    policy_gradient_loss | -0.00581    |\n",
      "|    std                  | 0.577       |\n",
      "|    value_loss           | 0.000176    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.77    |\n",
      "| time/              |          |\n",
      "|    fps             | 667      |\n",
      "|    iterations      | 186      |\n",
      "|    time_elapsed    | 2808     |\n",
      "|    total_timesteps | 1874880  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1895040, episode_reward=-0.12 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1.26e+03  |\n",
      "|    mean_reward          | -0.117    |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 1895040   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0099869 |\n",
      "|    clip_fraction        | 0.107     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -31.3     |\n",
      "|    explained_variance   | 0.176     |\n",
      "|    learning_rate        | 0.00186   |\n",
      "|    loss                 | -0.00774  |\n",
      "|    n_updates            | 1870      |\n",
      "|    policy_gradient_loss | -0.00583  |\n",
      "|    std                  | 0.572     |\n",
      "|    value_loss           | 2.63e-05  |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.71    |\n",
      "| time/              |          |\n",
      "|    fps             | 667      |\n",
      "|    iterations      | 188      |\n",
      "|    time_elapsed    | 2839     |\n",
      "|    total_timesteps | 1895040  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1915200, episode_reward=-0.13 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.13       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1915200     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011144995 |\n",
      "|    clip_fraction        | 0.145       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -31         |\n",
      "|    explained_variance   | 0.097       |\n",
      "|    learning_rate        | 0.00182     |\n",
      "|    loss                 | -0.00856    |\n",
      "|    n_updates            | 1890        |\n",
      "|    policy_gradient_loss | -0.00616    |\n",
      "|    std                  | 0.569       |\n",
      "|    value_loss           | 0.000234    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.72    |\n",
      "| time/              |          |\n",
      "|    fps             | 667      |\n",
      "|    iterations      | 190      |\n",
      "|    time_elapsed    | 2870     |\n",
      "|    total_timesteps | 1915200  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1935360, episode_reward=-0.15 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.146      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1935360     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010155578 |\n",
      "|    clip_fraction        | 0.134       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -30.9       |\n",
      "|    explained_variance   | 0.0812      |\n",
      "|    learning_rate        | 0.00179     |\n",
      "|    loss                 | -0.00364    |\n",
      "|    n_updates            | 1910        |\n",
      "|    policy_gradient_loss | -0.00575    |\n",
      "|    std                  | 0.569       |\n",
      "|    value_loss           | 0.000253    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 667      |\n",
      "|    iterations      | 192      |\n",
      "|    time_elapsed    | 2901     |\n",
      "|    total_timesteps | 1935360  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1955520, episode_reward=-0.16 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.159      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1955520     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007907418 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -30.5       |\n",
      "|    explained_variance   | 0.363       |\n",
      "|    learning_rate        | 0.00176     |\n",
      "|    loss                 | -0.00955    |\n",
      "|    n_updates            | 1930        |\n",
      "|    policy_gradient_loss | -0.00726    |\n",
      "|    std                  | 0.564       |\n",
      "|    value_loss           | 7.73e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.65    |\n",
      "| time/              |          |\n",
      "|    fps             | 666      |\n",
      "|    iterations      | 194      |\n",
      "|    time_elapsed    | 2932     |\n",
      "|    total_timesteps | 1955520  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1975680, episode_reward=-0.14 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.144      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1975680     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011629695 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -30.2       |\n",
      "|    explained_variance   | 0.0812      |\n",
      "|    learning_rate        | 0.00172     |\n",
      "|    loss                 | -0.00718    |\n",
      "|    n_updates            | 1950        |\n",
      "|    policy_gradient_loss | -0.00556    |\n",
      "|    std                  | 0.561       |\n",
      "|    value_loss           | 0.000246    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.67    |\n",
      "| time/              |          |\n",
      "|    fps             | 666      |\n",
      "|    iterations      | 196      |\n",
      "|    time_elapsed    | 2962     |\n",
      "|    total_timesteps | 1975680  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1995840, episode_reward=-0.18 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.18       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1995840     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010800863 |\n",
      "|    clip_fraction        | 0.11        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -29.9       |\n",
      "|    explained_variance   | 0.318       |\n",
      "|    learning_rate        | 0.00169     |\n",
      "|    loss                 | -0.00806    |\n",
      "|    n_updates            | 1970        |\n",
      "|    policy_gradient_loss | -0.00582    |\n",
      "|    std                  | 0.556       |\n",
      "|    value_loss           | 5.36e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.65    |\n",
      "| time/              |          |\n",
      "|    fps             | 667      |\n",
      "|    iterations      | 198      |\n",
      "|    time_elapsed    | 2992     |\n",
      "|    total_timesteps | 1995840  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2016000, episode_reward=-0.14 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.143      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2016000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009329631 |\n",
      "|    clip_fraction        | 0.101       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -29.5       |\n",
      "|    explained_variance   | 0.388       |\n",
      "|    learning_rate        | 0.00166     |\n",
      "|    loss                 | -0.00515    |\n",
      "|    n_updates            | 1990        |\n",
      "|    policy_gradient_loss | -0.00578    |\n",
      "|    std                  | 0.553       |\n",
      "|    value_loss           | 1.73e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.64    |\n",
      "| time/              |          |\n",
      "|    fps             | 667      |\n",
      "|    iterations      | 200      |\n",
      "|    time_elapsed    | 3021     |\n",
      "|    total_timesteps | 2016000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2036160, episode_reward=-0.11 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.113      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2036160     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010331093 |\n",
      "|    clip_fraction        | 0.11        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -29         |\n",
      "|    explained_variance   | 0.295       |\n",
      "|    learning_rate        | 0.00162     |\n",
      "|    loss                 | -0.0102     |\n",
      "|    n_updates            | 2010        |\n",
      "|    policy_gradient_loss | -0.00709    |\n",
      "|    std                  | 0.546       |\n",
      "|    value_loss           | 1.32e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.61    |\n",
      "| time/              |          |\n",
      "|    fps             | 667      |\n",
      "|    iterations      | 202      |\n",
      "|    time_elapsed    | 3051     |\n",
      "|    total_timesteps | 2036160  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2056320, episode_reward=-0.14 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.14       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2056320     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011794286 |\n",
      "|    clip_fraction        | 0.101       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -28.8       |\n",
      "|    explained_variance   | 0.129       |\n",
      "|    learning_rate        | 0.00159     |\n",
      "|    loss                 | -0.00569    |\n",
      "|    n_updates            | 2030        |\n",
      "|    policy_gradient_loss | -0.00476    |\n",
      "|    std                  | 0.544       |\n",
      "|    value_loss           | 0.000114    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 667      |\n",
      "|    iterations      | 204      |\n",
      "|    time_elapsed    | 3081     |\n",
      "|    total_timesteps | 2056320  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2076480, episode_reward=-0.14 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.139      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2076480     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012288142 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -28.5       |\n",
      "|    explained_variance   | 0.139       |\n",
      "|    learning_rate        | 0.00156     |\n",
      "|    loss                 | -0.00659    |\n",
      "|    n_updates            | 2050        |\n",
      "|    policy_gradient_loss | -0.00629    |\n",
      "|    std                  | 0.541       |\n",
      "|    value_loss           | 9.87e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.63    |\n",
      "| time/              |          |\n",
      "|    fps             | 667      |\n",
      "|    iterations      | 206      |\n",
      "|    time_elapsed    | 3111     |\n",
      "|    total_timesteps | 2076480  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2096640, episode_reward=-0.16 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.155      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2096640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009151709 |\n",
      "|    clip_fraction        | 0.109       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -28.2       |\n",
      "|    explained_variance   | 0.149       |\n",
      "|    learning_rate        | 0.00152     |\n",
      "|    loss                 | -0.0078     |\n",
      "|    n_updates            | 2070        |\n",
      "|    policy_gradient_loss | -0.00542    |\n",
      "|    std                  | 0.536       |\n",
      "|    value_loss           | 5.33e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.57    |\n",
      "| time/              |          |\n",
      "|    fps             | 667      |\n",
      "|    iterations      | 208      |\n",
      "|    time_elapsed    | 3141     |\n",
      "|    total_timesteps | 2096640  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2116800, episode_reward=-0.15 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.149      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2116800     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012348111 |\n",
      "|    clip_fraction        | 0.136       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -28         |\n",
      "|    explained_variance   | 0.224       |\n",
      "|    learning_rate        | 0.00149     |\n",
      "|    loss                 | -0.00894    |\n",
      "|    n_updates            | 2090        |\n",
      "|    policy_gradient_loss | -0.00611    |\n",
      "|    std                  | 0.535       |\n",
      "|    value_loss           | 0.000147    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.55    |\n",
      "| time/              |          |\n",
      "|    fps             | 667      |\n",
      "|    iterations      | 210      |\n",
      "|    time_elapsed    | 3170     |\n",
      "|    total_timesteps | 2116800  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2136960, episode_reward=-0.14 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.14       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2136960     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009637833 |\n",
      "|    clip_fraction        | 0.11        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -27.6       |\n",
      "|    explained_variance   | 0.298       |\n",
      "|    learning_rate        | 0.00146     |\n",
      "|    loss                 | -0.00773    |\n",
      "|    n_updates            | 2110        |\n",
      "|    policy_gradient_loss | -0.00654    |\n",
      "|    std                  | 0.53        |\n",
      "|    value_loss           | 2.29e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.57    |\n",
      "| time/              |          |\n",
      "|    fps             | 667      |\n",
      "|    iterations      | 212      |\n",
      "|    time_elapsed    | 3200     |\n",
      "|    total_timesteps | 2136960  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2157120, episode_reward=-0.15 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.148      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2157120     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008497003 |\n",
      "|    clip_fraction        | 0.0945      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -27.2       |\n",
      "|    explained_variance   | 0.368       |\n",
      "|    learning_rate        | 0.00142     |\n",
      "|    loss                 | -0.00758    |\n",
      "|    n_updates            | 2130        |\n",
      "|    policy_gradient_loss | -0.0064     |\n",
      "|    std                  | 0.524       |\n",
      "|    value_loss           | 8.49e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.55    |\n",
      "| time/              |          |\n",
      "|    fps             | 667      |\n",
      "|    iterations      | 214      |\n",
      "|    time_elapsed    | 3230     |\n",
      "|    total_timesteps | 2157120  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2177280, episode_reward=-0.17 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.169      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2177280     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012689911 |\n",
      "|    clip_fraction        | 0.132       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -26.9       |\n",
      "|    explained_variance   | 0.0382      |\n",
      "|    learning_rate        | 0.00139     |\n",
      "|    loss                 | -0.00835    |\n",
      "|    n_updates            | 2150        |\n",
      "|    policy_gradient_loss | -0.006      |\n",
      "|    std                  | 0.522       |\n",
      "|    value_loss           | 0.000277    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.52    |\n",
      "| time/              |          |\n",
      "|    fps             | 667      |\n",
      "|    iterations      | 216      |\n",
      "|    time_elapsed    | 3260     |\n",
      "|    total_timesteps | 2177280  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2197440, episode_reward=-0.16 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.162      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2197440     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010427952 |\n",
      "|    clip_fraction        | 0.116       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -26.5       |\n",
      "|    explained_variance   | 0.216       |\n",
      "|    learning_rate        | 0.00135     |\n",
      "|    loss                 | -0.0104     |\n",
      "|    n_updates            | 2170        |\n",
      "|    policy_gradient_loss | -0.00668    |\n",
      "|    std                  | 0.517       |\n",
      "|    value_loss           | 3.22e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 667      |\n",
      "|    iterations      | 218      |\n",
      "|    time_elapsed    | 3289     |\n",
      "|    total_timesteps | 2197440  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2217600, episode_reward=-0.18 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.181      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2217600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010368189 |\n",
      "|    clip_fraction        | 0.0904      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -26.1       |\n",
      "|    explained_variance   | 0.193       |\n",
      "|    learning_rate        | 0.00132     |\n",
      "|    loss                 | -0.0105     |\n",
      "|    n_updates            | 2190        |\n",
      "|    policy_gradient_loss | -0.00591    |\n",
      "|    std                  | 0.511       |\n",
      "|    value_loss           | 5.56e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 667      |\n",
      "|    iterations      | 220      |\n",
      "|    time_elapsed    | 3319     |\n",
      "|    total_timesteps | 2217600  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2237760, episode_reward=-0.17 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.167      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2237760     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010727328 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -25.8       |\n",
      "|    explained_variance   | 0.0502      |\n",
      "|    learning_rate        | 0.00129     |\n",
      "|    loss                 | -0.00767    |\n",
      "|    n_updates            | 2210        |\n",
      "|    policy_gradient_loss | -0.00518    |\n",
      "|    std                  | 0.509       |\n",
      "|    value_loss           | 0.000365    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.48    |\n",
      "| time/              |          |\n",
      "|    fps             | 668      |\n",
      "|    iterations      | 222      |\n",
      "|    time_elapsed    | 3349     |\n",
      "|    total_timesteps | 2237760  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2257920, episode_reward=-0.17 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.167     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2257920    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01003799 |\n",
      "|    clip_fraction        | 0.0891     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -25.4      |\n",
      "|    explained_variance   | 0.295      |\n",
      "|    learning_rate        | 0.00125    |\n",
      "|    loss                 | -0.0089    |\n",
      "|    n_updates            | 2230       |\n",
      "|    policy_gradient_loss | -0.00555   |\n",
      "|    std                  | 0.504      |\n",
      "|    value_loss           | 2.31e-05   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.45    |\n",
      "| time/              |          |\n",
      "|    fps             | 668      |\n",
      "|    iterations      | 224      |\n",
      "|    time_elapsed    | 3379     |\n",
      "|    total_timesteps | 2257920  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2278080, episode_reward=-0.15 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.149      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2278080     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011080537 |\n",
      "|    clip_fraction        | 0.116       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -25.1       |\n",
      "|    explained_variance   | 0.16        |\n",
      "|    learning_rate        | 0.00122     |\n",
      "|    loss                 | -0.0114     |\n",
      "|    n_updates            | 2250        |\n",
      "|    policy_gradient_loss | -0.0061     |\n",
      "|    std                  | 0.5         |\n",
      "|    value_loss           | 6.93e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.48    |\n",
      "| time/              |          |\n",
      "|    fps             | 668      |\n",
      "|    iterations      | 226      |\n",
      "|    time_elapsed    | 3409     |\n",
      "|    total_timesteps | 2278080  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2298240, episode_reward=-0.16 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.157     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2298240    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00999123 |\n",
      "|    clip_fraction        | 0.0974     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -24.8      |\n",
      "|    explained_variance   | 0.308      |\n",
      "|    learning_rate        | 0.00119    |\n",
      "|    loss                 | -0.00929   |\n",
      "|    n_updates            | 2270       |\n",
      "|    policy_gradient_loss | -0.00765   |\n",
      "|    std                  | 0.497      |\n",
      "|    value_loss           | 6.38e-06   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.45    |\n",
      "| time/              |          |\n",
      "|    fps             | 668      |\n",
      "|    iterations      | 228      |\n",
      "|    time_elapsed    | 3438     |\n",
      "|    total_timesteps | 2298240  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2318400, episode_reward=-0.14 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.143      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2318400     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007947894 |\n",
      "|    clip_fraction        | 0.0877      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -24.4       |\n",
      "|    explained_variance   | 0.395       |\n",
      "|    learning_rate        | 0.00115     |\n",
      "|    loss                 | -0.0101     |\n",
      "|    n_updates            | 2290        |\n",
      "|    policy_gradient_loss | -0.00627    |\n",
      "|    std                  | 0.494       |\n",
      "|    value_loss           | 1.24e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.47    |\n",
      "| time/              |          |\n",
      "|    fps             | 668      |\n",
      "|    iterations      | 230      |\n",
      "|    time_elapsed    | 3468     |\n",
      "|    total_timesteps | 2318400  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2338560, episode_reward=-0.16 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.155      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2338560     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007086251 |\n",
      "|    clip_fraction        | 0.0662      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -24.2       |\n",
      "|    explained_variance   | 0.38        |\n",
      "|    learning_rate        | 0.00112     |\n",
      "|    loss                 | -0.0026     |\n",
      "|    n_updates            | 2310        |\n",
      "|    policy_gradient_loss | -0.00544    |\n",
      "|    std                  | 0.492       |\n",
      "|    value_loss           | 5e-06       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.47    |\n",
      "| time/              |          |\n",
      "|    fps             | 668      |\n",
      "|    iterations      | 232      |\n",
      "|    time_elapsed    | 3499     |\n",
      "|    total_timesteps | 2338560  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2358720, episode_reward=-0.20 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2358720     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011793509 |\n",
      "|    clip_fraction        | 0.125       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -24         |\n",
      "|    explained_variance   | 0.0151      |\n",
      "|    learning_rate        | 0.00109     |\n",
      "|    loss                 | -0.00783    |\n",
      "|    n_updates            | 2330        |\n",
      "|    policy_gradient_loss | -0.00474    |\n",
      "|    std                  | 0.49        |\n",
      "|    value_loss           | 0.000554    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.45    |\n",
      "| time/              |          |\n",
      "|    fps             | 668      |\n",
      "|    iterations      | 234      |\n",
      "|    time_elapsed    | 3528     |\n",
      "|    total_timesteps | 2358720  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2378880, episode_reward=-0.15 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.146      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2378880     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011585612 |\n",
      "|    clip_fraction        | 0.1         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -23.9       |\n",
      "|    explained_variance   | 0.248       |\n",
      "|    learning_rate        | 0.00105     |\n",
      "|    loss                 | -0.00788    |\n",
      "|    n_updates            | 2350        |\n",
      "|    policy_gradient_loss | -0.00617    |\n",
      "|    std                  | 0.489       |\n",
      "|    value_loss           | 2.73e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.47    |\n",
      "| time/              |          |\n",
      "|    fps             | 668      |\n",
      "|    iterations      | 236      |\n",
      "|    time_elapsed    | 3559     |\n",
      "|    total_timesteps | 2378880  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2399040, episode_reward=-0.12 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.117      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2399040     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008458047 |\n",
      "|    clip_fraction        | 0.0693      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -23.7       |\n",
      "|    explained_variance   | 0.273       |\n",
      "|    learning_rate        | 0.00102     |\n",
      "|    loss                 | -0.00934    |\n",
      "|    n_updates            | 2370        |\n",
      "|    policy_gradient_loss | -0.00493    |\n",
      "|    std                  | 0.486       |\n",
      "|    value_loss           | 3.38e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.45    |\n",
      "| time/              |          |\n",
      "|    fps             | 668      |\n",
      "|    iterations      | 238      |\n",
      "|    time_elapsed    | 3588     |\n",
      "|    total_timesteps | 2399040  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2419200, episode_reward=-0.11 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.111      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2419200     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012161255 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -23.4       |\n",
      "|    explained_variance   | 0.125       |\n",
      "|    learning_rate        | 0.000985    |\n",
      "|    loss                 | -0.012      |\n",
      "|    n_updates            | 2390        |\n",
      "|    policy_gradient_loss | -0.00583    |\n",
      "|    std                  | 0.483       |\n",
      "|    value_loss           | 0.00012     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.46    |\n",
      "| time/              |          |\n",
      "|    fps             | 668      |\n",
      "|    iterations      | 240      |\n",
      "|    time_elapsed    | 3618     |\n",
      "|    total_timesteps | 2419200  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2439360, episode_reward=-0.13 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.135       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2439360      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0097472705 |\n",
      "|    clip_fraction        | 0.0895       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -23.1        |\n",
      "|    explained_variance   | 0.279        |\n",
      "|    learning_rate        | 0.000951     |\n",
      "|    loss                 | -0.0109      |\n",
      "|    n_updates            | 2410         |\n",
      "|    policy_gradient_loss | -0.00703     |\n",
      "|    std                  | 0.479        |\n",
      "|    value_loss           | 9.39e-06     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.46    |\n",
      "| time/              |          |\n",
      "|    fps             | 668      |\n",
      "|    iterations      | 242      |\n",
      "|    time_elapsed    | 3648     |\n",
      "|    total_timesteps | 2439360  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2459520, episode_reward=-0.13 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.134       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2459520      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0081240935 |\n",
      "|    clip_fraction        | 0.0694       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -22.8        |\n",
      "|    explained_variance   | 0.311        |\n",
      "|    learning_rate        | 0.000918     |\n",
      "|    loss                 | -0.00551     |\n",
      "|    n_updates            | 2430         |\n",
      "|    policy_gradient_loss | -0.00485     |\n",
      "|    std                  | 0.476        |\n",
      "|    value_loss           | 1.33e-05     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.43    |\n",
      "| time/              |          |\n",
      "|    fps             | 668      |\n",
      "|    iterations      | 244      |\n",
      "|    time_elapsed    | 3678     |\n",
      "|    total_timesteps | 2459520  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2479680, episode_reward=-0.14 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.141      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2479680     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009092606 |\n",
      "|    clip_fraction        | 0.0786      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -22.4       |\n",
      "|    explained_variance   | 0.414       |\n",
      "|    learning_rate        | 0.000884    |\n",
      "|    loss                 | -0.0129     |\n",
      "|    n_updates            | 2450        |\n",
      "|    policy_gradient_loss | -0.00662    |\n",
      "|    std                  | 0.471       |\n",
      "|    value_loss           | 5.21e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.39    |\n",
      "| time/              |          |\n",
      "|    fps             | 668      |\n",
      "|    iterations      | 246      |\n",
      "|    time_elapsed    | 3708     |\n",
      "|    total_timesteps | 2479680  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2499840, episode_reward=-0.13 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.135      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2499840     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007398369 |\n",
      "|    clip_fraction        | 0.065       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -22         |\n",
      "|    explained_variance   | 0.216       |\n",
      "|    learning_rate        | 0.00085     |\n",
      "|    loss                 | -0.00634    |\n",
      "|    n_updates            | 2470        |\n",
      "|    policy_gradient_loss | -0.00488    |\n",
      "|    std                  | 0.468       |\n",
      "|    value_loss           | 2.26e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.34    |\n",
      "| time/              |          |\n",
      "|    fps             | 668      |\n",
      "|    iterations      | 248      |\n",
      "|    time_elapsed    | 3738     |\n",
      "|    total_timesteps | 2499840  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2520000, episode_reward=-0.15 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.147      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2520000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008932564 |\n",
      "|    clip_fraction        | 0.0714      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -21.6       |\n",
      "|    explained_variance   | 0.319       |\n",
      "|    learning_rate        | 0.000817    |\n",
      "|    loss                 | -0.00925    |\n",
      "|    n_updates            | 2490        |\n",
      "|    policy_gradient_loss | -0.0057     |\n",
      "|    std                  | 0.463       |\n",
      "|    value_loss           | 1.52e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.33    |\n",
      "| time/              |          |\n",
      "|    fps             | 668      |\n",
      "|    iterations      | 250      |\n",
      "|    time_elapsed    | 3767     |\n",
      "|    total_timesteps | 2520000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2540160, episode_reward=-0.13 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.133      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2540160     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008997239 |\n",
      "|    clip_fraction        | 0.0601      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -21.3       |\n",
      "|    explained_variance   | 0.0396      |\n",
      "|    learning_rate        | 0.000783    |\n",
      "|    loss                 | -0.00451    |\n",
      "|    n_updates            | 2510        |\n",
      "|    policy_gradient_loss | -0.00424    |\n",
      "|    std                  | 0.46        |\n",
      "|    value_loss           | 0.000129    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 668      |\n",
      "|    iterations      | 252      |\n",
      "|    time_elapsed    | 3797     |\n",
      "|    total_timesteps | 2540160  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2560320, episode_reward=-0.12 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.116       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2560320      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073397635 |\n",
      "|    clip_fraction        | 0.04         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -21.1        |\n",
      "|    explained_variance   | 0.102        |\n",
      "|    learning_rate        | 0.00075      |\n",
      "|    loss                 | -0.00661     |\n",
      "|    n_updates            | 2530         |\n",
      "|    policy_gradient_loss | -0.00374     |\n",
      "|    std                  | 0.459        |\n",
      "|    value_loss           | 8.53e-05     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 668      |\n",
      "|    iterations      | 254      |\n",
      "|    time_elapsed    | 3827     |\n",
      "|    total_timesteps | 2560320  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2580480, episode_reward=-0.11 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.114      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2580480     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008072509 |\n",
      "|    clip_fraction        | 0.0763      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -20.9       |\n",
      "|    explained_variance   | 0.311       |\n",
      "|    learning_rate        | 0.000716    |\n",
      "|    loss                 | -0.00976    |\n",
      "|    n_updates            | 2550        |\n",
      "|    policy_gradient_loss | -0.00571    |\n",
      "|    std                  | 0.456       |\n",
      "|    value_loss           | 1.66e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.31    |\n",
      "| time/              |          |\n",
      "|    fps             | 668      |\n",
      "|    iterations      | 256      |\n",
      "|    time_elapsed    | 3857     |\n",
      "|    total_timesteps | 2580480  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2600640, episode_reward=-0.12 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.117      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2600640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007773395 |\n",
      "|    clip_fraction        | 0.062       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -20.6       |\n",
      "|    explained_variance   | 0.26        |\n",
      "|    learning_rate        | 0.000682    |\n",
      "|    loss                 | -0.00795    |\n",
      "|    n_updates            | 2570        |\n",
      "|    policy_gradient_loss | -0.00598    |\n",
      "|    std                  | 0.453       |\n",
      "|    value_loss           | 4.83e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.29    |\n",
      "| time/              |          |\n",
      "|    fps             | 669      |\n",
      "|    iterations      | 258      |\n",
      "|    time_elapsed    | 3887     |\n",
      "|    total_timesteps | 2600640  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2620800, episode_reward=-0.12 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.123      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2620800     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008859193 |\n",
      "|    clip_fraction        | 0.0661      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -20.1       |\n",
      "|    explained_variance   | 0.344       |\n",
      "|    learning_rate        | 0.000649    |\n",
      "|    loss                 | -0.00759    |\n",
      "|    n_updates            | 2590        |\n",
      "|    policy_gradient_loss | -0.00591    |\n",
      "|    std                  | 0.448       |\n",
      "|    value_loss           | 6.85e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 669      |\n",
      "|    iterations      | 260      |\n",
      "|    time_elapsed    | 3917     |\n",
      "|    total_timesteps | 2620800  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2640960, episode_reward=-0.13 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.131      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2640960     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007246514 |\n",
      "|    clip_fraction        | 0.0538      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -19.8       |\n",
      "|    explained_variance   | 0.387       |\n",
      "|    learning_rate        | 0.000615    |\n",
      "|    loss                 | -0.0123     |\n",
      "|    n_updates            | 2610        |\n",
      "|    policy_gradient_loss | -0.00625    |\n",
      "|    std                  | 0.445       |\n",
      "|    value_loss           | 4.44e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.29    |\n",
      "| time/              |          |\n",
      "|    fps             | 669      |\n",
      "|    iterations      | 262      |\n",
      "|    time_elapsed    | 3946     |\n",
      "|    total_timesteps | 2640960  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2661120, episode_reward=-0.14 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.136       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2661120      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0079612285 |\n",
      "|    clip_fraction        | 0.0444       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -19.5        |\n",
      "|    explained_variance   | 0.209        |\n",
      "|    learning_rate        | 0.000582     |\n",
      "|    loss                 | -0.00837     |\n",
      "|    n_updates            | 2630         |\n",
      "|    policy_gradient_loss | -0.00505     |\n",
      "|    std                  | 0.442        |\n",
      "|    value_loss           | 2.31e-05     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.28    |\n",
      "| time/              |          |\n",
      "|    fps             | 669      |\n",
      "|    iterations      | 264      |\n",
      "|    time_elapsed    | 3976     |\n",
      "|    total_timesteps | 2661120  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2681280, episode_reward=-0.14 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.138       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2681280      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0083606765 |\n",
      "|    clip_fraction        | 0.0382       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -19.3        |\n",
      "|    explained_variance   | 0.267        |\n",
      "|    learning_rate        | 0.000548     |\n",
      "|    loss                 | -0.00923     |\n",
      "|    n_updates            | 2650         |\n",
      "|    policy_gradient_loss | -0.00404     |\n",
      "|    std                  | 0.44         |\n",
      "|    value_loss           | 2.18e-05     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.25    |\n",
      "| time/              |          |\n",
      "|    fps             | 669      |\n",
      "|    iterations      | 266      |\n",
      "|    time_elapsed    | 4006     |\n",
      "|    total_timesteps | 2681280  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2701440, episode_reward=-0.13 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.133     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2701440    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00811639 |\n",
      "|    clip_fraction        | 0.0523     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -19.1      |\n",
      "|    explained_variance   | 0.139      |\n",
      "|    learning_rate        | 0.000514   |\n",
      "|    loss                 | -0.00762   |\n",
      "|    n_updates            | 2670       |\n",
      "|    policy_gradient_loss | -0.00423   |\n",
      "|    std                  | 0.438      |\n",
      "|    value_loss           | 6.1e-05    |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.26    |\n",
      "| time/              |          |\n",
      "|    fps             | 669      |\n",
      "|    iterations      | 268      |\n",
      "|    time_elapsed    | 4036     |\n",
      "|    total_timesteps | 2701440  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2721600, episode_reward=-0.12 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.124      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2721600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008689862 |\n",
      "|    clip_fraction        | 0.0425      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -19         |\n",
      "|    explained_variance   | 0.0725      |\n",
      "|    learning_rate        | 0.000481    |\n",
      "|    loss                 | -0.00808    |\n",
      "|    n_updates            | 2690        |\n",
      "|    policy_gradient_loss | -0.0041     |\n",
      "|    std                  | 0.438       |\n",
      "|    value_loss           | 9.42e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.25    |\n",
      "| time/              |          |\n",
      "|    fps             | 669      |\n",
      "|    iterations      | 270      |\n",
      "|    time_elapsed    | 4066     |\n",
      "|    total_timesteps | 2721600  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2741760, episode_reward=-0.12 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.121       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2741760      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077452445 |\n",
      "|    clip_fraction        | 0.0317       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -18.8        |\n",
      "|    explained_variance   | 0.0281       |\n",
      "|    learning_rate        | 0.000447     |\n",
      "|    loss                 | -0.00809     |\n",
      "|    n_updates            | 2710         |\n",
      "|    policy_gradient_loss | -0.00441     |\n",
      "|    std                  | 0.435        |\n",
      "|    value_loss           | 8.23e-05     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.26    |\n",
      "| time/              |          |\n",
      "|    fps             | 669      |\n",
      "|    iterations      | 272      |\n",
      "|    time_elapsed    | 4096     |\n",
      "|    total_timesteps | 2741760  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2761920, episode_reward=-0.13 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.129       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2761920      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069310395 |\n",
      "|    clip_fraction        | 0.0238       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -18.4        |\n",
      "|    explained_variance   | 0.285        |\n",
      "|    learning_rate        | 0.000414     |\n",
      "|    loss                 | -0.0083      |\n",
      "|    n_updates            | 2730         |\n",
      "|    policy_gradient_loss | -0.00424     |\n",
      "|    std                  | 0.432        |\n",
      "|    value_loss           | 6.35e-06     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.23    |\n",
      "| time/              |          |\n",
      "|    fps             | 669      |\n",
      "|    iterations      | 274      |\n",
      "|    time_elapsed    | 4125     |\n",
      "|    total_timesteps | 2761920  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2782080, episode_reward=-0.12 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.124      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2782080     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008042854 |\n",
      "|    clip_fraction        | 0.0396      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -18.2       |\n",
      "|    explained_variance   | 0.303       |\n",
      "|    learning_rate        | 0.00038     |\n",
      "|    loss                 | -0.0109     |\n",
      "|    n_updates            | 2750        |\n",
      "|    policy_gradient_loss | -0.00468    |\n",
      "|    std                  | 0.429       |\n",
      "|    value_loss           | 1.78e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.23    |\n",
      "| time/              |          |\n",
      "|    fps             | 669      |\n",
      "|    iterations      | 276      |\n",
      "|    time_elapsed    | 4155     |\n",
      "|    total_timesteps | 2782080  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2802240, episode_reward=-0.12 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.123       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2802240      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069187423 |\n",
      "|    clip_fraction        | 0.0204       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17.9        |\n",
      "|    explained_variance   | 0.0738       |\n",
      "|    learning_rate        | 0.000346     |\n",
      "|    loss                 | -0.00722     |\n",
      "|    n_updates            | 2770         |\n",
      "|    policy_gradient_loss | -0.0039      |\n",
      "|    std                  | 0.427        |\n",
      "|    value_loss           | 8.12e-05     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.24    |\n",
      "| time/              |          |\n",
      "|    fps             | 669      |\n",
      "|    iterations      | 278      |\n",
      "|    time_elapsed    | 4185     |\n",
      "|    total_timesteps | 2802240  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2822400, episode_reward=-0.12 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.117       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2822400      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038933754 |\n",
      "|    clip_fraction        | 0.00435      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17.8        |\n",
      "|    explained_variance   | 0.0587       |\n",
      "|    learning_rate        | 0.000313     |\n",
      "|    loss                 | -0.00544     |\n",
      "|    n_updates            | 2790         |\n",
      "|    policy_gradient_loss | -0.0027      |\n",
      "|    std                  | 0.426        |\n",
      "|    value_loss           | 8.65e-05     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.22    |\n",
      "| time/              |          |\n",
      "|    fps             | 669      |\n",
      "|    iterations      | 280      |\n",
      "|    time_elapsed    | 4215     |\n",
      "|    total_timesteps | 2822400  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2842560, episode_reward=-0.12 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.123      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2842560     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004318181 |\n",
      "|    clip_fraction        | 0.00546     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -17.6       |\n",
      "|    explained_variance   | 0.139       |\n",
      "|    learning_rate        | 0.000279    |\n",
      "|    loss                 | -0.00499    |\n",
      "|    n_updates            | 2810        |\n",
      "|    policy_gradient_loss | -0.00256    |\n",
      "|    std                  | 0.425       |\n",
      "|    value_loss           | 2.3e-05     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.23    |\n",
      "| time/              |          |\n",
      "|    fps             | 669      |\n",
      "|    iterations      | 282      |\n",
      "|    time_elapsed    | 4245     |\n",
      "|    total_timesteps | 2842560  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2862720, episode_reward=-0.12 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.124      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2862720     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004733202 |\n",
      "|    clip_fraction        | 0.00725     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -17.3       |\n",
      "|    explained_variance   | 0.469       |\n",
      "|    learning_rate        | 0.000246    |\n",
      "|    loss                 | -0.00788    |\n",
      "|    n_updates            | 2830        |\n",
      "|    policy_gradient_loss | -0.00316    |\n",
      "|    std                  | 0.422       |\n",
      "|    value_loss           | 6.31e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.21    |\n",
      "| time/              |          |\n",
      "|    fps             | 669      |\n",
      "|    iterations      | 284      |\n",
      "|    time_elapsed    | 4275     |\n",
      "|    total_timesteps | 2862720  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2882880, episode_reward=-0.12 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.119       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2882880      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037142257 |\n",
      "|    clip_fraction        | 0.00336      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17.2        |\n",
      "|    explained_variance   | 0.169        |\n",
      "|    learning_rate        | 0.000212     |\n",
      "|    loss                 | -0.00724     |\n",
      "|    n_updates            | 2850         |\n",
      "|    policy_gradient_loss | -0.00289     |\n",
      "|    std                  | 0.421        |\n",
      "|    value_loss           | 2.43e-05     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.22    |\n",
      "| time/              |          |\n",
      "|    fps             | 669      |\n",
      "|    iterations      | 286      |\n",
      "|    time_elapsed    | 4304     |\n",
      "|    total_timesteps | 2882880  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2903040, episode_reward=-0.11 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.109       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2903040      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034703882 |\n",
      "|    clip_fraction        | 0.00239      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.313        |\n",
      "|    learning_rate        | 0.000178     |\n",
      "|    loss                 | -0.00199     |\n",
      "|    n_updates            | 2870         |\n",
      "|    policy_gradient_loss | -0.00308     |\n",
      "|    std                  | 0.42         |\n",
      "|    value_loss           | 5.38e-06     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.18    |\n",
      "| time/              |          |\n",
      "|    fps             | 669      |\n",
      "|    iterations      | 288      |\n",
      "|    time_elapsed    | 4334     |\n",
      "|    total_timesteps | 2903040  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2923200, episode_reward=-0.13 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.126      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2923200     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002053076 |\n",
      "|    clip_fraction        | 0.000149    |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -16.9       |\n",
      "|    explained_variance   | 0.519       |\n",
      "|    learning_rate        | 0.000145    |\n",
      "|    loss                 | -0.0056     |\n",
      "|    n_updates            | 2890        |\n",
      "|    policy_gradient_loss | -0.00237    |\n",
      "|    std                  | 0.419       |\n",
      "|    value_loss           | 6.25e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.18    |\n",
      "| time/              |          |\n",
      "|    fps             | 669      |\n",
      "|    iterations      | 290      |\n",
      "|    time_elapsed    | 4365     |\n",
      "|    total_timesteps | 2923200  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2943360, episode_reward=-0.12 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.12        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2943360      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007027752 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -16.7        |\n",
      "|    explained_variance   | 0.466        |\n",
      "|    learning_rate        | 0.000111     |\n",
      "|    loss                 | -0.0022      |\n",
      "|    n_updates            | 2910         |\n",
      "|    policy_gradient_loss | -0.00121     |\n",
      "|    std                  | 0.418        |\n",
      "|    value_loss           | 4.7e-06      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.15    |\n",
      "| time/              |          |\n",
      "|    fps             | 669      |\n",
      "|    iterations      | 292      |\n",
      "|    time_elapsed    | 4395     |\n",
      "|    total_timesteps | 2943360  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2963520, episode_reward=-0.15 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1.26e+03      |\n",
      "|    mean_reward          | -0.153        |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 2963520       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00036932674 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -16.7         |\n",
      "|    explained_variance   | 0.535         |\n",
      "|    learning_rate        | 7.76e-05      |\n",
      "|    loss                 | -0.00239      |\n",
      "|    n_updates            | 2930          |\n",
      "|    policy_gradient_loss | -0.00095      |\n",
      "|    std                  | 0.417         |\n",
      "|    value_loss           | 5.58e-06      |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.15    |\n",
      "| time/              |          |\n",
      "|    fps             | 669      |\n",
      "|    iterations      | 294      |\n",
      "|    time_elapsed    | 4425     |\n",
      "|    total_timesteps | 2963520  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2983680, episode_reward=-0.12 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1.26e+03      |\n",
      "|    mean_reward          | -0.118        |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 2983680       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00011891708 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -16.6         |\n",
      "|    explained_variance   | 0.0549        |\n",
      "|    learning_rate        | 4.4e-05       |\n",
      "|    loss                 | -0.000533     |\n",
      "|    n_updates            | 2950          |\n",
      "|    policy_gradient_loss | -0.000487     |\n",
      "|    std                  | 0.417         |\n",
      "|    value_loss           | 0.000118      |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.16    |\n",
      "| time/              |          |\n",
      "|    fps             | 669      |\n",
      "|    iterations      | 296      |\n",
      "|    time_elapsed    | 4455     |\n",
      "|    total_timesteps | 2983680  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3003840, episode_reward=-0.12 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.117       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3003840      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.279798e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -16.6        |\n",
      "|    explained_variance   | 0.277        |\n",
      "|    learning_rate        | 1.04e-05     |\n",
      "|    loss                 | -0.000241    |\n",
      "|    n_updates            | 2970         |\n",
      "|    policy_gradient_loss | -0.000147    |\n",
      "|    std                  | 0.417        |\n",
      "|    value_loss           | 1.12e-05     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.17    |\n",
      "| time/              |          |\n",
      "|    fps             | 669      |\n",
      "|    iterations      | 298      |\n",
      "|    time_elapsed    | 4485     |\n",
      "|    total_timesteps | 3003840  |\n",
      "---------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x16e78529150>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Big Action\n",
    "envs = VecMonitor(DummyVecEnv([\n",
    "    lambda: tradingEng(paths1,action = 'big', obs = 'xs'),\n",
    "    lambda: tradingEng(paths2,action = 'big', obs = 'xs')\n",
    "]),filename='logsBigA-Train')\n",
    "ev_env = VecMonitor(DummyVecEnv([\n",
    "    lambda: tradingEng(paths_ev,action = 'big', obs = 'xs'),\n",
    "]))\n",
    "\n",
    "eval_callback = EvalCallback(\n",
    "    ev_env,\n",
    "    best_model_save_path='./logs/best_modelBigAct',\n",
    "    log_path='./eval_logsBigA/ev',\n",
    "    eval_freq=252*8*5,\n",
    "    deterministic=True,\n",
    "    render=False,\n",
    "    verbose = True,\n",
    "    n_eval_episodes = 8\n",
    ")\n",
    "model = PPO(\"MlpPolicy\", envs, batch_size = 252*2*5, learning_rate=linear_schedule(0.005), policy_kwargs=policy_kwargs, n_steps=252*4*5, normalize_advantage=True, gamma = 0.9, verbose = 1) \n",
    "\n",
    "model.learn(total_timesteps=3e6, log_interval=2, callback=eval_callback) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Eval num_timesteps=20160, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0012      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 20160        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032088247 |\n",
      "|    clip_fraction        | 0.0117       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.83        |\n",
      "|    explained_variance   | -3.75        |\n",
      "|    learning_rate        | 0.00498      |\n",
      "|    loss                 | -0.0014      |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00082     |\n",
      "|    std                  | 0.994        |\n",
      "|    value_loss           | 0.00263      |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.000844 |\n",
      "| time/              |           |\n",
      "|    fps             | 742       |\n",
      "|    iterations      | 2         |\n",
      "|    time_elapsed    | 27        |\n",
      "|    total_timesteps | 20160     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=40320, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.000318   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 40320       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001443869 |\n",
      "|    clip_fraction        | 0.00538     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.81       |\n",
      "|    explained_variance   | -0.209      |\n",
      "|    learning_rate        | 0.00495     |\n",
      "|    loss                 | 0.000203    |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00023    |\n",
      "|    std                  | 0.98        |\n",
      "|    value_loss           | 6.1e-07     |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.00138 |\n",
      "| time/              |          |\n",
      "|    fps             | 701      |\n",
      "|    iterations      | 4        |\n",
      "|    time_elapsed    | 57       |\n",
      "|    total_timesteps | 40320    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=60480, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0017      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 60480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023003905 |\n",
      "|    clip_fraction        | 0.0153       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.84        |\n",
      "|    explained_variance   | 0.00103      |\n",
      "|    learning_rate        | 0.00492      |\n",
      "|    loss                 | -0.00104     |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.000335    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 5.89e-06     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.00457 |\n",
      "| time/              |          |\n",
      "|    fps             | 688      |\n",
      "|    iterations      | 6        |\n",
      "|    time_elapsed    | 87       |\n",
      "|    total_timesteps | 60480    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=80640, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00168     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 80640        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009956709 |\n",
      "|    clip_fraction        | 0.00181      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.85        |\n",
      "|    explained_variance   | -2.51        |\n",
      "|    learning_rate        | 0.00488      |\n",
      "|    loss                 | 0.000177     |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.000103    |\n",
      "|    std                  | 0.999        |\n",
      "|    value_loss           | 3.67e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.00509 |\n",
      "| time/              |          |\n",
      "|    fps             | 683      |\n",
      "|    iterations      | 8        |\n",
      "|    time_elapsed    | 118      |\n",
      "|    total_timesteps | 80640    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=100800, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0163      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 100800       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020246096 |\n",
      "|    clip_fraction        | 0.00559      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.72        |\n",
      "|    explained_variance   | -1.27        |\n",
      "|    learning_rate        | 0.00485      |\n",
      "|    loss                 | -0.000622    |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.000229    |\n",
      "|    std                  | 0.945        |\n",
      "|    value_loss           | 5.79e-09     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.00416 |\n",
      "| time/              |          |\n",
      "|    fps             | 679      |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 148      |\n",
      "|    total_timesteps | 100800   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=120960, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.000122   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 120960      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004164638 |\n",
      "|    clip_fraction        | 0.0191      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.71       |\n",
      "|    explained_variance   | -0.00497    |\n",
      "|    learning_rate        | 0.00482     |\n",
      "|    loss                 | 0.00352     |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.000393   |\n",
      "|    std                  | 0.95        |\n",
      "|    value_loss           | 4.19e-07    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.00397 |\n",
      "| time/              |          |\n",
      "|    fps             | 679      |\n",
      "|    iterations      | 12       |\n",
      "|    time_elapsed    | 178      |\n",
      "|    total_timesteps | 120960   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=141120, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.00263    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 141120      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014364356 |\n",
      "|    clip_fraction        | 0.0652      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.58       |\n",
      "|    explained_variance   | -0.00679    |\n",
      "|    learning_rate        | 0.00478     |\n",
      "|    loss                 | -0.00131    |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0023     |\n",
      "|    std                  | 0.872       |\n",
      "|    value_loss           | 4.91e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.00439 |\n",
      "| time/              |          |\n",
      "|    fps             | 678      |\n",
      "|    iterations      | 14       |\n",
      "|    time_elapsed    | 208      |\n",
      "|    total_timesteps | 141120   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=161280, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.000451   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 161280      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007110243 |\n",
      "|    clip_fraction        | 0.05        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.5        |\n",
      "|    explained_variance   | -0.0362     |\n",
      "|    learning_rate        | 0.00475     |\n",
      "|    loss                 | -0.000127   |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.00118    |\n",
      "|    std                  | 0.85        |\n",
      "|    value_loss           | 2.31e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.00431 |\n",
      "| time/              |          |\n",
      "|    fps             | 677      |\n",
      "|    iterations      | 16       |\n",
      "|    time_elapsed    | 237      |\n",
      "|    total_timesteps | 161280   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=181440, episode_reward=-0.00 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.003       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 181440       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071589765 |\n",
      "|    clip_fraction        | 0.0371       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.4         |\n",
      "|    explained_variance   | -0.292       |\n",
      "|    learning_rate        | 0.00471      |\n",
      "|    loss                 | -0.001       |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.0014      |\n",
      "|    std                  | 0.819        |\n",
      "|    value_loss           | 1.68e-09     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.00262 |\n",
      "| time/              |          |\n",
      "|    fps             | 676      |\n",
      "|    iterations      | 18       |\n",
      "|    time_elapsed    | 268      |\n",
      "|    total_timesteps | 181440   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=201600, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0072      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 201600       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027154167 |\n",
      "|    clip_fraction        | 0.0105       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.33        |\n",
      "|    explained_variance   | -0.204       |\n",
      "|    learning_rate        | 0.00468      |\n",
      "|    loss                 | -0.00443     |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.000695    |\n",
      "|    std                  | 0.786        |\n",
      "|    value_loss           | 1.15e-09     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.00266 |\n",
      "| time/              |          |\n",
      "|    fps             | 676      |\n",
      "|    iterations      | 20       |\n",
      "|    time_elapsed    | 298      |\n",
      "|    total_timesteps | 201600   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=221760, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.00017    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 221760      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006195449 |\n",
      "|    clip_fraction        | 0.0572      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.32       |\n",
      "|    explained_variance   | -0.0035     |\n",
      "|    learning_rate        | 0.00465     |\n",
      "|    loss                 | -0.00137    |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.000858   |\n",
      "|    std                  | 0.799       |\n",
      "|    value_loss           | 2.69e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.00185 |\n",
      "| time/              |          |\n",
      "|    fps             | 676      |\n",
      "|    iterations      | 22       |\n",
      "|    time_elapsed    | 327      |\n",
      "|    total_timesteps | 221760   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=241920, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.000357    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 241920       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051308796 |\n",
      "|    clip_fraction        | 0.0343       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.32        |\n",
      "|    explained_variance   | -0.142       |\n",
      "|    learning_rate        | 0.00461      |\n",
      "|    loss                 | -0.00247     |\n",
      "|    n_updates            | 230          |\n",
      "|    policy_gradient_loss | -0.00156     |\n",
      "|    std                  | 0.794        |\n",
      "|    value_loss           | 5.08e-09     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.00148 |\n",
      "| time/              |          |\n",
      "|    fps             | 676      |\n",
      "|    iterations      | 24       |\n",
      "|    time_elapsed    | 357      |\n",
      "|    total_timesteps | 241920   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=262080, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.00161    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 262080      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005897301 |\n",
      "|    clip_fraction        | 0.0402      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.28       |\n",
      "|    explained_variance   | -0.000801   |\n",
      "|    learning_rate        | 0.00458     |\n",
      "|    loss                 | -0.00185    |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.000651   |\n",
      "|    std                  | 0.774       |\n",
      "|    value_loss           | 7.35e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0015  |\n",
      "| time/              |          |\n",
      "|    fps             | 676      |\n",
      "|    iterations      | 26       |\n",
      "|    time_elapsed    | 387      |\n",
      "|    total_timesteps | 262080   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=282240, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.00103    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 282240      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001502666 |\n",
      "|    clip_fraction        | 0.00485     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.19       |\n",
      "|    explained_variance   | -3.54       |\n",
      "|    learning_rate        | 0.00455     |\n",
      "|    loss                 | 0.000404    |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.000306   |\n",
      "|    std                  | 0.748       |\n",
      "|    value_loss           | 8.98e-10    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.00109 |\n",
      "| time/              |          |\n",
      "|    fps             | 676      |\n",
      "|    iterations      | 28       |\n",
      "|    time_elapsed    | 417      |\n",
      "|    total_timesteps | 282240   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=302400, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.003       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 302400       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067315046 |\n",
      "|    clip_fraction        | 0.0886       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.14        |\n",
      "|    explained_variance   | -0.0117      |\n",
      "|    learning_rate        | 0.00451      |\n",
      "|    loss                 | -0.00115     |\n",
      "|    n_updates            | 290          |\n",
      "|    policy_gradient_loss | -0.0016      |\n",
      "|    std                  | 0.721        |\n",
      "|    value_loss           | 3.18e-09     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.00106 |\n",
      "| time/              |          |\n",
      "|    fps             | 675      |\n",
      "|    iterations      | 30       |\n",
      "|    time_elapsed    | 447      |\n",
      "|    total_timesteps | 302400   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=322560, episode_reward=-0.01 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0127      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 322560       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071782283 |\n",
      "|    clip_fraction        | 0.0344       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.13        |\n",
      "|    explained_variance   | -0.0301      |\n",
      "|    learning_rate        | 0.00448      |\n",
      "|    loss                 | -0.000173    |\n",
      "|    n_updates            | 310          |\n",
      "|    policy_gradient_loss | -0.000815    |\n",
      "|    std                  | 0.731        |\n",
      "|    value_loss           | 1.25e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.00117 |\n",
      "| time/              |          |\n",
      "|    fps             | 675      |\n",
      "|    iterations      | 32       |\n",
      "|    time_elapsed    | 477      |\n",
      "|    total_timesteps | 322560   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=342720, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.00304    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 342720      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004524463 |\n",
      "|    clip_fraction        | 0.0254      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.03       |\n",
      "|    explained_variance   | -1.06       |\n",
      "|    learning_rate        | 0.00445     |\n",
      "|    loss                 | -0.00398    |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.00216    |\n",
      "|    std                  | 0.688       |\n",
      "|    value_loss           | 7.27e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.00115 |\n",
      "| time/              |          |\n",
      "|    fps             | 675      |\n",
      "|    iterations      | 34       |\n",
      "|    time_elapsed    | 507      |\n",
      "|    total_timesteps | 342720   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=362880, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00108     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 362880       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042308196 |\n",
      "|    clip_fraction        | 0.0101       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.05        |\n",
      "|    explained_variance   | -0.0961      |\n",
      "|    learning_rate        | 0.00441      |\n",
      "|    loss                 | -0.000119    |\n",
      "|    n_updates            | 350          |\n",
      "|    policy_gradient_loss | -0.000321    |\n",
      "|    std                  | 0.691        |\n",
      "|    value_loss           | 1.9e-08      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.00121 |\n",
      "| time/              |          |\n",
      "|    fps             | 675      |\n",
      "|    iterations      | 36       |\n",
      "|    time_elapsed    | 537      |\n",
      "|    total_timesteps | 362880   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=383040, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0177      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 383040       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037374601 |\n",
      "|    clip_fraction        | 0.0199       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.92        |\n",
      "|    explained_variance   | -0.0797      |\n",
      "|    learning_rate        | 0.00438      |\n",
      "|    loss                 | -0.00512     |\n",
      "|    n_updates            | 370          |\n",
      "|    policy_gradient_loss | -0.00134     |\n",
      "|    std                  | 0.645        |\n",
      "|    value_loss           | 2.5e-07      |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.000827 |\n",
      "| time/              |           |\n",
      "|    fps             | 675       |\n",
      "|    iterations      | 38        |\n",
      "|    time_elapsed    | 567       |\n",
      "|    total_timesteps | 383040    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=403200, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.00272    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 403200      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010920319 |\n",
      "|    clip_fraction        | 0.0815      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.94       |\n",
      "|    explained_variance   | -0.00586    |\n",
      "|    learning_rate        | 0.00434     |\n",
      "|    loss                 | -0.00241    |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.00117    |\n",
      "|    std                  | 0.66        |\n",
      "|    value_loss           | 3.58e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.00114 |\n",
      "| time/              |          |\n",
      "|    fps             | 675      |\n",
      "|    iterations      | 40       |\n",
      "|    time_elapsed    | 596      |\n",
      "|    total_timesteps | 403200   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=423360, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00219     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 423360       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027597696 |\n",
      "|    clip_fraction        | 0.00544      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.9         |\n",
      "|    explained_variance   | -0.496       |\n",
      "|    learning_rate        | 0.00431      |\n",
      "|    loss                 | 0.000585     |\n",
      "|    n_updates            | 410          |\n",
      "|    policy_gradient_loss | -0.000313    |\n",
      "|    std                  | 0.642        |\n",
      "|    value_loss           | 1.51e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.00119 |\n",
      "| time/              |          |\n",
      "|    fps             | 675      |\n",
      "|    iterations      | 42       |\n",
      "|    time_elapsed    | 627      |\n",
      "|    total_timesteps | 423360   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=443520, episode_reward=-0.00 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.00384   |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 443520     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00912242 |\n",
      "|    clip_fraction        | 0.0887     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.83      |\n",
      "|    explained_variance   | -0.0204    |\n",
      "|    learning_rate        | 0.00428    |\n",
      "|    loss                 | -0.00403   |\n",
      "|    n_updates            | 430        |\n",
      "|    policy_gradient_loss | -0.00217   |\n",
      "|    std                  | 0.619      |\n",
      "|    value_loss           | 4.3e-08    |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.00123 |\n",
      "| time/              |          |\n",
      "|    fps             | 674      |\n",
      "|    iterations      | 44       |\n",
      "|    time_elapsed    | 657      |\n",
      "|    total_timesteps | 443520   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=463680, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00031     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 463680       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049607726 |\n",
      "|    clip_fraction        | 0.0277       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.8         |\n",
      "|    explained_variance   | -2.03        |\n",
      "|    learning_rate        | 0.00424      |\n",
      "|    loss                 | 0.000825     |\n",
      "|    n_updates            | 450          |\n",
      "|    policy_gradient_loss | -0.00159     |\n",
      "|    std                  | 0.612        |\n",
      "|    value_loss           | 1.05e-09     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.00115 |\n",
      "| time/              |          |\n",
      "|    fps             | 674      |\n",
      "|    iterations      | 46       |\n",
      "|    time_elapsed    | 686      |\n",
      "|    total_timesteps | 463680   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=483840, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -2.84e-05   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 483840      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008364516 |\n",
      "|    clip_fraction        | 0.0667      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.73       |\n",
      "|    explained_variance   | -0.0241     |\n",
      "|    learning_rate        | 0.00421     |\n",
      "|    loss                 | -0.0014     |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.00194    |\n",
      "|    std                  | 0.596       |\n",
      "|    value_loss           | 5.68e-07    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.00118 |\n",
      "| time/              |          |\n",
      "|    fps             | 674      |\n",
      "|    iterations      | 48       |\n",
      "|    time_elapsed    | 716      |\n",
      "|    total_timesteps | 483840   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=504000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -4.87e-05    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 504000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025612386 |\n",
      "|    clip_fraction        | 0.0141       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.64        |\n",
      "|    explained_variance   | -1.14        |\n",
      "|    learning_rate        | 0.00418      |\n",
      "|    loss                 | -0.000451    |\n",
      "|    n_updates            | 490          |\n",
      "|    policy_gradient_loss | -0.00077     |\n",
      "|    std                  | 0.575        |\n",
      "|    value_loss           | 1.81e-09     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.00102 |\n",
      "| time/              |          |\n",
      "|    fps             | 674      |\n",
      "|    iterations      | 50       |\n",
      "|    time_elapsed    | 746      |\n",
      "|    total_timesteps | 504000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=524160, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.000244   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 524160      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003530669 |\n",
      "|    clip_fraction        | 0.0133      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.59       |\n",
      "|    explained_variance   | -0.723      |\n",
      "|    learning_rate        | 0.00414     |\n",
      "|    loss                 | -0.00106    |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.000679   |\n",
      "|    std                  | 0.56        |\n",
      "|    value_loss           | 1.35e-09    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.000767 |\n",
      "| time/              |           |\n",
      "|    fps             | 674       |\n",
      "|    iterations      | 52        |\n",
      "|    time_elapsed    | 776       |\n",
      "|    total_timesteps | 524160    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=544320, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00223     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 544320       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039615547 |\n",
      "|    clip_fraction        | 0.0248       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.56        |\n",
      "|    explained_variance   | -2.44        |\n",
      "|    learning_rate        | 0.00411      |\n",
      "|    loss                 | -0.0033      |\n",
      "|    n_updates            | 530          |\n",
      "|    policy_gradient_loss | -0.00152     |\n",
      "|    std                  | 0.549        |\n",
      "|    value_loss           | 2.28e-09     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.000686 |\n",
      "| time/              |           |\n",
      "|    fps             | 674       |\n",
      "|    iterations      | 54        |\n",
      "|    time_elapsed    | 806       |\n",
      "|    total_timesteps | 544320    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=564480, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00164     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 564480       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032983634 |\n",
      "|    clip_fraction        | 0.0178       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.51        |\n",
      "|    explained_variance   | -1.79        |\n",
      "|    learning_rate        | 0.00408      |\n",
      "|    loss                 | -0.00127     |\n",
      "|    n_updates            | 550          |\n",
      "|    policy_gradient_loss | -0.0013      |\n",
      "|    std                  | 0.534        |\n",
      "|    value_loss           | 6.69e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.00055 |\n",
      "| time/              |          |\n",
      "|    fps             | 674      |\n",
      "|    iterations      | 56       |\n",
      "|    time_elapsed    | 836      |\n",
      "|    total_timesteps | 564480   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=584640, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.000264    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 584640       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037215499 |\n",
      "|    clip_fraction        | 0.0107       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.48        |\n",
      "|    explained_variance   | -1.6         |\n",
      "|    learning_rate        | 0.00404      |\n",
      "|    loss                 | -0.000501    |\n",
      "|    n_updates            | 570          |\n",
      "|    policy_gradient_loss | -0.000595    |\n",
      "|    std                  | 0.524        |\n",
      "|    value_loss           | 4.51e-09     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.000559 |\n",
      "| time/              |           |\n",
      "|    fps             | 674       |\n",
      "|    iterations      | 58        |\n",
      "|    time_elapsed    | 867       |\n",
      "|    total_timesteps | 584640    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=604800, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00237     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 604800       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015605096 |\n",
      "|    clip_fraction        | 0.00188      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.44        |\n",
      "|    explained_variance   | -3.75        |\n",
      "|    learning_rate        | 0.00401      |\n",
      "|    loss                 | -0.000321    |\n",
      "|    n_updates            | 590          |\n",
      "|    policy_gradient_loss | -0.00014     |\n",
      "|    std                  | 0.508        |\n",
      "|    value_loss           | 5.68e-10     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.000231 |\n",
      "| time/              |           |\n",
      "|    fps             | 674       |\n",
      "|    iterations      | 60        |\n",
      "|    time_elapsed    | 897       |\n",
      "|    total_timesteps | 604800    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=624960, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.00664    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 624960      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005779784 |\n",
      "|    clip_fraction        | 0.0181      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.44       |\n",
      "|    explained_variance   | -0.0378     |\n",
      "|    learning_rate        | 0.00398     |\n",
      "|    loss                 | -0.00232    |\n",
      "|    n_updates            | 610         |\n",
      "|    policy_gradient_loss | -0.00058    |\n",
      "|    std                  | 0.515       |\n",
      "|    value_loss           | 1.68e-08    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.000278 |\n",
      "| time/              |           |\n",
      "|    fps             | 671       |\n",
      "|    iterations      | 62        |\n",
      "|    time_elapsed    | 930       |\n",
      "|    total_timesteps | 624960    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=645120, episode_reward=-0.00 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00323     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 645120       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022609686 |\n",
      "|    clip_fraction        | 0.02         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | -3.79        |\n",
      "|    learning_rate        | 0.00394      |\n",
      "|    loss                 | -0.00239     |\n",
      "|    n_updates            | 630          |\n",
      "|    policy_gradient_loss | -0.0014      |\n",
      "|    std                  | 0.504        |\n",
      "|    value_loss           | 3e-10        |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.000268 |\n",
      "| time/              |           |\n",
      "|    fps             | 670       |\n",
      "|    iterations      | 64        |\n",
      "|    time_elapsed    | 962       |\n",
      "|    total_timesteps | 645120    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=665280, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.000636    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 665280       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035802252 |\n",
      "|    clip_fraction        | 0.0148       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.37        |\n",
      "|    explained_variance   | -4.1         |\n",
      "|    learning_rate        | 0.00391      |\n",
      "|    loss                 | -0.00208     |\n",
      "|    n_updates            | 650          |\n",
      "|    policy_gradient_loss | -0.000832    |\n",
      "|    std                  | 0.495        |\n",
      "|    value_loss           | 2.63e-10     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.000278 |\n",
      "| time/              |           |\n",
      "|    fps             | 668       |\n",
      "|    iterations      | 66        |\n",
      "|    time_elapsed    | 994       |\n",
      "|    total_timesteps | 665280    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=685440, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.000431    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 685440       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038240007 |\n",
      "|    clip_fraction        | 0.00788      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.3         |\n",
      "|    explained_variance   | -3.24        |\n",
      "|    learning_rate        | 0.00387      |\n",
      "|    loss                 | -0.000262    |\n",
      "|    n_updates            | 670          |\n",
      "|    policy_gradient_loss | -0.000477    |\n",
      "|    std                  | 0.478        |\n",
      "|    value_loss           | 5.42e-10     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.000266 |\n",
      "| time/              |           |\n",
      "|    fps             | 667       |\n",
      "|    iterations      | 68        |\n",
      "|    time_elapsed    | 1026      |\n",
      "|    total_timesteps | 685440    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=705600, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.00124    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 705600      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003542459 |\n",
      "|    clip_fraction        | 0.0169      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.34       |\n",
      "|    explained_variance   | -4.07       |\n",
      "|    learning_rate        | 0.00384     |\n",
      "|    loss                 | -0.00278    |\n",
      "|    n_updates            | 690         |\n",
      "|    policy_gradient_loss | -0.00127    |\n",
      "|    std                  | 0.491       |\n",
      "|    value_loss           | 8.72e-09    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.00025 |\n",
      "| time/              |          |\n",
      "|    fps             | 666      |\n",
      "|    iterations      | 70       |\n",
      "|    time_elapsed    | 1058     |\n",
      "|    total_timesteps | 705600   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=725760, episode_reward=-0.01 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0089      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 725760       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009637431 |\n",
      "|    clip_fraction        | 0.0073       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.36        |\n",
      "|    explained_variance   | -2.14        |\n",
      "|    learning_rate        | 0.00381      |\n",
      "|    loss                 | -0.000868    |\n",
      "|    n_updates            | 710          |\n",
      "|    policy_gradient_loss | -0.00055     |\n",
      "|    std                  | 0.502        |\n",
      "|    value_loss           | 6.42e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.00037 |\n",
      "| time/              |          |\n",
      "|    fps             | 664      |\n",
      "|    iterations      | 72       |\n",
      "|    time_elapsed    | 1091     |\n",
      "|    total_timesteps | 725760   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=745920, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00175     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 745920       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020519358 |\n",
      "|    clip_fraction        | 0.0145       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | -4.04        |\n",
      "|    learning_rate        | 0.00377      |\n",
      "|    loss                 | -0.00139     |\n",
      "|    n_updates            | 730          |\n",
      "|    policy_gradient_loss | -0.000999    |\n",
      "|    std                  | 0.517        |\n",
      "|    value_loss           | 1.49e-09     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.000332 |\n",
      "| time/              |           |\n",
      "|    fps             | 653       |\n",
      "|    iterations      | 74        |\n",
      "|    time_elapsed    | 1140      |\n",
      "|    total_timesteps | 745920    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=766080, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.000306    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 766080       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031842606 |\n",
      "|    clip_fraction        | 0.00625      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.33        |\n",
      "|    explained_variance   | -2.43        |\n",
      "|    learning_rate        | 0.00374      |\n",
      "|    loss                 | -0.00356     |\n",
      "|    n_updates            | 750          |\n",
      "|    policy_gradient_loss | -0.000531    |\n",
      "|    std                  | 0.508        |\n",
      "|    value_loss           | 8.22e-10     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.000332 |\n",
      "| time/              |           |\n",
      "|    fps             | 641       |\n",
      "|    iterations      | 76        |\n",
      "|    time_elapsed    | 1193      |\n",
      "|    total_timesteps | 766080    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=786240, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.000722   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 786240      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003422189 |\n",
      "|    clip_fraction        | 0.00823     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.29       |\n",
      "|    explained_variance   | -3.1        |\n",
      "|    learning_rate        | 0.00371     |\n",
      "|    loss                 | -0.000841   |\n",
      "|    n_updates            | 770         |\n",
      "|    policy_gradient_loss | -0.000647   |\n",
      "|    std                  | 0.5         |\n",
      "|    value_loss           | 7.86e-07    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.000326 |\n",
      "| time/              |           |\n",
      "|    fps             | 635       |\n",
      "|    iterations      | 78        |\n",
      "|    time_elapsed    | 1237      |\n",
      "|    total_timesteps | 786240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=806400, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.00613    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 806400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003146786 |\n",
      "|    clip_fraction        | 0.00788     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.31       |\n",
      "|    explained_variance   | -1.67       |\n",
      "|    learning_rate        | 0.00367     |\n",
      "|    loss                 | -9.66e-05   |\n",
      "|    n_updates            | 790         |\n",
      "|    policy_gradient_loss | -0.000569   |\n",
      "|    std                  | 0.511       |\n",
      "|    value_loss           | 6.19e-09    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.000412 |\n",
      "| time/              |           |\n",
      "|    fps             | 632       |\n",
      "|    iterations      | 80        |\n",
      "|    time_elapsed    | 1274      |\n",
      "|    total_timesteps | 806400    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=826560, episode_reward=-0.01 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.00954    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 826560      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002249231 |\n",
      "|    clip_fraction        | 0.0113      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.28       |\n",
      "|    explained_variance   | -0.359      |\n",
      "|    learning_rate        | 0.00364     |\n",
      "|    loss                 | -0.00138    |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | -0.000595   |\n",
      "|    std                  | 0.521       |\n",
      "|    value_loss           | 1.53e-09    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.000395 |\n",
      "| time/              |           |\n",
      "|    fps             | 629       |\n",
      "|    iterations      | 82        |\n",
      "|    time_elapsed    | 1313      |\n",
      "|    total_timesteps | 826560    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=846720, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00803     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 846720       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033520248 |\n",
      "|    clip_fraction        | 0.00967      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.28        |\n",
      "|    explained_variance   | -0.247       |\n",
      "|    learning_rate        | 0.00361      |\n",
      "|    loss                 | -0.00157     |\n",
      "|    n_updates            | 830          |\n",
      "|    policy_gradient_loss | -0.000442    |\n",
      "|    std                  | 0.509        |\n",
      "|    value_loss           | 1.43e-09     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.000365 |\n",
      "| time/              |           |\n",
      "|    fps             | 625       |\n",
      "|    iterations      | 84        |\n",
      "|    time_elapsed    | 1353      |\n",
      "|    total_timesteps | 846720    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=866880, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00614     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 866880       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017676137 |\n",
      "|    clip_fraction        | 0.0116       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.28        |\n",
      "|    explained_variance   | -0.618       |\n",
      "|    learning_rate        | 0.00357      |\n",
      "|    loss                 | -4.7e-05     |\n",
      "|    n_updates            | 850          |\n",
      "|    policy_gradient_loss | -0.000648    |\n",
      "|    std                  | 0.509        |\n",
      "|    value_loss           | 2.07e-09     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.000584 |\n",
      "| time/              |           |\n",
      "|    fps             | 622       |\n",
      "|    iterations      | 86        |\n",
      "|    time_elapsed    | 1391      |\n",
      "|    total_timesteps | 866880    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=887040, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00144     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 887040       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018112044 |\n",
      "|    clip_fraction        | 0.00714      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | -3.45        |\n",
      "|    learning_rate        | 0.00354      |\n",
      "|    loss                 | -0.00115     |\n",
      "|    n_updates            | 870          |\n",
      "|    policy_gradient_loss | -0.000483    |\n",
      "|    std                  | 0.54         |\n",
      "|    value_loss           | 4.18e-09     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.000584 |\n",
      "| time/              |           |\n",
      "|    fps             | 619       |\n",
      "|    iterations      | 88        |\n",
      "|    time_elapsed    | 1431      |\n",
      "|    total_timesteps | 887040    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=907200, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00163     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 907200       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027968183 |\n",
      "|    clip_fraction        | 0.00395      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.34        |\n",
      "|    explained_variance   | -0.654       |\n",
      "|    learning_rate        | 0.0035       |\n",
      "|    loss                 | -3.8e-05     |\n",
      "|    n_updates            | 890          |\n",
      "|    policy_gradient_loss | -0.000294    |\n",
      "|    std                  | 0.531        |\n",
      "|    value_loss           | 3.7e-10      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.00059 |\n",
      "| time/              |          |\n",
      "|    fps             | 615      |\n",
      "|    iterations      | 90       |\n",
      "|    time_elapsed    | 1472     |\n",
      "|    total_timesteps | 907200   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=927360, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.000136   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 927360      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004526832 |\n",
      "|    clip_fraction        | 0.0129      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.33       |\n",
      "|    explained_variance   | -0.116      |\n",
      "|    learning_rate        | 0.00347     |\n",
      "|    loss                 | -0.00257    |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | -0.000806   |\n",
      "|    std                  | 0.533       |\n",
      "|    value_loss           | 1.56e-09    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.000646 |\n",
      "| time/              |           |\n",
      "|    fps             | 608       |\n",
      "|    iterations      | 92        |\n",
      "|    time_elapsed    | 1525      |\n",
      "|    total_timesteps | 927360    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=947520, episode_reward=-0.00 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.00456    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 947520      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001196582 |\n",
      "|    clip_fraction        | 0.00694     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.36       |\n",
      "|    explained_variance   | -4.01       |\n",
      "|    learning_rate        | 0.00344     |\n",
      "|    loss                 | -6.04e-05   |\n",
      "|    n_updates            | 930         |\n",
      "|    policy_gradient_loss | -0.000414   |\n",
      "|    std                  | 0.547       |\n",
      "|    value_loss           | 1.7e-10     |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.000535 |\n",
      "| time/              |           |\n",
      "|    fps             | 601       |\n",
      "|    iterations      | 94        |\n",
      "|    time_elapsed    | 1574      |\n",
      "|    total_timesteps | 947520    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=967680, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.00232    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 967680      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005241609 |\n",
      "|    clip_fraction        | 0.0284      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.29       |\n",
      "|    explained_variance   | -0.197      |\n",
      "|    learning_rate        | 0.0034      |\n",
      "|    loss                 | -0.000796   |\n",
      "|    n_updates            | 950         |\n",
      "|    policy_gradient_loss | -0.00114    |\n",
      "|    std                  | 0.521       |\n",
      "|    value_loss           | 8.65e-09    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.000466 |\n",
      "| time/              |           |\n",
      "|    fps             | 595       |\n",
      "|    iterations      | 96        |\n",
      "|    time_elapsed    | 1624      |\n",
      "|    total_timesteps | 967680    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=987840, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.00033    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 987840      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005101626 |\n",
      "|    clip_fraction        | 0.0492      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.22       |\n",
      "|    explained_variance   | -0.0233     |\n",
      "|    learning_rate        | 0.00337     |\n",
      "|    loss                 | 0.00242     |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | -0.000803   |\n",
      "|    std                  | 0.516       |\n",
      "|    value_loss           | 9.56e-08    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.000428 |\n",
      "| time/              |           |\n",
      "|    fps             | 593       |\n",
      "|    iterations      | 98        |\n",
      "|    time_elapsed    | 1665      |\n",
      "|    total_timesteps | 987840    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1008000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.00131    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1008000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003050095 |\n",
      "|    clip_fraction        | 0.0155      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.23       |\n",
      "|    explained_variance   | -0.125      |\n",
      "|    learning_rate        | 0.00334     |\n",
      "|    loss                 | -0.00371    |\n",
      "|    n_updates            | 990         |\n",
      "|    policy_gradient_loss | -0.00121    |\n",
      "|    std                  | 0.518       |\n",
      "|    value_loss           | 6.52e-09    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.000383 |\n",
      "| time/              |           |\n",
      "|    fps             | 587       |\n",
      "|    iterations      | 100       |\n",
      "|    time_elapsed    | 1714      |\n",
      "|    total_timesteps | 1008000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1028160, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00632     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1028160      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049713627 |\n",
      "|    clip_fraction        | 0.0233       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.2         |\n",
      "|    explained_variance   | -0.142       |\n",
      "|    learning_rate        | 0.0033       |\n",
      "|    loss                 | -0.00359     |\n",
      "|    n_updates            | 1010         |\n",
      "|    policy_gradient_loss | -0.000913    |\n",
      "|    std                  | 0.513        |\n",
      "|    value_loss           | 2.57e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0004  |\n",
      "| time/              |          |\n",
      "|    fps             | 585      |\n",
      "|    iterations      | 102      |\n",
      "|    time_elapsed    | 1756     |\n",
      "|    total_timesteps | 1028160  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1048320, episode_reward=-0.00 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00414     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1048320      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018434679 |\n",
      "|    clip_fraction        | 0.00895      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.09        |\n",
      "|    explained_variance   | -3.81        |\n",
      "|    learning_rate        | 0.00327      |\n",
      "|    loss                 | 3.71e-05     |\n",
      "|    n_updates            | 1030         |\n",
      "|    policy_gradient_loss | -0.000537    |\n",
      "|    std                  | 0.48         |\n",
      "|    value_loss           | 8.06e-10     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.000386 |\n",
      "| time/              |           |\n",
      "|    fps             | 581       |\n",
      "|    iterations      | 104       |\n",
      "|    time_elapsed    | 1803      |\n",
      "|    total_timesteps | 1048320   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1068480, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00161     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1068480      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028836722 |\n",
      "|    clip_fraction        | 0.0132       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.08        |\n",
      "|    explained_variance   | -2.54        |\n",
      "|    learning_rate        | 0.00324      |\n",
      "|    loss                 | -0.00192     |\n",
      "|    n_updates            | 1050         |\n",
      "|    policy_gradient_loss | -0.00107     |\n",
      "|    std                  | 0.487        |\n",
      "|    value_loss           | 3.93e-07     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.000338 |\n",
      "| time/              |           |\n",
      "|    fps             | 579       |\n",
      "|    iterations      | 106       |\n",
      "|    time_elapsed    | 1842      |\n",
      "|    total_timesteps | 1068480   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1088640, episode_reward=-0.00 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00401     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1088640      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050607026 |\n",
      "|    clip_fraction        | 0.0229       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.04        |\n",
      "|    explained_variance   | -3.32        |\n",
      "|    learning_rate        | 0.0032       |\n",
      "|    loss                 | 0.00212      |\n",
      "|    n_updates            | 1070         |\n",
      "|    policy_gradient_loss | -0.00142     |\n",
      "|    std                  | 0.477        |\n",
      "|    value_loss           | 5.26e-08     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.000311 |\n",
      "| time/              |           |\n",
      "|    fps             | 579       |\n",
      "|    iterations      | 108       |\n",
      "|    time_elapsed    | 1879      |\n",
      "|    total_timesteps | 1088640   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1108800, episode_reward=-0.00 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00495     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1108800      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040990054 |\n",
      "|    clip_fraction        | 0.0215       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.05        |\n",
      "|    explained_variance   | -0.0427      |\n",
      "|    learning_rate        | 0.00317      |\n",
      "|    loss                 | -0.0022      |\n",
      "|    n_updates            | 1090         |\n",
      "|    policy_gradient_loss | -0.000964    |\n",
      "|    std                  | 0.499        |\n",
      "|    value_loss           | 5.89e-09     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.000343 |\n",
      "| time/              |           |\n",
      "|    fps             | 577       |\n",
      "|    iterations      | 110       |\n",
      "|    time_elapsed    | 1920      |\n",
      "|    total_timesteps | 1108800   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1128960, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.000395    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1128960      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036295913 |\n",
      "|    clip_fraction        | 0.0121       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.11        |\n",
      "|    explained_variance   | -0.629       |\n",
      "|    learning_rate        | 0.00314      |\n",
      "|    loss                 | -0.00113     |\n",
      "|    n_updates            | 1110         |\n",
      "|    policy_gradient_loss | -0.000784    |\n",
      "|    std                  | 0.525        |\n",
      "|    value_loss           | 2.37e-07     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.000345 |\n",
      "| time/              |           |\n",
      "|    fps             | 577       |\n",
      "|    iterations      | 112       |\n",
      "|    time_elapsed    | 1953      |\n",
      "|    total_timesteps | 1128960   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1149120, episode_reward=-0.00 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00263     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1149120      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036483915 |\n",
      "|    clip_fraction        | 0.00517      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | -1.24        |\n",
      "|    learning_rate        | 0.0031       |\n",
      "|    loss                 | -0.00193     |\n",
      "|    n_updates            | 1130         |\n",
      "|    policy_gradient_loss | -0.000699    |\n",
      "|    std                  | 0.512        |\n",
      "|    value_loss           | 1.84e-08     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.000322 |\n",
      "| time/              |           |\n",
      "|    fps             | 576       |\n",
      "|    iterations      | 114       |\n",
      "|    time_elapsed    | 1993      |\n",
      "|    total_timesteps | 1149120   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1169280, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00027     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1169280      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040349504 |\n",
      "|    clip_fraction        | 0.0133       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.03        |\n",
      "|    explained_variance   | -2.35        |\n",
      "|    learning_rate        | 0.00307      |\n",
      "|    loss                 | -0.00111     |\n",
      "|    n_updates            | 1150         |\n",
      "|    policy_gradient_loss | -0.000725    |\n",
      "|    std                  | 0.506        |\n",
      "|    value_loss           | 2.58e-09     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.000304 |\n",
      "| time/              |           |\n",
      "|    fps             | 573       |\n",
      "|    iterations      | 116       |\n",
      "|    time_elapsed    | 2038      |\n",
      "|    total_timesteps | 1169280   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1189440, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.00364   |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1189440    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00320312 |\n",
      "|    clip_fraction        | 0.0165     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.01      |\n",
      "|    explained_variance   | -4.46      |\n",
      "|    learning_rate        | 0.00303    |\n",
      "|    loss                 | 6.84e-05   |\n",
      "|    n_updates            | 1170       |\n",
      "|    policy_gradient_loss | -0.000986  |\n",
      "|    std                  | 0.502      |\n",
      "|    value_loss           | 2.93e-07   |\n",
      "----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.000305 |\n",
      "| time/              |           |\n",
      "|    fps             | 571       |\n",
      "|    iterations      | 118       |\n",
      "|    time_elapsed    | 2080      |\n",
      "|    total_timesteps | 1189440   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1209600, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.000349    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1209600      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033167137 |\n",
      "|    clip_fraction        | 0.0209       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.993       |\n",
      "|    explained_variance   | -4.29        |\n",
      "|    learning_rate        | 0.003        |\n",
      "|    loss                 | 0.000539     |\n",
      "|    n_updates            | 1190         |\n",
      "|    policy_gradient_loss | -0.00145     |\n",
      "|    std                  | 0.488        |\n",
      "|    value_loss           | 4.01e-08     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.000311 |\n",
      "| time/              |           |\n",
      "|    fps             | 570       |\n",
      "|    iterations      | 120       |\n",
      "|    time_elapsed    | 2121      |\n",
      "|    total_timesteps | 1209600   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1229760, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.000892    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1229760      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023818105 |\n",
      "|    clip_fraction        | 0.0092       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.961       |\n",
      "|    explained_variance   | -3.43        |\n",
      "|    learning_rate        | 0.00297      |\n",
      "|    loss                 | -0.0018      |\n",
      "|    n_updates            | 1210         |\n",
      "|    policy_gradient_loss | -0.000677    |\n",
      "|    std                  | 0.477        |\n",
      "|    value_loss           | 1.77e-10     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.000259 |\n",
      "| time/              |           |\n",
      "|    fps             | 567       |\n",
      "|    iterations      | 122       |\n",
      "|    time_elapsed    | 2166      |\n",
      "|    total_timesteps | 1229760   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1249920, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -8.11e-05   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1249920     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002375525 |\n",
      "|    clip_fraction        | 0.0086      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.921      |\n",
      "|    explained_variance   | -0.997      |\n",
      "|    learning_rate        | 0.00293     |\n",
      "|    loss                 | -0.00181    |\n",
      "|    n_updates            | 1230        |\n",
      "|    policy_gradient_loss | -0.000594   |\n",
      "|    std                  | 0.473       |\n",
      "|    value_loss           | 9.07e-10    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.000108 |\n",
      "| time/              |           |\n",
      "|    fps             | 564       |\n",
      "|    iterations      | 124       |\n",
      "|    time_elapsed    | 2213      |\n",
      "|    total_timesteps | 1249920   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1270080, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00024     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1270080      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024172931 |\n",
      "|    clip_fraction        | 0.0141       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.901       |\n",
      "|    explained_variance   | -0.113       |\n",
      "|    learning_rate        | 0.0029       |\n",
      "|    loss                 | -0.000307    |\n",
      "|    n_updates            | 1250         |\n",
      "|    policy_gradient_loss | -0.000622    |\n",
      "|    std                  | 0.472        |\n",
      "|    value_loss           | 3.24e-07     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.000164 |\n",
      "| time/              |           |\n",
      "|    fps             | 563       |\n",
      "|    iterations      | 126       |\n",
      "|    time_elapsed    | 2253      |\n",
      "|    total_timesteps | 1270080   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1290240, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00111     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1290240      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045966306 |\n",
      "|    clip_fraction        | 0.0202       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.856       |\n",
      "|    explained_variance   | -3.98        |\n",
      "|    learning_rate        | 0.00287      |\n",
      "|    loss                 | -0.00215     |\n",
      "|    n_updates            | 1270         |\n",
      "|    policy_gradient_loss | -0.00131     |\n",
      "|    std                  | 0.454        |\n",
      "|    value_loss           | 4.02e-08     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.000164 |\n",
      "| time/              |           |\n",
      "|    fps             | 562       |\n",
      "|    iterations      | 128       |\n",
      "|    time_elapsed    | 2291      |\n",
      "|    total_timesteps | 1290240   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1310400, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00109     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1310400      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034779487 |\n",
      "|    clip_fraction        | 0.012        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.857       |\n",
      "|    explained_variance   | -3.19        |\n",
      "|    learning_rate        | 0.00283      |\n",
      "|    loss                 | -0.00143     |\n",
      "|    n_updates            | 1290         |\n",
      "|    policy_gradient_loss | -0.000898    |\n",
      "|    std                  | 0.457        |\n",
      "|    value_loss           | 4.11e-10     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.000167 |\n",
      "| time/              |           |\n",
      "|    fps             | 561       |\n",
      "|    iterations      | 130       |\n",
      "|    time_elapsed    | 2332      |\n",
      "|    total_timesteps | 1310400   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1330560, episode_reward=-0.00 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.00432    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1330560     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003917613 |\n",
      "|    clip_fraction        | 0.00719     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.848      |\n",
      "|    explained_variance   | -0.102      |\n",
      "|    learning_rate        | 0.0028      |\n",
      "|    loss                 | 0.00119     |\n",
      "|    n_updates            | 1310        |\n",
      "|    policy_gradient_loss | -0.00041    |\n",
      "|    std                  | 0.471       |\n",
      "|    value_loss           | 3e-09       |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.000189 |\n",
      "| time/              |           |\n",
      "|    fps             | 559       |\n",
      "|    iterations      | 132       |\n",
      "|    time_elapsed    | 2376      |\n",
      "|    total_timesteps | 1330560   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1350720, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00338     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1350720      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040019266 |\n",
      "|    clip_fraction        | 0.0181       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.787       |\n",
      "|    explained_variance   | -1.08        |\n",
      "|    learning_rate        | 0.00277      |\n",
      "|    loss                 | -0.00178     |\n",
      "|    n_updates            | 1330         |\n",
      "|    policy_gradient_loss | -0.00119     |\n",
      "|    std                  | 0.438        |\n",
      "|    value_loss           | 6.71e-07     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.000206 |\n",
      "| time/              |           |\n",
      "|    fps             | 560       |\n",
      "|    iterations      | 134       |\n",
      "|    time_elapsed    | 2410      |\n",
      "|    total_timesteps | 1350720   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1370880, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.00192    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1370880     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004001677 |\n",
      "|    clip_fraction        | 0.012       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.756      |\n",
      "|    explained_variance   | -2.77       |\n",
      "|    learning_rate        | 0.00273     |\n",
      "|    loss                 | -0.00253    |\n",
      "|    n_updates            | 1350        |\n",
      "|    policy_gradient_loss | -0.000988   |\n",
      "|    std                  | 0.434       |\n",
      "|    value_loss           | 2.33e-09    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.000192 |\n",
      "| time/              |           |\n",
      "|    fps             | 561       |\n",
      "|    iterations      | 136       |\n",
      "|    time_elapsed    | 2443      |\n",
      "|    total_timesteps | 1370880   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1391040, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00329     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1391040      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039547244 |\n",
      "|    clip_fraction        | 0.013        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.716       |\n",
      "|    explained_variance   | -1.99        |\n",
      "|    learning_rate        | 0.0027       |\n",
      "|    loss                 | -0.00135     |\n",
      "|    n_updates            | 1370         |\n",
      "|    policy_gradient_loss | -0.000969    |\n",
      "|    std                  | 0.424        |\n",
      "|    value_loss           | 8.11e-10     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.000157 |\n",
      "| time/              |           |\n",
      "|    fps             | 561       |\n",
      "|    iterations      | 138       |\n",
      "|    time_elapsed    | 2475      |\n",
      "|    total_timesteps | 1391040   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1411200, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00117     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1411200      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040685628 |\n",
      "|    clip_fraction        | 0.0134       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.686       |\n",
      "|    explained_variance   | -2.24        |\n",
      "|    learning_rate        | 0.00266      |\n",
      "|    loss                 | -0.00252     |\n",
      "|    n_updates            | 1390         |\n",
      "|    policy_gradient_loss | -0.000936    |\n",
      "|    std                  | 0.426        |\n",
      "|    value_loss           | 2.17e-10     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.000179 |\n",
      "| time/              |           |\n",
      "|    fps             | 562       |\n",
      "|    iterations      | 140       |\n",
      "|    time_elapsed    | 2508      |\n",
      "|    total_timesteps | 1411200   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1431360, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00229     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1431360      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015442271 |\n",
      "|    clip_fraction        | 0.00509      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.695       |\n",
      "|    explained_variance   | -2.47        |\n",
      "|    learning_rate        | 0.00263      |\n",
      "|    loss                 | -0.00128     |\n",
      "|    n_updates            | 1410         |\n",
      "|    policy_gradient_loss | -0.000378    |\n",
      "|    std                  | 0.438        |\n",
      "|    value_loss           | 3.7e-10      |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.000181 |\n",
      "| time/              |           |\n",
      "|    fps             | 563       |\n",
      "|    iterations      | 142       |\n",
      "|    time_elapsed    | 2540      |\n",
      "|    total_timesteps | 1431360   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1451520, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.000681    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1451520      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040166685 |\n",
      "|    clip_fraction        | 0.0213       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.702       |\n",
      "|    explained_variance   | -3.8         |\n",
      "|    learning_rate        | 0.0026       |\n",
      "|    loss                 | -0.00405     |\n",
      "|    n_updates            | 1430         |\n",
      "|    policy_gradient_loss | -0.00137     |\n",
      "|    std                  | 0.438        |\n",
      "|    value_loss           | 1.22e-10     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.000152 |\n",
      "| time/              |           |\n",
      "|    fps             | 564       |\n",
      "|    iterations      | 144       |\n",
      "|    time_elapsed    | 2572      |\n",
      "|    total_timesteps | 1451520   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1471680, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.000178   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1471680     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003901764 |\n",
      "|    clip_fraction        | 0.0127      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.688      |\n",
      "|    explained_variance   | -1.48       |\n",
      "|    learning_rate        | 0.00256     |\n",
      "|    loss                 | -0.000636   |\n",
      "|    n_updates            | 1450        |\n",
      "|    policy_gradient_loss | -0.000685   |\n",
      "|    std                  | 0.43        |\n",
      "|    value_loss           | 1.3e-10     |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.000132 |\n",
      "| time/              |           |\n",
      "|    fps             | 564       |\n",
      "|    iterations      | 146       |\n",
      "|    time_elapsed    | 2604      |\n",
      "|    total_timesteps | 1471680   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1491840, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00112     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1491840      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024464494 |\n",
      "|    clip_fraction        | 0.00925      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.679       |\n",
      "|    explained_variance   | -0.964       |\n",
      "|    learning_rate        | 0.00253      |\n",
      "|    loss                 | -0.00142     |\n",
      "|    n_updates            | 1470         |\n",
      "|    policy_gradient_loss | -0.000564    |\n",
      "|    std                  | 0.416        |\n",
      "|    value_loss           | 5.89e-10     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.000132 |\n",
      "| time/              |           |\n",
      "|    fps             | 565       |\n",
      "|    iterations      | 148       |\n",
      "|    time_elapsed    | 2637      |\n",
      "|    total_timesteps | 1491840   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1512000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.000234    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1512000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013177689 |\n",
      "|    clip_fraction        | 0.00182      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.64        |\n",
      "|    explained_variance   | -1.74        |\n",
      "|    learning_rate        | 0.0025       |\n",
      "|    loss                 | -5.64e-05    |\n",
      "|    n_updates            | 1490         |\n",
      "|    policy_gradient_loss | -0.000157    |\n",
      "|    std                  | 0.41         |\n",
      "|    value_loss           | 3.66e-10     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.000133 |\n",
      "| time/              |           |\n",
      "|    fps             | 566       |\n",
      "|    iterations      | 150       |\n",
      "|    time_elapsed    | 2669      |\n",
      "|    total_timesteps | 1512000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1532160, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.000225    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1532160      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027690378 |\n",
      "|    clip_fraction        | 0.0123       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.633       |\n",
      "|    explained_variance   | -2.54        |\n",
      "|    learning_rate        | 0.00246      |\n",
      "|    loss                 | -0.00198     |\n",
      "|    n_updates            | 1510         |\n",
      "|    policy_gradient_loss | -0.00107     |\n",
      "|    std                  | 0.406        |\n",
      "|    value_loss           | 1.15e-07     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -9.41e-05 |\n",
      "| time/              |           |\n",
      "|    fps             | 566       |\n",
      "|    iterations      | 152       |\n",
      "|    time_elapsed    | 2702      |\n",
      "|    total_timesteps | 1532160   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1552320, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.000257    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1552320      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030935695 |\n",
      "|    clip_fraction        | 0.00618      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.618       |\n",
      "|    explained_variance   | -0.36        |\n",
      "|    learning_rate        | 0.00243      |\n",
      "|    loss                 | -0.00229     |\n",
      "|    n_updates            | 1530         |\n",
      "|    policy_gradient_loss | -0.000255    |\n",
      "|    std                  | 0.408        |\n",
      "|    value_loss           | 4.85e-08     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.000107 |\n",
      "| time/              |           |\n",
      "|    fps             | 567       |\n",
      "|    iterations      | 154       |\n",
      "|    time_elapsed    | 2734      |\n",
      "|    total_timesteps | 1552320   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1572480, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00102     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1572480      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032716296 |\n",
      "|    clip_fraction        | 0.0143       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.635       |\n",
      "|    explained_variance   | -4.23        |\n",
      "|    learning_rate        | 0.0024       |\n",
      "|    loss                 | -0.00256     |\n",
      "|    n_updates            | 1550         |\n",
      "|    policy_gradient_loss | -0.000957    |\n",
      "|    std                  | 0.417        |\n",
      "|    value_loss           | 2.41e-08     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.000109 |\n",
      "| time/              |           |\n",
      "|    fps             | 568       |\n",
      "|    iterations      | 156       |\n",
      "|    time_elapsed    | 2766      |\n",
      "|    total_timesteps | 1572480   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1592640, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00227     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1592640      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046381173 |\n",
      "|    clip_fraction        | 0.0278       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.677       |\n",
      "|    explained_variance   | -0.404       |\n",
      "|    learning_rate        | 0.00236      |\n",
      "|    loss                 | -0.00272     |\n",
      "|    n_updates            | 1570         |\n",
      "|    policy_gradient_loss | -0.00115     |\n",
      "|    std                  | 0.437        |\n",
      "|    value_loss           | 7.73e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.00011 |\n",
      "| time/              |          |\n",
      "|    fps             | 568      |\n",
      "|    iterations      | 158      |\n",
      "|    time_elapsed    | 2800     |\n",
      "|    total_timesteps | 1592640  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1612800, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.000668    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1612800      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0102496045 |\n",
      "|    clip_fraction        | 0.0875       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.67        |\n",
      "|    explained_variance   | -0.00431     |\n",
      "|    learning_rate        | 0.00233      |\n",
      "|    loss                 | 0.000337     |\n",
      "|    n_updates            | 1590         |\n",
      "|    policy_gradient_loss | -0.00136     |\n",
      "|    std                  | 0.429        |\n",
      "|    value_loss           | 2.72e-08     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.000169 |\n",
      "| time/              |           |\n",
      "|    fps             | 569       |\n",
      "|    iterations      | 160       |\n",
      "|    time_elapsed    | 2832      |\n",
      "|    total_timesteps | 1612800   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1632960, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.000637  |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1632960    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00321317 |\n",
      "|    clip_fraction        | 0.0208     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.622     |\n",
      "|    explained_variance   | -3.48      |\n",
      "|    learning_rate        | 0.0023     |\n",
      "|    loss                 | -0.00357   |\n",
      "|    n_updates            | 1610       |\n",
      "|    policy_gradient_loss | -0.00133   |\n",
      "|    std                  | 0.423      |\n",
      "|    value_loss           | 9.96e-09   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.00018 |\n",
      "| time/              |          |\n",
      "|    fps             | 570      |\n",
      "|    iterations      | 162      |\n",
      "|    time_elapsed    | 2864     |\n",
      "|    total_timesteps | 1632960  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1653120, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00226     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1653120      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053255707 |\n",
      "|    clip_fraction        | 0.0259       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.598       |\n",
      "|    explained_variance   | -0.276       |\n",
      "|    learning_rate        | 0.00226      |\n",
      "|    loss                 | 0.00169      |\n",
      "|    n_updates            | 1630         |\n",
      "|    policy_gradient_loss | -0.00123     |\n",
      "|    std                  | 0.423        |\n",
      "|    value_loss           | 1.51e-08     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.000197 |\n",
      "| time/              |           |\n",
      "|    fps             | 570       |\n",
      "|    iterations      | 164       |\n",
      "|    time_elapsed    | 2896      |\n",
      "|    total_timesteps | 1653120   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1673280, episode_reward=-0.01 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00677     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1673280      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038748872 |\n",
      "|    clip_fraction        | 0.0196       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.571       |\n",
      "|    explained_variance   | -2.19        |\n",
      "|    learning_rate        | 0.00223      |\n",
      "|    loss                 | -0.00247     |\n",
      "|    n_updates            | 1650         |\n",
      "|    policy_gradient_loss | -0.00138     |\n",
      "|    std                  | 0.413        |\n",
      "|    value_loss           | 1.33e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.00018 |\n",
      "| time/              |          |\n",
      "|    fps             | 571      |\n",
      "|    iterations      | 166      |\n",
      "|    time_elapsed    | 2929     |\n",
      "|    total_timesteps | 1673280  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1693440, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00501     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1693440      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035444584 |\n",
      "|    clip_fraction        | 0.012        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.548       |\n",
      "|    explained_variance   | -2.87        |\n",
      "|    learning_rate        | 0.00219      |\n",
      "|    loss                 | 0.000564     |\n",
      "|    n_updates            | 1670         |\n",
      "|    policy_gradient_loss | -0.000691    |\n",
      "|    std                  | 0.408        |\n",
      "|    value_loss           | 4.07e-09     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.000178 |\n",
      "| time/              |           |\n",
      "|    fps             | 571       |\n",
      "|    iterations      | 168       |\n",
      "|    time_elapsed    | 2961      |\n",
      "|    total_timesteps | 1693440   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1713600, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.00352    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1713600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004704639 |\n",
      "|    clip_fraction        | 0.0142      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.522      |\n",
      "|    explained_variance   | -4.01       |\n",
      "|    learning_rate        | 0.00216     |\n",
      "|    loss                 | 0.000198    |\n",
      "|    n_updates            | 1690        |\n",
      "|    policy_gradient_loss | -0.000928   |\n",
      "|    std                  | 0.4         |\n",
      "|    value_loss           | 4.89e-10    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.000164 |\n",
      "| time/              |           |\n",
      "|    fps             | 572       |\n",
      "|    iterations      | 170       |\n",
      "|    time_elapsed    | 2993      |\n",
      "|    total_timesteps | 1713600   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1733760, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.00571    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1733760     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003960679 |\n",
      "|    clip_fraction        | 0.0355      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.5        |\n",
      "|    explained_variance   | -1.23       |\n",
      "|    learning_rate        | 0.00213     |\n",
      "|    loss                 | -0.00423    |\n",
      "|    n_updates            | 1710        |\n",
      "|    policy_gradient_loss | -0.00188    |\n",
      "|    std                  | 0.395       |\n",
      "|    value_loss           | 2.35e-09    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.000122 |\n",
      "| time/              |           |\n",
      "|    fps             | 573       |\n",
      "|    iterations      | 172       |\n",
      "|    time_elapsed    | 3023      |\n",
      "|    total_timesteps | 1733760   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1753920, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00172     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1753920      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033395065 |\n",
      "|    clip_fraction        | 0.0152       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.467       |\n",
      "|    explained_variance   | -3.75        |\n",
      "|    learning_rate        | 0.00209      |\n",
      "|    loss                 | -0.000832    |\n",
      "|    n_updates            | 1730         |\n",
      "|    policy_gradient_loss | -0.000875    |\n",
      "|    std                  | 0.393        |\n",
      "|    value_loss           | 1.02e-07     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.000103 |\n",
      "| time/              |           |\n",
      "|    fps             | 574       |\n",
      "|    iterations      | 174       |\n",
      "|    time_elapsed    | 3054      |\n",
      "|    total_timesteps | 1753920   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1774080, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00152     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1774080      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029321637 |\n",
      "|    clip_fraction        | 0.00744      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.45        |\n",
      "|    explained_variance   | -3.04        |\n",
      "|    learning_rate        | 0.00206      |\n",
      "|    loss                 | -0.00312     |\n",
      "|    n_updates            | 1750         |\n",
      "|    policy_gradient_loss | -0.000606    |\n",
      "|    std                  | 0.386        |\n",
      "|    value_loss           | 4.03e-09     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -8.76e-05 |\n",
      "| time/              |           |\n",
      "|    fps             | 574       |\n",
      "|    iterations      | 176       |\n",
      "|    time_elapsed    | 3085      |\n",
      "|    total_timesteps | 1774080   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1794240, episode_reward=-0.00 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1.26e+03      |\n",
      "|    mean_reward          | -0.00307      |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 1794240       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00072025904 |\n",
      "|    clip_fraction        | 0.000317      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.41         |\n",
      "|    explained_variance   | -1.06         |\n",
      "|    learning_rate        | 0.00203       |\n",
      "|    loss                 | -0.000877     |\n",
      "|    n_updates            | 1770          |\n",
      "|    policy_gradient_loss | -8.73e-05     |\n",
      "|    std                  | 0.389         |\n",
      "|    value_loss           | 6.53e-10      |\n",
      "-------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -9.55e-05 |\n",
      "| time/              |           |\n",
      "|    fps             | 575       |\n",
      "|    iterations      | 178       |\n",
      "|    time_elapsed    | 3115      |\n",
      "|    total_timesteps | 1794240   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1814400, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.000237    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1814400      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026707614 |\n",
      "|    clip_fraction        | 0.00796      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.399       |\n",
      "|    explained_variance   | -0.0304      |\n",
      "|    learning_rate        | 0.00199      |\n",
      "|    loss                 | 0.00136      |\n",
      "|    n_updates            | 1790         |\n",
      "|    policy_gradient_loss | -0.000307    |\n",
      "|    std                  | 0.387        |\n",
      "|    value_loss           | 6.95e-09     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.000117 |\n",
      "| time/              |           |\n",
      "|    fps             | 576       |\n",
      "|    iterations      | 180       |\n",
      "|    time_elapsed    | 3146      |\n",
      "|    total_timesteps | 1814400   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1834560, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.000485    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1834560      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031228778 |\n",
      "|    clip_fraction        | 0.00992      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.38        |\n",
      "|    explained_variance   | -0.376       |\n",
      "|    learning_rate        | 0.00196      |\n",
      "|    loss                 | -0.00182     |\n",
      "|    n_updates            | 1810         |\n",
      "|    policy_gradient_loss | -0.000628    |\n",
      "|    std                  | 0.385        |\n",
      "|    value_loss           | 1.1e-08      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.00013 |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 182      |\n",
      "|    time_elapsed    | 3176     |\n",
      "|    total_timesteps | 1834560  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1854720, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.000238    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1854720      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015120313 |\n",
      "|    clip_fraction        | 0.00785      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.388       |\n",
      "|    explained_variance   | -0.889       |\n",
      "|    learning_rate        | 0.00193      |\n",
      "|    loss                 | -0.00142     |\n",
      "|    n_updates            | 1830         |\n",
      "|    policy_gradient_loss | -0.000434    |\n",
      "|    std                  | 0.393        |\n",
      "|    value_loss           | 2.2e-09      |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.000137 |\n",
      "| time/              |           |\n",
      "|    fps             | 578       |\n",
      "|    iterations      | 184       |\n",
      "|    time_elapsed    | 3206      |\n",
      "|    total_timesteps | 1854720   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1874880, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.000715    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1874880      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032915373 |\n",
      "|    clip_fraction        | 0.00628      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.352       |\n",
      "|    explained_variance   | -0.479       |\n",
      "|    learning_rate        | 0.00189      |\n",
      "|    loss                 | -0.000806    |\n",
      "|    n_updates            | 1850         |\n",
      "|    policy_gradient_loss | -0.000365    |\n",
      "|    std                  | 0.384        |\n",
      "|    value_loss           | 3.81e-09     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.000154 |\n",
      "| time/              |           |\n",
      "|    fps             | 579       |\n",
      "|    iterations      | 186       |\n",
      "|    time_elapsed    | 3237      |\n",
      "|    total_timesteps | 1874880   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1895040, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00213     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1895040      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018310577 |\n",
      "|    clip_fraction        | 0.00548      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.352       |\n",
      "|    explained_variance   | -0.954       |\n",
      "|    learning_rate        | 0.00186      |\n",
      "|    loss                 | 0.000789     |\n",
      "|    n_updates            | 1870         |\n",
      "|    policy_gradient_loss | -0.000459    |\n",
      "|    std                  | 0.387        |\n",
      "|    value_loss           | 1.08e-09     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.000162 |\n",
      "| time/              |           |\n",
      "|    fps             | 579       |\n",
      "|    iterations      | 188       |\n",
      "|    time_elapsed    | 3267      |\n",
      "|    total_timesteps | 1895040   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1915200, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00327     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1915200      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014274685 |\n",
      "|    clip_fraction        | 0.00183      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.353       |\n",
      "|    explained_variance   | -1.85        |\n",
      "|    learning_rate        | 0.00182      |\n",
      "|    loss                 | -0.000525    |\n",
      "|    n_updates            | 1890         |\n",
      "|    policy_gradient_loss | -0.000301    |\n",
      "|    std                  | 0.392        |\n",
      "|    value_loss           | 1.39e-08     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.000153 |\n",
      "| time/              |           |\n",
      "|    fps             | 580       |\n",
      "|    iterations      | 190       |\n",
      "|    time_elapsed    | 3298      |\n",
      "|    total_timesteps | 1915200   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1935360, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.000365   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1935360     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003007918 |\n",
      "|    clip_fraction        | 0.00841     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.284      |\n",
      "|    explained_variance   | -2.73       |\n",
      "|    learning_rate        | 0.00179     |\n",
      "|    loss                 | -0.00218    |\n",
      "|    n_updates            | 1910        |\n",
      "|    policy_gradient_loss | -0.000922   |\n",
      "|    std                  | 0.381       |\n",
      "|    value_loss           | 6.52e-09    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.000139 |\n",
      "| time/              |           |\n",
      "|    fps             | 581       |\n",
      "|    iterations      | 192       |\n",
      "|    time_elapsed    | 3328      |\n",
      "|    total_timesteps | 1935360   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1955520, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.00923   |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1955520    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00403382 |\n",
      "|    clip_fraction        | 0.0135     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.244     |\n",
      "|    explained_variance   | -3.98      |\n",
      "|    learning_rate        | 0.00176    |\n",
      "|    loss                 | -0.000701  |\n",
      "|    n_updates            | 1930       |\n",
      "|    policy_gradient_loss | -0.00102   |\n",
      "|    std                  | 0.389      |\n",
      "|    value_loss           | 5.04e-09   |\n",
      "----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.000168 |\n",
      "| time/              |           |\n",
      "|    fps             | 582       |\n",
      "|    iterations      | 194       |\n",
      "|    time_elapsed    | 3359      |\n",
      "|    total_timesteps | 1955520   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1975680, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00179     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1975680      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022553662 |\n",
      "|    clip_fraction        | 0.00633      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.247       |\n",
      "|    explained_variance   | -2.25        |\n",
      "|    learning_rate        | 0.00172      |\n",
      "|    loss                 | -0.00351     |\n",
      "|    n_updates            | 1950         |\n",
      "|    policy_gradient_loss | -0.000652    |\n",
      "|    std                  | 0.387        |\n",
      "|    value_loss           | 6.49e-09     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.000148 |\n",
      "| time/              |           |\n",
      "|    fps             | 582       |\n",
      "|    iterations      | 196       |\n",
      "|    time_elapsed    | 3390      |\n",
      "|    total_timesteps | 1975680   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1995840, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00615     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1995840      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032028072 |\n",
      "|    clip_fraction        | 0.0133       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.276       |\n",
      "|    explained_variance   | -3.99        |\n",
      "|    learning_rate        | 0.00169      |\n",
      "|    loss                 | -0.00178     |\n",
      "|    n_updates            | 1970         |\n",
      "|    policy_gradient_loss | -0.00102     |\n",
      "|    std                  | 0.393        |\n",
      "|    value_loss           | 2.53e-09     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.000114 |\n",
      "| time/              |           |\n",
      "|    fps             | 583       |\n",
      "|    iterations      | 198       |\n",
      "|    time_elapsed    | 3420      |\n",
      "|    total_timesteps | 1995840   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2016000, episode_reward=-0.01 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00896     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2016000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024670037 |\n",
      "|    clip_fraction        | 0.00766      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.23        |\n",
      "|    explained_variance   | -0.891       |\n",
      "|    learning_rate        | 0.00166      |\n",
      "|    loss                 | -0.00071     |\n",
      "|    n_updates            | 1990         |\n",
      "|    policy_gradient_loss | -0.000673    |\n",
      "|    std                  | 0.38         |\n",
      "|    value_loss           | 2.08e-09     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.000113 |\n",
      "| time/              |           |\n",
      "|    fps             | 584       |\n",
      "|    iterations      | 200       |\n",
      "|    time_elapsed    | 3451      |\n",
      "|    total_timesteps | 2016000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2036160, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00206     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2036160      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044150637 |\n",
      "|    clip_fraction        | 0.0195       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.194       |\n",
      "|    explained_variance   | -3.42        |\n",
      "|    learning_rate        | 0.00162      |\n",
      "|    loss                 | -0.00163     |\n",
      "|    n_updates            | 2010         |\n",
      "|    policy_gradient_loss | -0.000986    |\n",
      "|    std                  | 0.383        |\n",
      "|    value_loss           | 8.83e-09     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.000138 |\n",
      "| time/              |           |\n",
      "|    fps             | 584       |\n",
      "|    iterations      | 202       |\n",
      "|    time_elapsed    | 3481      |\n",
      "|    total_timesteps | 2036160   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2056320, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.00391    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2056320     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002533453 |\n",
      "|    clip_fraction        | 0.00556     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.178      |\n",
      "|    explained_variance   | -3.56       |\n",
      "|    learning_rate        | 0.00159     |\n",
      "|    loss                 | -0.00175    |\n",
      "|    n_updates            | 2030        |\n",
      "|    policy_gradient_loss | -0.00053    |\n",
      "|    std                  | 0.374       |\n",
      "|    value_loss           | 1.12e-09    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.000159 |\n",
      "| time/              |           |\n",
      "|    fps             | 585       |\n",
      "|    iterations      | 204       |\n",
      "|    time_elapsed    | 3512      |\n",
      "|    total_timesteps | 2056320   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2076480, episode_reward=-0.00 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00375     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2076480      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017026537 |\n",
      "|    clip_fraction        | 0.00494      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.162       |\n",
      "|    explained_variance   | -0.535       |\n",
      "|    learning_rate        | 0.00156      |\n",
      "|    loss                 | 1.89e-05     |\n",
      "|    n_updates            | 2050         |\n",
      "|    policy_gradient_loss | -0.000285    |\n",
      "|    std                  | 0.378        |\n",
      "|    value_loss           | 3.72e-10     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.00013 |\n",
      "| time/              |          |\n",
      "|    fps             | 586      |\n",
      "|    iterations      | 206      |\n",
      "|    time_elapsed    | 3542     |\n",
      "|    total_timesteps | 2076480  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2096640, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00267     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2096640      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012953404 |\n",
      "|    clip_fraction        | 0.00591      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.113       |\n",
      "|    explained_variance   | -4.41        |\n",
      "|    learning_rate        | 0.00152      |\n",
      "|    loss                 | 2.44e-05     |\n",
      "|    n_updates            | 2070         |\n",
      "|    policy_gradient_loss | -0.000427    |\n",
      "|    std                  | 0.368        |\n",
      "|    value_loss           | 1.06e-10     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.00014 |\n",
      "| time/              |          |\n",
      "|    fps             | 586      |\n",
      "|    iterations      | 208      |\n",
      "|    time_elapsed    | 3573     |\n",
      "|    total_timesteps | 2096640  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2116800, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.00426    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2116800     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002265307 |\n",
      "|    clip_fraction        | 0.00554     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.103      |\n",
      "|    explained_variance   | -2.82       |\n",
      "|    learning_rate        | 0.00149     |\n",
      "|    loss                 | -0.000165   |\n",
      "|    n_updates            | 2090        |\n",
      "|    policy_gradient_loss | -0.000417   |\n",
      "|    std                  | 0.371       |\n",
      "|    value_loss           | 1.74e-10    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.000145 |\n",
      "| time/              |           |\n",
      "|    fps             | 587       |\n",
      "|    iterations      | 210       |\n",
      "|    time_elapsed    | 3604      |\n",
      "|    total_timesteps | 2116800   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2136960, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00176     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2136960      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020610504 |\n",
      "|    clip_fraction        | 0.00352      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0698      |\n",
      "|    explained_variance   | -3.99        |\n",
      "|    learning_rate        | 0.00146      |\n",
      "|    loss                 | 0.00131      |\n",
      "|    n_updates            | 2110         |\n",
      "|    policy_gradient_loss | -0.000115    |\n",
      "|    std                  | 0.363        |\n",
      "|    value_loss           | 1.54e-10     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.000135 |\n",
      "| time/              |           |\n",
      "|    fps             | 587       |\n",
      "|    iterations      | 212       |\n",
      "|    time_elapsed    | 3634      |\n",
      "|    total_timesteps | 2136960   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2157120, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00516     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2157120      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033349635 |\n",
      "|    clip_fraction        | 0.0107       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0278      |\n",
      "|    explained_variance   | -0.124       |\n",
      "|    learning_rate        | 0.00142      |\n",
      "|    loss                 | -0.000503    |\n",
      "|    n_updates            | 2130         |\n",
      "|    policy_gradient_loss | -0.000824    |\n",
      "|    std                  | 0.366        |\n",
      "|    value_loss           | 3.07e-10     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.000122 |\n",
      "| time/              |           |\n",
      "|    fps             | 588       |\n",
      "|    iterations      | 214       |\n",
      "|    time_elapsed    | 3665      |\n",
      "|    total_timesteps | 2157120   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2177280, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00101     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2177280      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024669277 |\n",
      "|    clip_fraction        | 0.00287      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.013       |\n",
      "|    explained_variance   | -0.868       |\n",
      "|    learning_rate        | 0.00139      |\n",
      "|    loss                 | 0.000308     |\n",
      "|    n_updates            | 2150         |\n",
      "|    policy_gradient_loss | -0.000232    |\n",
      "|    std                  | 0.363        |\n",
      "|    value_loss           | 2.03e-10     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.000117 |\n",
      "| time/              |           |\n",
      "|    fps             | 589       |\n",
      "|    iterations      | 216       |\n",
      "|    time_elapsed    | 3696      |\n",
      "|    total_timesteps | 2177280   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2197440, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00565     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2197440      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018735925 |\n",
      "|    clip_fraction        | 0.00381      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 0.0179       |\n",
      "|    explained_variance   | -3.72        |\n",
      "|    learning_rate        | 0.00135      |\n",
      "|    loss                 | 0.000893     |\n",
      "|    n_updates            | 2170         |\n",
      "|    policy_gradient_loss | -0.000325    |\n",
      "|    std                  | 0.351        |\n",
      "|    value_loss           | 7.45e-11     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -8.74e-05 |\n",
      "| time/              |           |\n",
      "|    fps             | 589       |\n",
      "|    iterations      | 218       |\n",
      "|    time_elapsed    | 3726      |\n",
      "|    total_timesteps | 2197440   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2217600, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.00195    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2217600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004122801 |\n",
      "|    clip_fraction        | 0.00882     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.0396      |\n",
      "|    explained_variance   | -2.91       |\n",
      "|    learning_rate        | 0.00132     |\n",
      "|    loss                 | 0.00161     |\n",
      "|    n_updates            | 2190        |\n",
      "|    policy_gradient_loss | -0.000632   |\n",
      "|    std                  | 0.346       |\n",
      "|    value_loss           | 9.34e-11    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -7.35e-05 |\n",
      "| time/              |           |\n",
      "|    fps             | 590       |\n",
      "|    iterations      | 220       |\n",
      "|    time_elapsed    | 3757      |\n",
      "|    total_timesteps | 2217600   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2237760, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0027      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2237760      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033165384 |\n",
      "|    clip_fraction        | 0.0239       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 0.0546       |\n",
      "|    explained_variance   | -3.52        |\n",
      "|    learning_rate        | 0.00129      |\n",
      "|    loss                 | -0.00383     |\n",
      "|    n_updates            | 2210         |\n",
      "|    policy_gradient_loss | -0.00152     |\n",
      "|    std                  | 0.341        |\n",
      "|    value_loss           | 7.39e-11     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -7.75e-05 |\n",
      "| time/              |           |\n",
      "|    fps             | 590       |\n",
      "|    iterations      | 222       |\n",
      "|    time_elapsed    | 3787      |\n",
      "|    total_timesteps | 2237760   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2257920, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00196     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2257920      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035620774 |\n",
      "|    clip_fraction        | 0.0134       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 0.0809       |\n",
      "|    explained_variance   | -0.859       |\n",
      "|    learning_rate        | 0.00125      |\n",
      "|    loss                 | 0.000184     |\n",
      "|    n_updates            | 2230         |\n",
      "|    policy_gradient_loss | -0.000634    |\n",
      "|    std                  | 0.338        |\n",
      "|    value_loss           | 1.99e-10     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -7.9e-05 |\n",
      "| time/              |          |\n",
      "|    fps             | 591      |\n",
      "|    iterations      | 224      |\n",
      "|    time_elapsed    | 3818     |\n",
      "|    total_timesteps | 2257920  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2278080, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00199     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2278080      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037734811 |\n",
      "|    clip_fraction        | 0.0147       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 0.105        |\n",
      "|    explained_variance   | -2.96        |\n",
      "|    learning_rate        | 0.00122      |\n",
      "|    loss                 | -0.00395     |\n",
      "|    n_updates            | 2250         |\n",
      "|    policy_gradient_loss | -0.0009      |\n",
      "|    std                  | 0.333        |\n",
      "|    value_loss           | 6.09e-11     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -7.58e-05 |\n",
      "| time/              |           |\n",
      "|    fps             | 591       |\n",
      "|    iterations      | 226       |\n",
      "|    time_elapsed    | 3849      |\n",
      "|    total_timesteps | 2278080   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2298240, episode_reward=-0.00 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00327     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2298240      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021549212 |\n",
      "|    clip_fraction        | 0.00371      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 0.0931       |\n",
      "|    explained_variance   | -2.96        |\n",
      "|    learning_rate        | 0.00119      |\n",
      "|    loss                 | -0.00256     |\n",
      "|    n_updates            | 2270         |\n",
      "|    policy_gradient_loss | -0.000413    |\n",
      "|    std                  | 0.333        |\n",
      "|    value_loss           | 1.09e-10     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -7.28e-05 |\n",
      "| time/              |           |\n",
      "|    fps             | 592       |\n",
      "|    iterations      | 228       |\n",
      "|    time_elapsed    | 3879      |\n",
      "|    total_timesteps | 2298240   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2318400, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.00129    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2318400     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005470774 |\n",
      "|    clip_fraction        | 0.0228      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.152       |\n",
      "|    explained_variance   | -2.39       |\n",
      "|    learning_rate        | 0.00115     |\n",
      "|    loss                 | -0.00128    |\n",
      "|    n_updates            | 2290        |\n",
      "|    policy_gradient_loss | -0.00134    |\n",
      "|    std                  | 0.318       |\n",
      "|    value_loss           | 8.91e-11    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -7.65e-05 |\n",
      "| time/              |           |\n",
      "|    fps             | 592       |\n",
      "|    iterations      | 230       |\n",
      "|    time_elapsed    | 3910      |\n",
      "|    total_timesteps | 2318400   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2338560, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00192     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2338560      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016177774 |\n",
      "|    clip_fraction        | 0.0071       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 0.148        |\n",
      "|    explained_variance   | -3.58        |\n",
      "|    learning_rate        | 0.00112      |\n",
      "|    loss                 | 0.000606     |\n",
      "|    n_updates            | 2310         |\n",
      "|    policy_gradient_loss | -0.00054     |\n",
      "|    std                  | 0.33         |\n",
      "|    value_loss           | 7.23e-11     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -8.45e-05 |\n",
      "| time/              |           |\n",
      "|    fps             | 593       |\n",
      "|    iterations      | 232       |\n",
      "|    time_elapsed    | 3941      |\n",
      "|    total_timesteps | 2338560   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2358720, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00628     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2358720      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022414285 |\n",
      "|    clip_fraction        | 0.00699      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 0.162        |\n",
      "|    explained_variance   | -3.23        |\n",
      "|    learning_rate        | 0.00109      |\n",
      "|    loss                 | -0.000922    |\n",
      "|    n_updates            | 2330         |\n",
      "|    policy_gradient_loss | -0.000599    |\n",
      "|    std                  | 0.331        |\n",
      "|    value_loss           | 3.04e-10     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -8.24e-05 |\n",
      "| time/              |           |\n",
      "|    fps             | 593       |\n",
      "|    iterations      | 234       |\n",
      "|    time_elapsed    | 3971      |\n",
      "|    total_timesteps | 2358720   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2378880, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0024     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2378880     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002367254 |\n",
      "|    clip_fraction        | 0.00196     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.184       |\n",
      "|    explained_variance   | -3.25       |\n",
      "|    learning_rate        | 0.00105     |\n",
      "|    loss                 | -0.000575   |\n",
      "|    n_updates            | 2350        |\n",
      "|    policy_gradient_loss | -0.00015    |\n",
      "|    std                  | 0.324       |\n",
      "|    value_loss           | 7.09e-11    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -7.69e-05 |\n",
      "| time/              |           |\n",
      "|    fps             | 594       |\n",
      "|    iterations      | 236       |\n",
      "|    time_elapsed    | 4002      |\n",
      "|    total_timesteps | 2378880   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2399040, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0019     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2399040     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005036407 |\n",
      "|    clip_fraction        | 0.0122      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.21        |\n",
      "|    explained_variance   | -3.69       |\n",
      "|    learning_rate        | 0.00102     |\n",
      "|    loss                 | -0.00194    |\n",
      "|    n_updates            | 2370        |\n",
      "|    policy_gradient_loss | -0.00105    |\n",
      "|    std                  | 0.32        |\n",
      "|    value_loss           | 9.86e-11    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -7.13e-05 |\n",
      "| time/              |           |\n",
      "|    fps             | 594       |\n",
      "|    iterations      | 238       |\n",
      "|    time_elapsed    | 4032      |\n",
      "|    total_timesteps | 2399040   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2419200, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.00251    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2419200     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004590636 |\n",
      "|    clip_fraction        | 0.0204      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.211       |\n",
      "|    explained_variance   | -3.55       |\n",
      "|    learning_rate        | 0.000985    |\n",
      "|    loss                 | -0.00383    |\n",
      "|    n_updates            | 2390        |\n",
      "|    policy_gradient_loss | -0.00111    |\n",
      "|    std                  | 0.325       |\n",
      "|    value_loss           | 1.05e-10    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -7.18e-05 |\n",
      "| time/              |           |\n",
      "|    fps             | 595       |\n",
      "|    iterations      | 240       |\n",
      "|    time_elapsed    | 4064      |\n",
      "|    total_timesteps | 2419200   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2439360, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.000919    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2439360      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013310189 |\n",
      "|    clip_fraction        | 0.000139     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 0.181        |\n",
      "|    explained_variance   | -0.521       |\n",
      "|    learning_rate        | 0.000951     |\n",
      "|    loss                 | -0.00082     |\n",
      "|    n_updates            | 2410         |\n",
      "|    policy_gradient_loss | -8.1e-05     |\n",
      "|    std                  | 0.329        |\n",
      "|    value_loss           | 6.31e-11     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -8.46e-05 |\n",
      "| time/              |           |\n",
      "|    fps             | 595       |\n",
      "|    iterations      | 242       |\n",
      "|    time_elapsed    | 4094      |\n",
      "|    total_timesteps | 2439360   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2459520, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00806     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2459520      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032090412 |\n",
      "|    clip_fraction        | 0.0117       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 0.19         |\n",
      "|    explained_variance   | -2.13        |\n",
      "|    learning_rate        | 0.000918     |\n",
      "|    loss                 | -0.000297    |\n",
      "|    n_updates            | 2430         |\n",
      "|    policy_gradient_loss | -0.000839    |\n",
      "|    std                  | 0.33         |\n",
      "|    value_loss           | 6.49e-11     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.000135 |\n",
      "| time/              |           |\n",
      "|    fps             | 596       |\n",
      "|    iterations      | 244       |\n",
      "|    time_elapsed    | 4125      |\n",
      "|    total_timesteps | 2459520   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2479680, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00359     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2479680      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032727232 |\n",
      "|    clip_fraction        | 0.0108       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 0.211        |\n",
      "|    explained_variance   | -4.04        |\n",
      "|    learning_rate        | 0.000884     |\n",
      "|    loss                 | -0.00262     |\n",
      "|    n_updates            | 2450         |\n",
      "|    policy_gradient_loss | -0.000838    |\n",
      "|    std                  | 0.327        |\n",
      "|    value_loss           | 1.9e-10      |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.000116 |\n",
      "| time/              |           |\n",
      "|    fps             | 596       |\n",
      "|    iterations      | 246       |\n",
      "|    time_elapsed    | 4155      |\n",
      "|    total_timesteps | 2479680   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2499840, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.00171    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2499840     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004910019 |\n",
      "|    clip_fraction        | 0.0202      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.228       |\n",
      "|    explained_variance   | -1.87       |\n",
      "|    learning_rate        | 0.00085     |\n",
      "|    loss                 | -0.00149    |\n",
      "|    n_updates            | 2470        |\n",
      "|    policy_gradient_loss | -0.0013     |\n",
      "|    std                  | 0.325       |\n",
      "|    value_loss           | 1.05e-10    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.000127 |\n",
      "| time/              |           |\n",
      "|    fps             | 597       |\n",
      "|    iterations      | 248       |\n",
      "|    time_elapsed    | 4186      |\n",
      "|    total_timesteps | 2499840   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2520000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00169     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2520000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032785046 |\n",
      "|    clip_fraction        | 0.0217       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 0.289        |\n",
      "|    explained_variance   | -0.344       |\n",
      "|    learning_rate        | 0.000817     |\n",
      "|    loss                 | -0.00111     |\n",
      "|    n_updates            | 2490         |\n",
      "|    policy_gradient_loss | -0.00101     |\n",
      "|    std                  | 0.314        |\n",
      "|    value_loss           | 2.51e-10     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.000144 |\n",
      "| time/              |           |\n",
      "|    fps             | 597       |\n",
      "|    iterations      | 250       |\n",
      "|    time_elapsed    | 4216      |\n",
      "|    total_timesteps | 2520000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2540160, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00148     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2540160      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027392842 |\n",
      "|    clip_fraction        | 0.0141       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 0.317        |\n",
      "|    explained_variance   | -0.367       |\n",
      "|    learning_rate        | 0.000783     |\n",
      "|    loss                 | -0.0035      |\n",
      "|    n_updates            | 2510         |\n",
      "|    policy_gradient_loss | -0.000798    |\n",
      "|    std                  | 0.311        |\n",
      "|    value_loss           | 4.79e-10     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.000159 |\n",
      "| time/              |           |\n",
      "|    fps             | 598       |\n",
      "|    iterations      | 252       |\n",
      "|    time_elapsed    | 4247      |\n",
      "|    total_timesteps | 2540160   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2560320, episode_reward=-0.00 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00373     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2560320      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022407456 |\n",
      "|    clip_fraction        | 0.00184      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 0.33         |\n",
      "|    explained_variance   | -3.56        |\n",
      "|    learning_rate        | 0.00075      |\n",
      "|    loss                 | -0.000879    |\n",
      "|    n_updates            | 2530         |\n",
      "|    policy_gradient_loss | -0.00022     |\n",
      "|    std                  | 0.308        |\n",
      "|    value_loss           | 8.47e-11     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.000142 |\n",
      "| time/              |           |\n",
      "|    fps             | 598       |\n",
      "|    iterations      | 254       |\n",
      "|    time_elapsed    | 4278      |\n",
      "|    total_timesteps | 2560320   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2580480, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00136     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2580480      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040605427 |\n",
      "|    clip_fraction        | 0.0181       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 0.337        |\n",
      "|    explained_variance   | -3.76        |\n",
      "|    learning_rate        | 0.000716     |\n",
      "|    loss                 | -0.000819    |\n",
      "|    n_updates            | 2550         |\n",
      "|    policy_gradient_loss | -0.00151     |\n",
      "|    std                  | 0.304        |\n",
      "|    value_loss           | 6.16e-11     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.000142 |\n",
      "| time/              |           |\n",
      "|    fps             | 598       |\n",
      "|    iterations      | 256       |\n",
      "|    time_elapsed    | 4308      |\n",
      "|    total_timesteps | 2580480   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2600640, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00377     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2600640      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032266877 |\n",
      "|    clip_fraction        | 0.00721      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 0.377        |\n",
      "|    explained_variance   | -0.824       |\n",
      "|    learning_rate        | 0.000682     |\n",
      "|    loss                 | -0.00141     |\n",
      "|    n_updates            | 2570         |\n",
      "|    policy_gradient_loss | -0.000702    |\n",
      "|    std                  | 0.298        |\n",
      "|    value_loss           | 1.13e-10     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.000104 |\n",
      "| time/              |           |\n",
      "|    fps             | 599       |\n",
      "|    iterations      | 258       |\n",
      "|    time_elapsed    | 4339      |\n",
      "|    total_timesteps | 2600640   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2620800, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00167     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2620800      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018626057 |\n",
      "|    clip_fraction        | 0.00369      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 0.384        |\n",
      "|    explained_variance   | -3.95        |\n",
      "|    learning_rate        | 0.000649     |\n",
      "|    loss                 | -0.00143     |\n",
      "|    n_updates            | 2590         |\n",
      "|    policy_gradient_loss | -0.000532    |\n",
      "|    std                  | 0.296        |\n",
      "|    value_loss           | 3.43e-11     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0001  |\n",
      "| time/              |          |\n",
      "|    fps             | 599      |\n",
      "|    iterations      | 260      |\n",
      "|    time_elapsed    | 4370     |\n",
      "|    total_timesteps | 2620800  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2640960, episode_reward=-0.00 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.00335    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2640960     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002915416 |\n",
      "|    clip_fraction        | 0.0039      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.394       |\n",
      "|    explained_variance   | -3.43       |\n",
      "|    learning_rate        | 0.000615    |\n",
      "|    loss                 | 0.000937    |\n",
      "|    n_updates            | 2610        |\n",
      "|    policy_gradient_loss | -0.000259   |\n",
      "|    std                  | 0.294       |\n",
      "|    value_loss           | 6.25e-11    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -8.36e-05 |\n",
      "| time/              |           |\n",
      "|    fps             | 600       |\n",
      "|    iterations      | 262       |\n",
      "|    time_elapsed    | 4401      |\n",
      "|    total_timesteps | 2640960   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2661120, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0014      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2661120      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026532293 |\n",
      "|    clip_fraction        | 0.00711      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 0.382        |\n",
      "|    explained_variance   | -3.26        |\n",
      "|    learning_rate        | 0.000582     |\n",
      "|    loss                 | 0.000127     |\n",
      "|    n_updates            | 2630         |\n",
      "|    policy_gradient_loss | -0.000534    |\n",
      "|    std                  | 0.298        |\n",
      "|    value_loss           | 5.29e-11     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -6.89e-05 |\n",
      "| time/              |           |\n",
      "|    fps             | 600       |\n",
      "|    iterations      | 264       |\n",
      "|    time_elapsed    | 4431      |\n",
      "|    total_timesteps | 2661120   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2681280, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.00312    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2681280     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005181947 |\n",
      "|    clip_fraction        | 0.0285      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.385       |\n",
      "|    explained_variance   | -0.386      |\n",
      "|    learning_rate        | 0.000548    |\n",
      "|    loss                 | 0.00119     |\n",
      "|    n_updates            | 2650        |\n",
      "|    policy_gradient_loss | -0.00142    |\n",
      "|    std                  | 0.296       |\n",
      "|    value_loss           | 2.25e-10    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -7.56e-05 |\n",
      "| time/              |           |\n",
      "|    fps             | 600       |\n",
      "|    iterations      | 266       |\n",
      "|    time_elapsed    | 4462      |\n",
      "|    total_timesteps | 2681280   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2701440, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.00163    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2701440     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002773678 |\n",
      "|    clip_fraction        | 0.0109      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.38        |\n",
      "|    explained_variance   | -3.75       |\n",
      "|    learning_rate        | 0.000514    |\n",
      "|    loss                 | -1.65e-05   |\n",
      "|    n_updates            | 2670        |\n",
      "|    policy_gradient_loss | -0.000866   |\n",
      "|    std                  | 0.296       |\n",
      "|    value_loss           | 5.85e-11    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -7.74e-05 |\n",
      "| time/              |           |\n",
      "|    fps             | 601       |\n",
      "|    iterations      | 268       |\n",
      "|    time_elapsed    | 4492      |\n",
      "|    total_timesteps | 2701440   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2721600, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00549     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2721600      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041463384 |\n",
      "|    clip_fraction        | 0.0227       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 0.38         |\n",
      "|    explained_variance   | -0.899       |\n",
      "|    learning_rate        | 0.000481     |\n",
      "|    loss                 | -0.00117     |\n",
      "|    n_updates            | 2690         |\n",
      "|    policy_gradient_loss | -0.00121     |\n",
      "|    std                  | 0.295        |\n",
      "|    value_loss           | 1.85e-10     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -7.71e-05 |\n",
      "| time/              |           |\n",
      "|    fps             | 601       |\n",
      "|    iterations      | 270       |\n",
      "|    time_elapsed    | 4523      |\n",
      "|    total_timesteps | 2721600   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2741760, episode_reward=-0.00 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00435     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2741760      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040361807 |\n",
      "|    clip_fraction        | 0.0175       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 0.384        |\n",
      "|    explained_variance   | -0.276       |\n",
      "|    learning_rate        | 0.000447     |\n",
      "|    loss                 | 0.0012       |\n",
      "|    n_updates            | 2710         |\n",
      "|    policy_gradient_loss | -0.000691    |\n",
      "|    std                  | 0.295        |\n",
      "|    value_loss           | 4.96e-10     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -9.52e-05 |\n",
      "| time/              |           |\n",
      "|    fps             | 602       |\n",
      "|    iterations      | 272       |\n",
      "|    time_elapsed    | 4554      |\n",
      "|    total_timesteps | 2741760   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2761920, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0022      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2761920      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042444523 |\n",
      "|    clip_fraction        | 0.0189       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 0.399        |\n",
      "|    explained_variance   | -3.9         |\n",
      "|    learning_rate        | 0.000414     |\n",
      "|    loss                 | -0.000999    |\n",
      "|    n_updates            | 2730         |\n",
      "|    policy_gradient_loss | -0.00133     |\n",
      "|    std                  | 0.29         |\n",
      "|    value_loss           | 4.1e-11      |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -9.49e-05 |\n",
      "| time/              |           |\n",
      "|    fps             | 602       |\n",
      "|    iterations      | 274       |\n",
      "|    time_elapsed    | 4584      |\n",
      "|    total_timesteps | 2761920   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2782080, episode_reward=-0.00 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.00358    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2782080     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003999088 |\n",
      "|    clip_fraction        | 0.008       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.41        |\n",
      "|    explained_variance   | -0.0162     |\n",
      "|    learning_rate        | 0.00038     |\n",
      "|    loss                 | -0.000487   |\n",
      "|    n_updates            | 2750        |\n",
      "|    policy_gradient_loss | -0.000349   |\n",
      "|    std                  | 0.293       |\n",
      "|    value_loss           | 3.48e-09    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.000127 |\n",
      "| time/              |           |\n",
      "|    fps             | 602       |\n",
      "|    iterations      | 276       |\n",
      "|    time_elapsed    | 4615      |\n",
      "|    total_timesteps | 2782080   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2802240, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.000294   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2802240     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004418593 |\n",
      "|    clip_fraction        | 0.0265      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.415       |\n",
      "|    explained_variance   | -0.143      |\n",
      "|    learning_rate        | 0.000346    |\n",
      "|    loss                 | -0.00191    |\n",
      "|    n_updates            | 2770        |\n",
      "|    policy_gradient_loss | -0.00104    |\n",
      "|    std                  | 0.294       |\n",
      "|    value_loss           | 5.68e-10    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.000126 |\n",
      "| time/              |           |\n",
      "|    fps             | 603       |\n",
      "|    iterations      | 278       |\n",
      "|    time_elapsed    | 4646      |\n",
      "|    total_timesteps | 2802240   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2822400, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00181     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2822400      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032780052 |\n",
      "|    clip_fraction        | 0.0143       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 0.431        |\n",
      "|    explained_variance   | -2.43        |\n",
      "|    learning_rate        | 0.000313     |\n",
      "|    loss                 | -0.000953    |\n",
      "|    n_updates            | 2790         |\n",
      "|    policy_gradient_loss | -0.0011      |\n",
      "|    std                  | 0.292        |\n",
      "|    value_loss           | 3.7e-11      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.00012 |\n",
      "| time/              |          |\n",
      "|    fps             | 603      |\n",
      "|    iterations      | 280      |\n",
      "|    time_elapsed    | 4677     |\n",
      "|    total_timesteps | 2822400  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2842560, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00178     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2842560      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032131106 |\n",
      "|    clip_fraction        | 0.0045       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 0.432        |\n",
      "|    explained_variance   | -0.476       |\n",
      "|    learning_rate        | 0.000279     |\n",
      "|    loss                 | -0.00207     |\n",
      "|    n_updates            | 2810         |\n",
      "|    policy_gradient_loss | -0.000296    |\n",
      "|    std                  | 0.292        |\n",
      "|    value_loss           | 1.06e-10     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.000119 |\n",
      "| time/              |           |\n",
      "|    fps             | 603       |\n",
      "|    iterations      | 282       |\n",
      "|    time_elapsed    | 4707      |\n",
      "|    total_timesteps | 2842560   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2862720, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.000767   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2862720     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003951927 |\n",
      "|    clip_fraction        | 0.0207      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.433       |\n",
      "|    explained_variance   | -0.165      |\n",
      "|    learning_rate        | 0.000246    |\n",
      "|    loss                 | -0.00195    |\n",
      "|    n_updates            | 2830        |\n",
      "|    policy_gradient_loss | -0.000749   |\n",
      "|    std                  | 0.293       |\n",
      "|    value_loss           | 4.38e-10    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.000103 |\n",
      "| time/              |           |\n",
      "|    fps             | 604       |\n",
      "|    iterations      | 284       |\n",
      "|    time_elapsed    | 4739      |\n",
      "|    total_timesteps | 2862720   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2882880, episode_reward=-0.00 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.00275    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2882880     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003552167 |\n",
      "|    clip_fraction        | 0.0144      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.421       |\n",
      "|    explained_variance   | -0.284      |\n",
      "|    learning_rate        | 0.000212    |\n",
      "|    loss                 | -0.000574   |\n",
      "|    n_updates            | 2850        |\n",
      "|    policy_gradient_loss | -0.000578   |\n",
      "|    std                  | 0.294       |\n",
      "|    value_loss           | 3.98e-10    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.000124 |\n",
      "| time/              |           |\n",
      "|    fps             | 604       |\n",
      "|    iterations      | 286       |\n",
      "|    time_elapsed    | 4770      |\n",
      "|    total_timesteps | 2882880   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2903040, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.000237   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2903040     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002580581 |\n",
      "|    clip_fraction        | 0.00116     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.421       |\n",
      "|    explained_variance   | -2.67       |\n",
      "|    learning_rate        | 0.000178    |\n",
      "|    loss                 | -0.000481   |\n",
      "|    n_updates            | 2870        |\n",
      "|    policy_gradient_loss | -0.000213   |\n",
      "|    std                  | 0.295       |\n",
      "|    value_loss           | 9.63e-11    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -8.58e-05 |\n",
      "| time/              |           |\n",
      "|    fps             | 604       |\n",
      "|    iterations      | 288       |\n",
      "|    time_elapsed    | 4801      |\n",
      "|    total_timesteps | 2903040   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2923200, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0051     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2923200     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004789712 |\n",
      "|    clip_fraction        | 0.0102      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.423       |\n",
      "|    explained_variance   | -0.11       |\n",
      "|    learning_rate        | 0.000145    |\n",
      "|    loss                 | -0.00339    |\n",
      "|    n_updates            | 2890        |\n",
      "|    policy_gradient_loss | -0.00039    |\n",
      "|    std                  | 0.294       |\n",
      "|    value_loss           | 4.63e-10    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -9.11e-05 |\n",
      "| time/              |           |\n",
      "|    fps             | 604       |\n",
      "|    iterations      | 290       |\n",
      "|    time_elapsed    | 4832      |\n",
      "|    total_timesteps | 2923200   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2943360, episode_reward=-0.00 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1.26e+03      |\n",
      "|    mean_reward          | -0.00434      |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 2943360       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00040039167 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | 0.421         |\n",
      "|    explained_variance   | -2.8          |\n",
      "|    learning_rate        | 0.000111      |\n",
      "|    loss                 | 1.19e-05      |\n",
      "|    n_updates            | 2910          |\n",
      "|    policy_gradient_loss | -3.5e-05      |\n",
      "|    std                  | 0.295         |\n",
      "|    value_loss           | 4.08e-11      |\n",
      "-------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -9.56e-05 |\n",
      "| time/              |           |\n",
      "|    fps             | 605       |\n",
      "|    iterations      | 292       |\n",
      "|    time_elapsed    | 4862      |\n",
      "|    total_timesteps | 2943360   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2963520, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.000434    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2963520      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043247975 |\n",
      "|    clip_fraction        | 0.00626      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 0.42         |\n",
      "|    explained_variance   | -4.09        |\n",
      "|    learning_rate        | 7.76e-05     |\n",
      "|    loss                 | -0.00103     |\n",
      "|    n_updates            | 2930         |\n",
      "|    policy_gradient_loss | -0.000858    |\n",
      "|    std                  | 0.295        |\n",
      "|    value_loss           | 5.22e-11     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -9.32e-05 |\n",
      "| time/              |           |\n",
      "|    fps             | 605       |\n",
      "|    iterations      | 294       |\n",
      "|    time_elapsed    | 4893      |\n",
      "|    total_timesteps | 2963520   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2983680, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00241     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2983680      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014252028 |\n",
      "|    clip_fraction        | 2.98e-05     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 0.419        |\n",
      "|    explained_variance   | -1.49        |\n",
      "|    learning_rate        | 4.4e-05      |\n",
      "|    loss                 | -0.00249     |\n",
      "|    n_updates            | 2950         |\n",
      "|    policy_gradient_loss | -0.000603    |\n",
      "|    std                  | 0.295        |\n",
      "|    value_loss           | 5.13e-11     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -8.79e-05 |\n",
      "| time/              |           |\n",
      "|    fps             | 605       |\n",
      "|    iterations      | 296       |\n",
      "|    time_elapsed    | 4924      |\n",
      "|    total_timesteps | 2983680   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3003840, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1.26e+03      |\n",
      "|    mean_reward          | -0.000122     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 3003840       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.2671964e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | 0.418         |\n",
      "|    explained_variance   | -1.92         |\n",
      "|    learning_rate        | 1.04e-05      |\n",
      "|    loss                 | -0.000332     |\n",
      "|    n_updates            | 2970          |\n",
      "|    policy_gradient_loss | -4.33e-05     |\n",
      "|    std                  | 0.295         |\n",
      "|    value_loss           | 4.43e-11      |\n",
      "-------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -7.84e-05 |\n",
      "| time/              |           |\n",
      "|    fps             | 606       |\n",
      "|    iterations      | 298       |\n",
      "|    time_elapsed    | 4955      |\n",
      "|    total_timesteps | 3003840   |\n",
      "----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x16e783e6a90>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reward l2\n",
    "envs = VecMonitor(DummyVecEnv([\n",
    "    lambda: tradingEng(paths1,action = 'small-More-Trust', obs = 'xs', reward = '2a'),\n",
    "    lambda: tradingEng(paths2,action = 'small-More-Trust', obs = 'xs', reward = '2a')\n",
    "]),filename='logsL2-train')\n",
    "\n",
    "ev_env = VecMonitor(DummyVecEnv([\n",
    "    lambda: tradingEng(paths_ev,action = 'small-More-Trust', obs = 'xs', reward = '2a'),\n",
    "]))\n",
    "\n",
    "eval_callback = EvalCallback(\n",
    "    ev_env,\n",
    "    best_model_save_path='./logs/best_modelL2',\n",
    "    log_path='./logs/eval_logsL2/ev',\n",
    "    eval_freq=252*8*5,\n",
    "    deterministic=True,\n",
    "    render=False,\n",
    "    verbose = True,\n",
    "    n_eval_episodes = 8\n",
    ")\n",
    "\n",
    "# Instantiate the agent\n",
    "policy_kwargs = dict(activation_fn=th.nn.LeakyReLU,\n",
    "                     net_arch=dict(pi=[512,512,256,128,64,64,64,64,36,18], vf=[512,512,256,128,64,64,64,64,36,18], optimizers_class = th.optim.Adam, log_std_init = 0.005)) #\n",
    "model = PPO(\"MlpPolicy\", envs, batch_size = 252*2*5, learning_rate=linear_schedule(0.005), policy_kwargs=policy_kwargs, n_steps=252*4*5, normalize_advantage=True, gamma = 0.9, verbose = 1) \n",
    "\n",
    "model.learn(total_timesteps=3e6, log_interval=2, callback=eval_callback) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.32       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 775         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 20160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006998197 |\n",
      "|    clip_fraction        | 0.0749      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -58.2       |\n",
      "|    explained_variance   | -1.8        |\n",
      "|    learning_rate        | 0.00249     |\n",
      "|    loss                 | -0.0105     |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00659    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 0.000731    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.329      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 707         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 56          |\n",
      "|    total_timesteps      | 40320       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008296808 |\n",
      "|    clip_fraction        | 0.0824      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -58.2       |\n",
      "|    explained_variance   | -0.00812    |\n",
      "|    learning_rate        | 0.00248     |\n",
      "|    loss                 | -0.00525    |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.005      |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 3.61e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=60480, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0142      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 60480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070884144 |\n",
      "|    clip_fraction        | 0.0834       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -58.1        |\n",
      "|    explained_variance   | 0.0251       |\n",
      "|    learning_rate        | 0.00247      |\n",
      "|    loss                 | -0.00582     |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00528     |\n",
      "|    std                  | 0.998        |\n",
      "|    value_loss           | 5.13e-07     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.317   |\n",
      "| time/              |          |\n",
      "|    fps             | 657      |\n",
      "|    iterations      | 6        |\n",
      "|    time_elapsed    | 92       |\n",
      "|    total_timesteps | 60480    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.312      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 668         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 120         |\n",
      "|    total_timesteps      | 80640       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009165736 |\n",
      "|    clip_fraction        | 0.0941      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -57.9       |\n",
      "|    explained_variance   | 0.0422      |\n",
      "|    learning_rate        | 0.00246     |\n",
      "|    loss                 | -0.00778    |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00566    |\n",
      "|    std                  | 0.994       |\n",
      "|    value_loss           | 1.09e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.315      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 676         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 148         |\n",
      "|    total_timesteps      | 100800      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010473416 |\n",
      "|    clip_fraction        | 0.109       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -57.9       |\n",
      "|    explained_variance   | 0.0276      |\n",
      "|    learning_rate        | 0.00245     |\n",
      "|    loss                 | -0.00586    |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00598    |\n",
      "|    std                  | 0.992       |\n",
      "|    value_loss           | 3e-06       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=120960, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0244     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 120960      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009363139 |\n",
      "|    clip_fraction        | 0.106       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -57.8       |\n",
      "|    explained_variance   | 0.0391      |\n",
      "|    learning_rate        | 0.00244     |\n",
      "|    loss                 | -0.00749    |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.00645    |\n",
      "|    std                  | 0.991       |\n",
      "|    value_loss           | 7.22e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.31    |\n",
      "| time/              |          |\n",
      "|    fps             | 618      |\n",
      "|    iterations      | 12       |\n",
      "|    time_elapsed    | 195      |\n",
      "|    total_timesteps | 120960   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.26e+03   |\n",
      "|    ep_rew_mean          | -0.309     |\n",
      "| time/                   |            |\n",
      "|    fps                  | 634        |\n",
      "|    iterations           | 14         |\n",
      "|    time_elapsed         | 222        |\n",
      "|    total_timesteps      | 141120     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01013007 |\n",
      "|    clip_fraction        | 0.115      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -57.7      |\n",
      "|    explained_variance   | 0.034      |\n",
      "|    learning_rate        | 0.00243    |\n",
      "|    loss                 | -0.00789   |\n",
      "|    n_updates            | 130        |\n",
      "|    policy_gradient_loss | -0.00636   |\n",
      "|    std                  | 0.989      |\n",
      "|    value_loss           | 9.29e-07   |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.26e+03     |\n",
      "|    ep_rew_mean          | -0.301       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 649          |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 248          |\n",
      "|    total_timesteps      | 161280       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0083592525 |\n",
      "|    clip_fraction        | 0.106        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -57.6        |\n",
      "|    explained_variance   | 0.06         |\n",
      "|    learning_rate        | 0.00242      |\n",
      "|    loss                 | -0.00929     |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.00657     |\n",
      "|    std                  | 0.985        |\n",
      "|    value_loss           | 6.59e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=181440, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0153     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 181440      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010916104 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -57.2       |\n",
      "|    explained_variance   | 0.101       |\n",
      "|    learning_rate        | 0.00241     |\n",
      "|    loss                 | -0.00613    |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.00603    |\n",
      "|    std                  | 0.977       |\n",
      "|    value_loss           | 3.06e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.304   |\n",
      "| time/              |          |\n",
      "|    fps             | 643      |\n",
      "|    iterations      | 18       |\n",
      "|    time_elapsed    | 281      |\n",
      "|    total_timesteps | 181440   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.26e+03     |\n",
      "|    ep_rew_mean          | -0.309       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 653          |\n",
      "|    iterations           | 20           |\n",
      "|    time_elapsed         | 308          |\n",
      "|    total_timesteps      | 201600       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073947436 |\n",
      "|    clip_fraction        | 0.0822       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -57.1        |\n",
      "|    explained_variance   | 0.0648       |\n",
      "|    learning_rate        | 0.0024       |\n",
      "|    loss                 | -0.00983     |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.00539     |\n",
      "|    std                  | 0.974        |\n",
      "|    value_loss           | 8.7e-07      |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.298      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 661         |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 335         |\n",
      "|    total_timesteps      | 221760      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008181999 |\n",
      "|    clip_fraction        | 0.0995      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -57.2       |\n",
      "|    explained_variance   | 0.143       |\n",
      "|    learning_rate        | 0.00239     |\n",
      "|    loss                 | -0.00789    |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.00675    |\n",
      "|    std                  | 0.977       |\n",
      "|    value_loss           | 7.71e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=241920, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0165    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 241920     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00849774 |\n",
      "|    clip_fraction        | 0.0953     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -57.1      |\n",
      "|    explained_variance   | 0.0913     |\n",
      "|    learning_rate        | 0.00238    |\n",
      "|    loss                 | -0.0088    |\n",
      "|    n_updates            | 230        |\n",
      "|    policy_gradient_loss | -0.00576   |\n",
      "|    std                  | 0.974      |\n",
      "|    value_loss           | 8.33e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.301   |\n",
      "| time/              |          |\n",
      "|    fps             | 654      |\n",
      "|    iterations      | 24       |\n",
      "|    time_elapsed    | 369      |\n",
      "|    total_timesteps | 241920   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.299      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 662         |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 395         |\n",
      "|    total_timesteps      | 262080      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008815855 |\n",
      "|    clip_fraction        | 0.09        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -56.9       |\n",
      "|    explained_variance   | 0.095       |\n",
      "|    learning_rate        | 0.00237     |\n",
      "|    loss                 | -0.00904    |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.00591    |\n",
      "|    std                  | 0.97        |\n",
      "|    value_loss           | 1e-06       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.297      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 665         |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 424         |\n",
      "|    total_timesteps      | 282240      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010480848 |\n",
      "|    clip_fraction        | 0.0998      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -56.8       |\n",
      "|    explained_variance   | 0.0788      |\n",
      "|    learning_rate        | 0.00236     |\n",
      "|    loss                 | -0.00425    |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.00485    |\n",
      "|    std                  | 0.967       |\n",
      "|    value_loss           | 1.83e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=302400, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.016      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 302400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009643872 |\n",
      "|    clip_fraction        | 0.119       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -56.6       |\n",
      "|    explained_variance   | 0.154       |\n",
      "|    learning_rate        | 0.00235     |\n",
      "|    loss                 | -0.0114     |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.00727    |\n",
      "|    std                  | 0.963       |\n",
      "|    value_loss           | 1.13e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.292   |\n",
      "| time/              |          |\n",
      "|    fps             | 653      |\n",
      "|    iterations      | 30       |\n",
      "|    time_elapsed    | 462      |\n",
      "|    total_timesteps | 302400   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.291      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 655         |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 492         |\n",
      "|    total_timesteps      | 322560      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010554515 |\n",
      "|    clip_fraction        | 0.111       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -56.3       |\n",
      "|    explained_variance   | 0.103       |\n",
      "|    learning_rate        | 0.00234     |\n",
      "|    loss                 | -0.00877    |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.0065     |\n",
      "|    std                  | 0.958       |\n",
      "|    value_loss           | 1.7e-06     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.289      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 661         |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 518         |\n",
      "|    total_timesteps      | 342720      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007825473 |\n",
      "|    clip_fraction        | 0.0961      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -56.2       |\n",
      "|    explained_variance   | 0.0793      |\n",
      "|    learning_rate        | 0.00233     |\n",
      "|    loss                 | -0.00856    |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.0063     |\n",
      "|    std                  | 0.956       |\n",
      "|    value_loss           | 6.79e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=362880, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0177     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 362880      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008459546 |\n",
      "|    clip_fraction        | 0.0998      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -55.9       |\n",
      "|    explained_variance   | 0.158       |\n",
      "|    learning_rate        | 0.00232     |\n",
      "|    loss                 | -0.00698    |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.00633    |\n",
      "|    std                  | 0.949       |\n",
      "|    value_loss           | 8.48e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.292   |\n",
      "| time/              |          |\n",
      "|    fps             | 652      |\n",
      "|    iterations      | 36       |\n",
      "|    time_elapsed    | 555      |\n",
      "|    total_timesteps | 362880   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.294      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 653         |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 586         |\n",
      "|    total_timesteps      | 383040      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010879811 |\n",
      "|    clip_fraction        | 0.111       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -55.7       |\n",
      "|    explained_variance   | 0.0663      |\n",
      "|    learning_rate        | 0.00231     |\n",
      "|    loss                 | -0.00611    |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.00613    |\n",
      "|    std                  | 0.945       |\n",
      "|    value_loss           | 3.76e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.295      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 653         |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 616         |\n",
      "|    total_timesteps      | 403200      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009988464 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -55.4       |\n",
      "|    explained_variance   | 0.0428      |\n",
      "|    learning_rate        | 0.0023      |\n",
      "|    loss                 | -0.0124     |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.00768    |\n",
      "|    std                  | 0.937       |\n",
      "|    value_loss           | 1.05e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=423360, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0187     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 423360      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010662951 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -55.1       |\n",
      "|    explained_variance   | 0.138       |\n",
      "|    learning_rate        | 0.00229     |\n",
      "|    loss                 | -0.00342    |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.00608    |\n",
      "|    std                  | 0.93        |\n",
      "|    value_loss           | 1.88e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.293   |\n",
      "| time/              |          |\n",
      "|    fps             | 650      |\n",
      "|    iterations      | 42       |\n",
      "|    time_elapsed    | 650      |\n",
      "|    total_timesteps | 423360   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.295      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 655         |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 676         |\n",
      "|    total_timesteps      | 443520      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011867122 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -55         |\n",
      "|    explained_variance   | 0.0845      |\n",
      "|    learning_rate        | 0.00228     |\n",
      "|    loss                 | -0.0097     |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.00628    |\n",
      "|    std                  | 0.931       |\n",
      "|    value_loss           | 3.7e-06     |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1.26e+03  |\n",
      "|    ep_rew_mean          | -0.297    |\n",
      "| time/                   |           |\n",
      "|    fps                  | 661       |\n",
      "|    iterations           | 46        |\n",
      "|    time_elapsed         | 701       |\n",
      "|    total_timesteps      | 463680    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0086994 |\n",
      "|    clip_fraction        | 0.1       |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -54.9     |\n",
      "|    explained_variance   | 0.0628    |\n",
      "|    learning_rate        | 0.00227   |\n",
      "|    loss                 | -0.0102   |\n",
      "|    n_updates            | 450       |\n",
      "|    policy_gradient_loss | -0.00654  |\n",
      "|    std                  | 0.927     |\n",
      "|    value_loss           | 1.87e-06  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=483840, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0211     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 483840      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010169505 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -54.7       |\n",
      "|    explained_variance   | 0.102       |\n",
      "|    learning_rate        | 0.00226     |\n",
      "|    loss                 | -0.0104     |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.00614    |\n",
      "|    std                  | 0.921       |\n",
      "|    value_loss           | 2.09e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.292   |\n",
      "| time/              |          |\n",
      "|    fps             | 659      |\n",
      "|    iterations      | 48       |\n",
      "|    time_elapsed    | 733      |\n",
      "|    total_timesteps | 483840   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.288      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 663         |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 759         |\n",
      "|    total_timesteps      | 504000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007402865 |\n",
      "|    clip_fraction        | 0.0705      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -54.5       |\n",
      "|    explained_variance   | 0.222       |\n",
      "|    learning_rate        | 0.00225     |\n",
      "|    loss                 | -0.00171    |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | -0.00413    |\n",
      "|    std                  | 0.919       |\n",
      "|    value_loss           | 5.36e-07    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.292      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 667         |\n",
      "|    iterations           | 52          |\n",
      "|    time_elapsed         | 785         |\n",
      "|    total_timesteps      | 524160      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010660037 |\n",
      "|    clip_fraction        | 0.107       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -54.3       |\n",
      "|    explained_variance   | 0.123       |\n",
      "|    learning_rate        | 0.00224     |\n",
      "|    loss                 | -0.00719    |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.00572    |\n",
      "|    std                  | 0.916       |\n",
      "|    value_loss           | 1.51e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=544320, episode_reward=-0.03 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0289     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 544320      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009928554 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -54.3       |\n",
      "|    explained_variance   | 0.115       |\n",
      "|    learning_rate        | 0.00223     |\n",
      "|    loss                 | -0.00975    |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | -0.00707    |\n",
      "|    std                  | 0.914       |\n",
      "|    value_loss           | 1.33e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.29    |\n",
      "| time/              |          |\n",
      "|    fps             | 666      |\n",
      "|    iterations      | 54       |\n",
      "|    time_elapsed    | 817      |\n",
      "|    total_timesteps | 544320   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.291      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 669         |\n",
      "|    iterations           | 56          |\n",
      "|    time_elapsed         | 842         |\n",
      "|    total_timesteps      | 564480      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008394399 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -54         |\n",
      "|    explained_variance   | 0.115       |\n",
      "|    learning_rate        | 0.00222     |\n",
      "|    loss                 | -0.00981    |\n",
      "|    n_updates            | 550         |\n",
      "|    policy_gradient_loss | -0.00659    |\n",
      "|    std                  | 0.909       |\n",
      "|    value_loss           | 6.14e-07    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.29       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 671         |\n",
      "|    iterations           | 58          |\n",
      "|    time_elapsed         | 870         |\n",
      "|    total_timesteps      | 584640      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007521634 |\n",
      "|    clip_fraction        | 0.0998      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -53.7       |\n",
      "|    explained_variance   | 0.254       |\n",
      "|    learning_rate        | 0.00221     |\n",
      "|    loss                 | -0.00795    |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | -0.0069     |\n",
      "|    std                  | 0.902       |\n",
      "|    value_loss           | 5.04e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=604800, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0188      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 604800       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077105556 |\n",
      "|    clip_fraction        | 0.0861       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -53.5        |\n",
      "|    explained_variance   | 0.262        |\n",
      "|    learning_rate        | 0.0022       |\n",
      "|    loss                 | -0.00459     |\n",
      "|    n_updates            | 590          |\n",
      "|    policy_gradient_loss | -0.00509     |\n",
      "|    std                  | 0.9          |\n",
      "|    value_loss           | 4.86e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.285   |\n",
      "| time/              |          |\n",
      "|    fps             | 668      |\n",
      "|    iterations      | 60       |\n",
      "|    time_elapsed    | 904      |\n",
      "|    total_timesteps | 604800   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.26e+03     |\n",
      "|    ep_rew_mean          | -0.286       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 672          |\n",
      "|    iterations           | 62           |\n",
      "|    time_elapsed         | 929          |\n",
      "|    total_timesteps      | 624960       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0110419765 |\n",
      "|    clip_fraction        | 0.108        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -53.3        |\n",
      "|    explained_variance   | 0.0381       |\n",
      "|    learning_rate        | 0.00219      |\n",
      "|    loss                 | -0.00843     |\n",
      "|    n_updates            | 610          |\n",
      "|    policy_gradient_loss | -0.00574     |\n",
      "|    std                  | 0.895        |\n",
      "|    value_loss           | 4.86e-06     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.281      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 676         |\n",
      "|    iterations           | 64          |\n",
      "|    time_elapsed         | 953         |\n",
      "|    total_timesteps      | 645120      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012038242 |\n",
      "|    clip_fraction        | 0.116       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -53.1       |\n",
      "|    explained_variance   | 0.0666      |\n",
      "|    learning_rate        | 0.00218     |\n",
      "|    loss                 | -0.00582    |\n",
      "|    n_updates            | 630         |\n",
      "|    policy_gradient_loss | -0.00597    |\n",
      "|    std                  | 0.89        |\n",
      "|    value_loss           | 4.87e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=665280, episode_reward=-0.03 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0253    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 665280     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00887993 |\n",
      "|    clip_fraction        | 0.101      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -52.9      |\n",
      "|    explained_variance   | 0.158      |\n",
      "|    learning_rate        | 0.00217    |\n",
      "|    loss                 | -0.00784   |\n",
      "|    n_updates            | 650        |\n",
      "|    policy_gradient_loss | -0.00547   |\n",
      "|    std                  | 0.888      |\n",
      "|    value_loss           | 9.8e-07    |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.279   |\n",
      "| time/              |          |\n",
      "|    fps             | 675      |\n",
      "|    iterations      | 66       |\n",
      "|    time_elapsed    | 984      |\n",
      "|    total_timesteps | 665280   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.277      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 679         |\n",
      "|    iterations           | 68          |\n",
      "|    time_elapsed         | 1008        |\n",
      "|    total_timesteps      | 685440      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011511339 |\n",
      "|    clip_fraction        | 0.11        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -52.8       |\n",
      "|    explained_variance   | 0.0774      |\n",
      "|    learning_rate        | 0.00216     |\n",
      "|    loss                 | -0.0088     |\n",
      "|    n_updates            | 670         |\n",
      "|    policy_gradient_loss | -0.00593    |\n",
      "|    std                  | 0.886       |\n",
      "|    value_loss           | 2.66e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.275      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 683         |\n",
      "|    iterations           | 70          |\n",
      "|    time_elapsed         | 1032        |\n",
      "|    total_timesteps      | 705600      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008923751 |\n",
      "|    clip_fraction        | 0.0842      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -52.7       |\n",
      "|    explained_variance   | 0.0357      |\n",
      "|    learning_rate        | 0.00215     |\n",
      "|    loss                 | -0.00613    |\n",
      "|    n_updates            | 690         |\n",
      "|    policy_gradient_loss | -0.00497    |\n",
      "|    std                  | 0.882       |\n",
      "|    value_loss           | 2.71e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=725760, episode_reward=-0.03 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0253     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 725760      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009570578 |\n",
      "|    clip_fraction        | 0.112       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -52.4       |\n",
      "|    explained_variance   | 0.0954      |\n",
      "|    learning_rate        | 0.00214     |\n",
      "|    loss                 | -0.00896    |\n",
      "|    n_updates            | 710         |\n",
      "|    policy_gradient_loss | -0.00654    |\n",
      "|    std                  | 0.878       |\n",
      "|    value_loss           | 1.57e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.279   |\n",
      "| time/              |          |\n",
      "|    fps             | 681      |\n",
      "|    iterations      | 72       |\n",
      "|    time_elapsed    | 1064     |\n",
      "|    total_timesteps | 725760   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.26e+03     |\n",
      "|    ep_rew_mean          | -0.28        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 684          |\n",
      "|    iterations           | 74           |\n",
      "|    time_elapsed         | 1088         |\n",
      "|    total_timesteps      | 745920       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0103770085 |\n",
      "|    clip_fraction        | 0.113        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -52.2        |\n",
      "|    explained_variance   | 0.132        |\n",
      "|    learning_rate        | 0.00213      |\n",
      "|    loss                 | -0.00516     |\n",
      "|    n_updates            | 730          |\n",
      "|    policy_gradient_loss | -0.00644     |\n",
      "|    std                  | 0.875        |\n",
      "|    value_loss           | 1.23e-06     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.275      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 687         |\n",
      "|    iterations           | 76          |\n",
      "|    time_elapsed         | 1113        |\n",
      "|    total_timesteps      | 766080      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008312521 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -52         |\n",
      "|    explained_variance   | 0.247       |\n",
      "|    learning_rate        | 0.00212     |\n",
      "|    loss                 | -0.0124     |\n",
      "|    n_updates            | 750         |\n",
      "|    policy_gradient_loss | -0.00719    |\n",
      "|    std                  | 0.87        |\n",
      "|    value_loss           | 5.4e-07     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=786240, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0218     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 786240      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009585024 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -51.8       |\n",
      "|    explained_variance   | 0.104       |\n",
      "|    learning_rate        | 0.00211     |\n",
      "|    loss                 | -0.0079     |\n",
      "|    n_updates            | 770         |\n",
      "|    policy_gradient_loss | -0.00662    |\n",
      "|    std                  | 0.867       |\n",
      "|    value_loss           | 1.16e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.272   |\n",
      "| time/              |          |\n",
      "|    fps             | 687      |\n",
      "|    iterations      | 78       |\n",
      "|    time_elapsed    | 1144     |\n",
      "|    total_timesteps | 786240   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.277      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 690         |\n",
      "|    iterations           | 80          |\n",
      "|    time_elapsed         | 1168        |\n",
      "|    total_timesteps      | 806400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012366576 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -51.6       |\n",
      "|    explained_variance   | 0.0668      |\n",
      "|    learning_rate        | 0.0021      |\n",
      "|    loss                 | -0.0081     |\n",
      "|    n_updates            | 790         |\n",
      "|    policy_gradient_loss | -0.00526    |\n",
      "|    std                  | 0.863       |\n",
      "|    value_loss           | 1.4e-05     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.278      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 693         |\n",
      "|    iterations           | 82          |\n",
      "|    time_elapsed         | 1192        |\n",
      "|    total_timesteps      | 826560      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009554297 |\n",
      "|    clip_fraction        | 0.111       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -51.3       |\n",
      "|    explained_variance   | 0.199       |\n",
      "|    learning_rate        | 0.00209     |\n",
      "|    loss                 | -0.00569    |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | -0.00629    |\n",
      "|    std                  | 0.858       |\n",
      "|    value_loss           | 9.64e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=846720, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0198     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 846720      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009987088 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -51         |\n",
      "|    explained_variance   | 0.0902      |\n",
      "|    learning_rate        | 0.00208     |\n",
      "|    loss                 | -0.00607    |\n",
      "|    n_updates            | 830         |\n",
      "|    policy_gradient_loss | -0.00579    |\n",
      "|    std                  | 0.853       |\n",
      "|    value_loss           | 1.09e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.274   |\n",
      "| time/              |          |\n",
      "|    fps             | 691      |\n",
      "|    iterations      | 84       |\n",
      "|    time_elapsed    | 1223     |\n",
      "|    total_timesteps | 846720   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.271      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 694         |\n",
      "|    iterations           | 86          |\n",
      "|    time_elapsed         | 1247        |\n",
      "|    total_timesteps      | 866880      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010047618 |\n",
      "|    clip_fraction        | 0.1         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -50.8       |\n",
      "|    explained_variance   | 0.157       |\n",
      "|    learning_rate        | 0.00207     |\n",
      "|    loss                 | -0.0103     |\n",
      "|    n_updates            | 850         |\n",
      "|    policy_gradient_loss | -0.00552    |\n",
      "|    std                  | 0.848       |\n",
      "|    value_loss           | 1.15e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.269      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 697         |\n",
      "|    iterations           | 88          |\n",
      "|    time_elapsed         | 1271        |\n",
      "|    total_timesteps      | 887040      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008952655 |\n",
      "|    clip_fraction        | 0.11        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -50.5       |\n",
      "|    explained_variance   | 0.166       |\n",
      "|    learning_rate        | 0.00206     |\n",
      "|    loss                 | -0.00915    |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | -0.00705    |\n",
      "|    std                  | 0.844       |\n",
      "|    value_loss           | 6.14e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=907200, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0244    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 907200     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00931658 |\n",
      "|    clip_fraction        | 0.11       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -50.3      |\n",
      "|    explained_variance   | 0.316      |\n",
      "|    learning_rate        | 0.00205    |\n",
      "|    loss                 | -0.0084    |\n",
      "|    n_updates            | 890        |\n",
      "|    policy_gradient_loss | -0.00666   |\n",
      "|    std                  | 0.842      |\n",
      "|    value_loss           | 6.86e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.27    |\n",
      "| time/              |          |\n",
      "|    fps             | 696      |\n",
      "|    iterations      | 90       |\n",
      "|    time_elapsed    | 1302     |\n",
      "|    total_timesteps | 907200   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.258      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 698         |\n",
      "|    iterations           | 92          |\n",
      "|    time_elapsed         | 1327        |\n",
      "|    total_timesteps      | 927360      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011800941 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -50         |\n",
      "|    explained_variance   | 0.111       |\n",
      "|    learning_rate        | 0.00204     |\n",
      "|    loss                 | -0.00757    |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | -0.00631    |\n",
      "|    std                  | 0.836       |\n",
      "|    value_loss           | 2.27e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.257      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 701         |\n",
      "|    iterations           | 94          |\n",
      "|    time_elapsed         | 1351        |\n",
      "|    total_timesteps      | 947520      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012546955 |\n",
      "|    clip_fraction        | 0.125       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -49.8       |\n",
      "|    explained_variance   | 0.104       |\n",
      "|    learning_rate        | 0.00203     |\n",
      "|    loss                 | -0.0041     |\n",
      "|    n_updates            | 930         |\n",
      "|    policy_gradient_loss | -0.00606    |\n",
      "|    std                  | 0.832       |\n",
      "|    value_loss           | 3.13e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=967680, episode_reward=-0.03 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0261    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 967680     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00853723 |\n",
      "|    clip_fraction        | 0.0948     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -49.6      |\n",
      "|    explained_variance   | 0.285      |\n",
      "|    learning_rate        | 0.00202    |\n",
      "|    loss                 | -0.00823   |\n",
      "|    n_updates            | 950        |\n",
      "|    policy_gradient_loss | -0.00588   |\n",
      "|    std                  | 0.828      |\n",
      "|    value_loss           | 5.01e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.261   |\n",
      "| time/              |          |\n",
      "|    fps             | 700      |\n",
      "|    iterations      | 96       |\n",
      "|    time_elapsed    | 1382     |\n",
      "|    total_timesteps | 967680   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1.26e+03  |\n",
      "|    ep_rew_mean          | -0.264    |\n",
      "| time/                   |           |\n",
      "|    fps                  | 702       |\n",
      "|    iterations           | 98        |\n",
      "|    time_elapsed         | 1406      |\n",
      "|    total_timesteps      | 987840    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0146726 |\n",
      "|    clip_fraction        | 0.122     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -49.3     |\n",
      "|    explained_variance   | 0.0638    |\n",
      "|    learning_rate        | 0.00201   |\n",
      "|    loss                 | -0.00966  |\n",
      "|    n_updates            | 970       |\n",
      "|    policy_gradient_loss | -0.00592  |\n",
      "|    std                  | 0.822     |\n",
      "|    value_loss           | 4.87e-06  |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.26e+03   |\n",
      "|    ep_rew_mean          | -0.265     |\n",
      "| time/                   |            |\n",
      "|    fps                  | 704        |\n",
      "|    iterations           | 100        |\n",
      "|    time_elapsed         | 1430       |\n",
      "|    total_timesteps      | 1008000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00910434 |\n",
      "|    clip_fraction        | 0.112      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -49.1      |\n",
      "|    explained_variance   | 0.211      |\n",
      "|    learning_rate        | 0.002      |\n",
      "|    loss                 | -0.00753   |\n",
      "|    n_updates            | 990        |\n",
      "|    policy_gradient_loss | -0.007     |\n",
      "|    std                  | 0.819      |\n",
      "|    value_loss           | 5.66e-07   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=1028160, episode_reward=-0.03 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0301     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1028160     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008512144 |\n",
      "|    clip_fraction        | 0.095       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -48.8       |\n",
      "|    explained_variance   | 0.217       |\n",
      "|    learning_rate        | 0.00199     |\n",
      "|    loss                 | -0.00772    |\n",
      "|    n_updates            | 1010        |\n",
      "|    policy_gradient_loss | -0.00633    |\n",
      "|    std                  | 0.814       |\n",
      "|    value_loss           | 4.62e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.266   |\n",
      "| time/              |          |\n",
      "|    fps             | 703      |\n",
      "|    iterations      | 102      |\n",
      "|    time_elapsed    | 1461     |\n",
      "|    total_timesteps | 1028160  |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.26e+03     |\n",
      "|    ep_rew_mean          | -0.262       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 705          |\n",
      "|    iterations           | 104          |\n",
      "|    time_elapsed         | 1486         |\n",
      "|    total_timesteps      | 1048320      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0096650515 |\n",
      "|    clip_fraction        | 0.115        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -48.5        |\n",
      "|    explained_variance   | 0.128        |\n",
      "|    learning_rate        | 0.00198      |\n",
      "|    loss                 | -0.0093      |\n",
      "|    n_updates            | 1030         |\n",
      "|    policy_gradient_loss | -0.00758     |\n",
      "|    std                  | 0.808        |\n",
      "|    value_loss           | 5.61e-07     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.26e+03     |\n",
      "|    ep_rew_mean          | -0.262       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 707          |\n",
      "|    iterations           | 106          |\n",
      "|    time_elapsed         | 1510         |\n",
      "|    total_timesteps      | 1068480      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077837626 |\n",
      "|    clip_fraction        | 0.0922       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -48.2        |\n",
      "|    explained_variance   | 0.13         |\n",
      "|    learning_rate        | 0.00197      |\n",
      "|    loss                 | -0.00984     |\n",
      "|    n_updates            | 1050         |\n",
      "|    policy_gradient_loss | -0.00624     |\n",
      "|    std                  | 0.804        |\n",
      "|    value_loss           | 2.69e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1088640, episode_reward=-0.03 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.031      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1088640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011257531 |\n",
      "|    clip_fraction        | 0.119       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -48         |\n",
      "|    explained_variance   | 0.147       |\n",
      "|    learning_rate        | 0.00196     |\n",
      "|    loss                 | -0.0102     |\n",
      "|    n_updates            | 1070        |\n",
      "|    policy_gradient_loss | -0.00644    |\n",
      "|    std                  | 0.802       |\n",
      "|    value_loss           | 1.15e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.256   |\n",
      "| time/              |          |\n",
      "|    fps             | 706      |\n",
      "|    iterations      | 108      |\n",
      "|    time_elapsed    | 1540     |\n",
      "|    total_timesteps | 1088640  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.248      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 708         |\n",
      "|    iterations           | 110         |\n",
      "|    time_elapsed         | 1565        |\n",
      "|    total_timesteps      | 1108800     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010954494 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -47.8       |\n",
      "|    explained_variance   | 0.0576      |\n",
      "|    learning_rate        | 0.00195     |\n",
      "|    loss                 | -0.00683    |\n",
      "|    n_updates            | 1090        |\n",
      "|    policy_gradient_loss | -0.00498    |\n",
      "|    std                  | 0.797       |\n",
      "|    value_loss           | 6.5e-06     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.242      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 710         |\n",
      "|    iterations           | 112         |\n",
      "|    time_elapsed         | 1589        |\n",
      "|    total_timesteps      | 1128960     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009789551 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -47.4       |\n",
      "|    explained_variance   | 0.198       |\n",
      "|    learning_rate        | 0.00194     |\n",
      "|    loss                 | -0.0102     |\n",
      "|    n_updates            | 1110        |\n",
      "|    policy_gradient_loss | -0.00711    |\n",
      "|    std                  | 0.791       |\n",
      "|    value_loss           | 8.41e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1149120, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0232     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1149120     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007493152 |\n",
      "|    clip_fraction        | 0.088       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -47.3       |\n",
      "|    explained_variance   | 0.23        |\n",
      "|    learning_rate        | 0.00193     |\n",
      "|    loss                 | -0.00581    |\n",
      "|    n_updates            | 1130        |\n",
      "|    policy_gradient_loss | -0.00581    |\n",
      "|    std                  | 0.79        |\n",
      "|    value_loss           | 2.43e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.239   |\n",
      "| time/              |          |\n",
      "|    fps             | 709      |\n",
      "|    iterations      | 114      |\n",
      "|    time_elapsed    | 1619     |\n",
      "|    total_timesteps | 1149120  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.243      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 711         |\n",
      "|    iterations           | 116         |\n",
      "|    time_elapsed         | 1644        |\n",
      "|    total_timesteps      | 1169280     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010666039 |\n",
      "|    clip_fraction        | 0.0995      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -47.1       |\n",
      "|    explained_variance   | 0.276       |\n",
      "|    learning_rate        | 0.00192     |\n",
      "|    loss                 | -0.00871    |\n",
      "|    n_updates            | 1150        |\n",
      "|    policy_gradient_loss | -0.00559    |\n",
      "|    std                  | 0.788       |\n",
      "|    value_loss           | 9.37e-07    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.239      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 712         |\n",
      "|    iterations           | 118         |\n",
      "|    time_elapsed         | 1668        |\n",
      "|    total_timesteps      | 1189440     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007897969 |\n",
      "|    clip_fraction        | 0.0894      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -46.7       |\n",
      "|    explained_variance   | 0.179       |\n",
      "|    learning_rate        | 0.00191     |\n",
      "|    loss                 | -0.00773    |\n",
      "|    n_updates            | 1170        |\n",
      "|    policy_gradient_loss | -0.00544    |\n",
      "|    std                  | 0.782       |\n",
      "|    value_loss           | 3.16e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1209600, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0159      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1209600      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076791435 |\n",
      "|    clip_fraction        | 0.0926       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -46.4        |\n",
      "|    explained_variance   | 0.376        |\n",
      "|    learning_rate        | 0.0019       |\n",
      "|    loss                 | -0.0062      |\n",
      "|    n_updates            | 1190         |\n",
      "|    policy_gradient_loss | -0.00612     |\n",
      "|    std                  | 0.775        |\n",
      "|    value_loss           | 3.42e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.235   |\n",
      "| time/              |          |\n",
      "|    fps             | 711      |\n",
      "|    iterations      | 120      |\n",
      "|    time_elapsed    | 1699     |\n",
      "|    total_timesteps | 1209600  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.232      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 713         |\n",
      "|    iterations           | 122         |\n",
      "|    time_elapsed         | 1723        |\n",
      "|    total_timesteps      | 1229760     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008006172 |\n",
      "|    clip_fraction        | 0.0869      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -46         |\n",
      "|    explained_variance   | 0.228       |\n",
      "|    learning_rate        | 0.00189     |\n",
      "|    loss                 | -0.00899    |\n",
      "|    n_updates            | 1210        |\n",
      "|    policy_gradient_loss | -0.00526    |\n",
      "|    std                  | 0.769       |\n",
      "|    value_loss           | 8.63e-07    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.229      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 715         |\n",
      "|    iterations           | 124         |\n",
      "|    time_elapsed         | 1747        |\n",
      "|    total_timesteps      | 1249920     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009905247 |\n",
      "|    clip_fraction        | 0.11        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.7       |\n",
      "|    explained_variance   | 0.247       |\n",
      "|    learning_rate        | 0.00188     |\n",
      "|    loss                 | -0.009      |\n",
      "|    n_updates            | 1230        |\n",
      "|    policy_gradient_loss | -0.00671    |\n",
      "|    std                  | 0.763       |\n",
      "|    value_loss           | 2.99e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1270080, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0189     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1270080     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006755858 |\n",
      "|    clip_fraction        | 0.073       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.3       |\n",
      "|    explained_variance   | 0.407       |\n",
      "|    learning_rate        | 0.00187     |\n",
      "|    loss                 | -0.00789    |\n",
      "|    n_updates            | 1250        |\n",
      "|    policy_gradient_loss | -0.00503    |\n",
      "|    std                  | 0.758       |\n",
      "|    value_loss           | 1.54e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.227   |\n",
      "| time/              |          |\n",
      "|    fps             | 714      |\n",
      "|    iterations      | 126      |\n",
      "|    time_elapsed    | 1778     |\n",
      "|    total_timesteps | 1270080  |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.26e+03   |\n",
      "|    ep_rew_mean          | -0.223     |\n",
      "| time/                   |            |\n",
      "|    fps                  | 715        |\n",
      "|    iterations           | 128        |\n",
      "|    time_elapsed         | 1802       |\n",
      "|    total_timesteps      | 1290240    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01203342 |\n",
      "|    clip_fraction        | 0.109      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -45.1      |\n",
      "|    explained_variance   | 0.132      |\n",
      "|    learning_rate        | 0.00186    |\n",
      "|    loss                 | -0.00762   |\n",
      "|    n_updates            | 1270       |\n",
      "|    policy_gradient_loss | -0.00612   |\n",
      "|    std                  | 0.756      |\n",
      "|    value_loss           | 1.22e-06   |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.26e+03     |\n",
      "|    ep_rew_mean          | -0.225       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 717          |\n",
      "|    iterations           | 130          |\n",
      "|    time_elapsed         | 1825         |\n",
      "|    total_timesteps      | 1310400      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0092318915 |\n",
      "|    clip_fraction        | 0.0952       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -44.8        |\n",
      "|    explained_variance   | 0.241        |\n",
      "|    learning_rate        | 0.00185      |\n",
      "|    loss                 | -0.00786     |\n",
      "|    n_updates            | 1290         |\n",
      "|    policy_gradient_loss | -0.00577     |\n",
      "|    std                  | 0.749        |\n",
      "|    value_loss           | 5.64e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1330560, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0181     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1330560     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009992168 |\n",
      "|    clip_fraction        | 0.107       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.5       |\n",
      "|    explained_variance   | 0.242       |\n",
      "|    learning_rate        | 0.00184     |\n",
      "|    loss                 | -0.00829    |\n",
      "|    n_updates            | 1310        |\n",
      "|    policy_gradient_loss | -0.00667    |\n",
      "|    std                  | 0.746       |\n",
      "|    value_loss           | 5.82e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.224   |\n",
      "| time/              |          |\n",
      "|    fps             | 717      |\n",
      "|    iterations      | 132      |\n",
      "|    time_elapsed    | 1854     |\n",
      "|    total_timesteps | 1330560  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.225      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 719         |\n",
      "|    iterations           | 134         |\n",
      "|    time_elapsed         | 1877        |\n",
      "|    total_timesteps      | 1350720     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011335655 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.2       |\n",
      "|    explained_variance   | 0.165       |\n",
      "|    learning_rate        | 0.00183     |\n",
      "|    loss                 | -0.01       |\n",
      "|    n_updates            | 1330        |\n",
      "|    policy_gradient_loss | -0.00568    |\n",
      "|    std                  | 0.74        |\n",
      "|    value_loss           | 1.44e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.223      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 721         |\n",
      "|    iterations           | 136         |\n",
      "|    time_elapsed         | 1900        |\n",
      "|    total_timesteps      | 1370880     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009308684 |\n",
      "|    clip_fraction        | 0.101       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.9       |\n",
      "|    explained_variance   | 0.171       |\n",
      "|    learning_rate        | 0.00182     |\n",
      "|    loss                 | -0.00931    |\n",
      "|    n_updates            | 1350        |\n",
      "|    policy_gradient_loss | -0.00529    |\n",
      "|    std                  | 0.737       |\n",
      "|    value_loss           | 1.03e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1391040, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0209     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1391040     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013414722 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.6       |\n",
      "|    explained_variance   | 0.0445      |\n",
      "|    learning_rate        | 0.00181     |\n",
      "|    loss                 | -0.0089     |\n",
      "|    n_updates            | 1370        |\n",
      "|    policy_gradient_loss | -0.00513    |\n",
      "|    std                  | 0.733       |\n",
      "|    value_loss           | 6.1e-06     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.229   |\n",
      "| time/              |          |\n",
      "|    fps             | 720      |\n",
      "|    iterations      | 138      |\n",
      "|    time_elapsed    | 1929     |\n",
      "|    total_timesteps | 1391040  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.227      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 722         |\n",
      "|    iterations           | 140         |\n",
      "|    time_elapsed         | 1952        |\n",
      "|    total_timesteps      | 1411200     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010685765 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.6       |\n",
      "|    explained_variance   | 0.13        |\n",
      "|    learning_rate        | 0.0018      |\n",
      "|    loss                 | -0.00946    |\n",
      "|    n_updates            | 1390        |\n",
      "|    policy_gradient_loss | -0.0063     |\n",
      "|    std                  | 0.731       |\n",
      "|    value_loss           | 1.55e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.228      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 724         |\n",
      "|    iterations           | 142         |\n",
      "|    time_elapsed         | 1975        |\n",
      "|    total_timesteps      | 1431360     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010595313 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.5       |\n",
      "|    explained_variance   | 0.286       |\n",
      "|    learning_rate        | 0.00179     |\n",
      "|    loss                 | -0.00666    |\n",
      "|    n_updates            | 1410        |\n",
      "|    policy_gradient_loss | -0.00573    |\n",
      "|    std                  | 0.732       |\n",
      "|    value_loss           | 7.74e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1451520, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0205     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1451520     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011563478 |\n",
      "|    clip_fraction        | 0.134       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.1       |\n",
      "|    explained_variance   | 0.401       |\n",
      "|    learning_rate        | 0.00178     |\n",
      "|    loss                 | -0.0102     |\n",
      "|    n_updates            | 1430        |\n",
      "|    policy_gradient_loss | -0.00765    |\n",
      "|    std                  | 0.724       |\n",
      "|    value_loss           | 5e-07       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.228   |\n",
      "| time/              |          |\n",
      "|    fps             | 723      |\n",
      "|    iterations      | 144      |\n",
      "|    time_elapsed    | 2004     |\n",
      "|    total_timesteps | 1451520  |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.26e+03   |\n",
      "|    ep_rew_mean          | -0.224     |\n",
      "| time/                   |            |\n",
      "|    fps                  | 725        |\n",
      "|    iterations           | 146        |\n",
      "|    time_elapsed         | 2027       |\n",
      "|    total_timesteps      | 1471680    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01028042 |\n",
      "|    clip_fraction        | 0.105      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -42.8      |\n",
      "|    explained_variance   | 0.182      |\n",
      "|    learning_rate        | 0.00177    |\n",
      "|    loss                 | -0.0109    |\n",
      "|    n_updates            | 1450       |\n",
      "|    policy_gradient_loss | -0.0062    |\n",
      "|    std                  | 0.719      |\n",
      "|    value_loss           | 1.27e-06   |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.26e+03     |\n",
      "|    ep_rew_mean          | -0.22        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 727          |\n",
      "|    iterations           | 148          |\n",
      "|    time_elapsed         | 2051         |\n",
      "|    total_timesteps      | 1491840      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0094918525 |\n",
      "|    clip_fraction        | 0.0981       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -42.6        |\n",
      "|    explained_variance   | 0.323        |\n",
      "|    learning_rate        | 0.00176      |\n",
      "|    loss                 | -0.0081      |\n",
      "|    n_updates            | 1470         |\n",
      "|    policy_gradient_loss | -0.00565     |\n",
      "|    std                  | 0.718        |\n",
      "|    value_loss           | 4.84e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1512000, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0206     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1512000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011474794 |\n",
      "|    clip_fraction        | 0.111       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.4       |\n",
      "|    explained_variance   | 0.0767      |\n",
      "|    learning_rate        | 0.00175     |\n",
      "|    loss                 | -0.00793    |\n",
      "|    n_updates            | 1490        |\n",
      "|    policy_gradient_loss | -0.00593    |\n",
      "|    std                  | 0.717       |\n",
      "|    value_loss           | 4.18e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.217   |\n",
      "| time/              |          |\n",
      "|    fps             | 726      |\n",
      "|    iterations      | 150      |\n",
      "|    time_elapsed    | 2080     |\n",
      "|    total_timesteps | 1512000  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.219      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 728         |\n",
      "|    iterations           | 152         |\n",
      "|    time_elapsed         | 2103        |\n",
      "|    total_timesteps      | 1532160     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011393944 |\n",
      "|    clip_fraction        | 0.0942      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.1       |\n",
      "|    explained_variance   | 0.144       |\n",
      "|    learning_rate        | 0.00174     |\n",
      "|    loss                 | -0.00858    |\n",
      "|    n_updates            | 1510        |\n",
      "|    policy_gradient_loss | -0.00455    |\n",
      "|    std                  | 0.711       |\n",
      "|    value_loss           | 5.42e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.217      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 730         |\n",
      "|    iterations           | 154         |\n",
      "|    time_elapsed         | 2126        |\n",
      "|    total_timesteps      | 1552320     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011574896 |\n",
      "|    clip_fraction        | 0.135       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.9       |\n",
      "|    explained_variance   | 0.184       |\n",
      "|    learning_rate        | 0.00173     |\n",
      "|    loss                 | -0.00897    |\n",
      "|    n_updates            | 1530        |\n",
      "|    policy_gradient_loss | -0.0074     |\n",
      "|    std                  | 0.709       |\n",
      "|    value_loss           | 1.19e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1572480, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0196     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1572480     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011918661 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.7       |\n",
      "|    explained_variance   | 0.016       |\n",
      "|    learning_rate        | 0.00172     |\n",
      "|    loss                 | -0.00579    |\n",
      "|    n_updates            | 1550        |\n",
      "|    policy_gradient_loss | -0.00472    |\n",
      "|    std                  | 0.706       |\n",
      "|    value_loss           | 1.18e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.216   |\n",
      "| time/              |          |\n",
      "|    fps             | 729      |\n",
      "|    iterations      | 156      |\n",
      "|    time_elapsed    | 2155     |\n",
      "|    total_timesteps | 1572480  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.214      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 731         |\n",
      "|    iterations           | 158         |\n",
      "|    time_elapsed         | 2178        |\n",
      "|    total_timesteps      | 1592640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009637299 |\n",
      "|    clip_fraction        | 0.107       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.6       |\n",
      "|    explained_variance   | 0.113       |\n",
      "|    learning_rate        | 0.00171     |\n",
      "|    loss                 | -0.00949    |\n",
      "|    n_updates            | 1570        |\n",
      "|    policy_gradient_loss | -0.00583    |\n",
      "|    std                  | 0.704       |\n",
      "|    value_loss           | 1.18e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.213      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 732         |\n",
      "|    iterations           | 160         |\n",
      "|    time_elapsed         | 2201        |\n",
      "|    total_timesteps      | 1612800     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013579182 |\n",
      "|    clip_fraction        | 0.123       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.2       |\n",
      "|    explained_variance   | 0.0846      |\n",
      "|    learning_rate        | 0.0017      |\n",
      "|    loss                 | -0.0107     |\n",
      "|    n_updates            | 1590        |\n",
      "|    policy_gradient_loss | -0.00632    |\n",
      "|    std                  | 0.699       |\n",
      "|    value_loss           | 3.12e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1632960, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0172     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1632960     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010826776 |\n",
      "|    clip_fraction        | 0.131       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41         |\n",
      "|    explained_variance   | 0.221       |\n",
      "|    learning_rate        | 0.00169     |\n",
      "|    loss                 | -0.00759    |\n",
      "|    n_updates            | 1610        |\n",
      "|    policy_gradient_loss | -0.00822    |\n",
      "|    std                  | 0.697       |\n",
      "|    value_loss           | 4.07e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.214   |\n",
      "| time/              |          |\n",
      "|    fps             | 731      |\n",
      "|    iterations      | 162      |\n",
      "|    time_elapsed    | 2230     |\n",
      "|    total_timesteps | 1632960  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.21       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 733         |\n",
      "|    iterations           | 164         |\n",
      "|    time_elapsed         | 2254        |\n",
      "|    total_timesteps      | 1653120     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009859054 |\n",
      "|    clip_fraction        | 0.109       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -40.7       |\n",
      "|    explained_variance   | 0.0589      |\n",
      "|    learning_rate        | 0.00168     |\n",
      "|    loss                 | -0.00969    |\n",
      "|    n_updates            | 1630        |\n",
      "|    policy_gradient_loss | -0.00643    |\n",
      "|    std                  | 0.693       |\n",
      "|    value_loss           | 5.5e-07     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.204      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 734         |\n",
      "|    iterations           | 166         |\n",
      "|    time_elapsed         | 2276        |\n",
      "|    total_timesteps      | 1673280     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012950464 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -40.7       |\n",
      "|    explained_variance   | 0.0481      |\n",
      "|    learning_rate        | 0.00167     |\n",
      "|    loss                 | -0.00846    |\n",
      "|    n_updates            | 1650        |\n",
      "|    policy_gradient_loss | -0.00647    |\n",
      "|    std                  | 0.695       |\n",
      "|    value_loss           | 4.4e-06     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1693440, episode_reward=-0.03 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0285     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1693440     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010900561 |\n",
      "|    clip_fraction        | 0.123       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -40.4       |\n",
      "|    explained_variance   | 0.14        |\n",
      "|    learning_rate        | 0.00166     |\n",
      "|    loss                 | -0.0116     |\n",
      "|    n_updates            | 1670        |\n",
      "|    policy_gradient_loss | -0.00698    |\n",
      "|    std                  | 0.689       |\n",
      "|    value_loss           | 6.41e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 734      |\n",
      "|    iterations      | 168      |\n",
      "|    time_elapsed    | 2306     |\n",
      "|    total_timesteps | 1693440  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.199      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 735         |\n",
      "|    iterations           | 170         |\n",
      "|    time_elapsed         | 2328        |\n",
      "|    total_timesteps      | 1713600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008781953 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -40         |\n",
      "|    explained_variance   | 0.159       |\n",
      "|    learning_rate        | 0.00165     |\n",
      "|    loss                 | -0.0109     |\n",
      "|    n_updates            | 1690        |\n",
      "|    policy_gradient_loss | -0.00624    |\n",
      "|    std                  | 0.683       |\n",
      "|    value_loss           | 5.63e-07    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.194      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 737         |\n",
      "|    iterations           | 172         |\n",
      "|    time_elapsed         | 2351        |\n",
      "|    total_timesteps      | 1733760     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007917721 |\n",
      "|    clip_fraction        | 0.0927      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -39.6       |\n",
      "|    explained_variance   | 0.342       |\n",
      "|    learning_rate        | 0.00164     |\n",
      "|    loss                 | -0.00623    |\n",
      "|    n_updates            | 1710        |\n",
      "|    policy_gradient_loss | -0.00663    |\n",
      "|    std                  | 0.678       |\n",
      "|    value_loss           | 1.5e-07     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1753920, episode_reward=-0.03 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0306    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1753920    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00987902 |\n",
      "|    clip_fraction        | 0.099      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -39.3      |\n",
      "|    explained_variance   | 0.064      |\n",
      "|    learning_rate        | 0.00163    |\n",
      "|    loss                 | -0.00705   |\n",
      "|    n_updates            | 1730       |\n",
      "|    policy_gradient_loss | -0.00441   |\n",
      "|    std                  | 0.675      |\n",
      "|    value_loss           | 2.56e-06   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.195   |\n",
      "| time/              |          |\n",
      "|    fps             | 736      |\n",
      "|    iterations      | 174      |\n",
      "|    time_elapsed    | 2381     |\n",
      "|    total_timesteps | 1753920  |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1.26e+03  |\n",
      "|    ep_rew_mean          | -0.192    |\n",
      "| time/                   |           |\n",
      "|    fps                  | 737       |\n",
      "|    iterations           | 176       |\n",
      "|    time_elapsed         | 2404      |\n",
      "|    total_timesteps      | 1774080   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0091706 |\n",
      "|    clip_fraction        | 0.0978    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -39       |\n",
      "|    explained_variance   | 0.235     |\n",
      "|    learning_rate        | 0.00162   |\n",
      "|    loss                 | -0.00809  |\n",
      "|    n_updates            | 1750      |\n",
      "|    policy_gradient_loss | -0.00604  |\n",
      "|    std                  | 0.669     |\n",
      "|    value_loss           | 3e-07     |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.189      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 739         |\n",
      "|    iterations           | 178         |\n",
      "|    time_elapsed         | 2427        |\n",
      "|    total_timesteps      | 1794240     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009265076 |\n",
      "|    clip_fraction        | 0.0966      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -38.8       |\n",
      "|    explained_variance   | 0.243       |\n",
      "|    learning_rate        | 0.00161     |\n",
      "|    loss                 | -0.00588    |\n",
      "|    n_updates            | 1770        |\n",
      "|    policy_gradient_loss | -0.00536    |\n",
      "|    std                  | 0.665       |\n",
      "|    value_loss           | 2.98e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1814400, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.022      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1814400     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011605963 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -38.6       |\n",
      "|    explained_variance   | 0.0495      |\n",
      "|    learning_rate        | 0.0016      |\n",
      "|    loss                 | -0.00916    |\n",
      "|    n_updates            | 1790        |\n",
      "|    policy_gradient_loss | -0.00525    |\n",
      "|    std                  | 0.665       |\n",
      "|    value_loss           | 3.69e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.189   |\n",
      "| time/              |          |\n",
      "|    fps             | 738      |\n",
      "|    iterations      | 180      |\n",
      "|    time_elapsed    | 2456     |\n",
      "|    total_timesteps | 1814400  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.193      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 739         |\n",
      "|    iterations           | 182         |\n",
      "|    time_elapsed         | 2479        |\n",
      "|    total_timesteps      | 1834560     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009876139 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -38.4       |\n",
      "|    explained_variance   | 0.26        |\n",
      "|    learning_rate        | 0.00159     |\n",
      "|    loss                 | -0.00774    |\n",
      "|    n_updates            | 1810        |\n",
      "|    policy_gradient_loss | -0.00589    |\n",
      "|    std                  | 0.661       |\n",
      "|    value_loss           | 3.4e-07     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.198      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 741         |\n",
      "|    iterations           | 184         |\n",
      "|    time_elapsed         | 2502        |\n",
      "|    total_timesteps      | 1854720     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011939319 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -38.3       |\n",
      "|    explained_variance   | 0.0783      |\n",
      "|    learning_rate        | 0.00158     |\n",
      "|    loss                 | -0.00935    |\n",
      "|    n_updates            | 1830        |\n",
      "|    policy_gradient_loss | -0.00519    |\n",
      "|    std                  | 0.661       |\n",
      "|    value_loss           | 2.1e-06     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1874880, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.021      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1874880     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009193625 |\n",
      "|    clip_fraction        | 0.0921      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -38.1       |\n",
      "|    explained_variance   | 0.461       |\n",
      "|    learning_rate        | 0.00157     |\n",
      "|    loss                 | -0.00818    |\n",
      "|    n_updates            | 1850        |\n",
      "|    policy_gradient_loss | -0.00601    |\n",
      "|    std                  | 0.657       |\n",
      "|    value_loss           | 2.23e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.194   |\n",
      "| time/              |          |\n",
      "|    fps             | 740      |\n",
      "|    iterations      | 186      |\n",
      "|    time_elapsed    | 2531     |\n",
      "|    total_timesteps | 1874880  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.198      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 741         |\n",
      "|    iterations           | 188         |\n",
      "|    time_elapsed         | 2555        |\n",
      "|    total_timesteps      | 1895040     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010421554 |\n",
      "|    clip_fraction        | 0.0887      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -37.9       |\n",
      "|    explained_variance   | 0.114       |\n",
      "|    learning_rate        | 0.00156     |\n",
      "|    loss                 | -0.00513    |\n",
      "|    n_updates            | 1870        |\n",
      "|    policy_gradient_loss | -0.00471    |\n",
      "|    std                  | 0.655       |\n",
      "|    value_loss           | 2.39e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.201      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 742         |\n",
      "|    iterations           | 190         |\n",
      "|    time_elapsed         | 2578        |\n",
      "|    total_timesteps      | 1915200     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013227201 |\n",
      "|    clip_fraction        | 0.135       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -37.6       |\n",
      "|    explained_variance   | 0.154       |\n",
      "|    learning_rate        | 0.00155     |\n",
      "|    loss                 | -0.0095     |\n",
      "|    n_updates            | 1890        |\n",
      "|    policy_gradient_loss | -0.00678    |\n",
      "|    std                  | 0.649       |\n",
      "|    value_loss           | 1.26e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1935360, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0234      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1935360      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0107890135 |\n",
      "|    clip_fraction        | 0.101        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -37.3        |\n",
      "|    explained_variance   | 0.124        |\n",
      "|    learning_rate        | 0.00154      |\n",
      "|    loss                 | -0.00561     |\n",
      "|    n_updates            | 1910         |\n",
      "|    policy_gradient_loss | -0.00594     |\n",
      "|    std                  | 0.648        |\n",
      "|    value_loss           | 6.31e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.197   |\n",
      "| time/              |          |\n",
      "|    fps             | 742      |\n",
      "|    iterations      | 192      |\n",
      "|    time_elapsed    | 2607     |\n",
      "|    total_timesteps | 1935360  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.193      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 743         |\n",
      "|    iterations           | 194         |\n",
      "|    time_elapsed         | 2630        |\n",
      "|    total_timesteps      | 1955520     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007711813 |\n",
      "|    clip_fraction        | 0.0942      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -37.1       |\n",
      "|    explained_variance   | 0.215       |\n",
      "|    learning_rate        | 0.00153     |\n",
      "|    loss                 | -0.0103     |\n",
      "|    n_updates            | 1930        |\n",
      "|    policy_gradient_loss | -0.00741    |\n",
      "|    std                  | 0.645       |\n",
      "|    value_loss           | 8.82e-08    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.26e+03     |\n",
      "|    ep_rew_mean          | -0.19        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 744          |\n",
      "|    iterations           | 196          |\n",
      "|    time_elapsed         | 2653         |\n",
      "|    total_timesteps      | 1975680      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0106804045 |\n",
      "|    clip_fraction        | 0.105        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -36.9        |\n",
      "|    explained_variance   | 0.318        |\n",
      "|    learning_rate        | 0.00152      |\n",
      "|    loss                 | -0.00797     |\n",
      "|    n_updates            | 1950         |\n",
      "|    policy_gradient_loss | -0.00635     |\n",
      "|    std                  | 0.643        |\n",
      "|    value_loss           | 2.91e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1995840, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0201     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1995840     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011202902 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -36.8       |\n",
      "|    explained_variance   | 0.169       |\n",
      "|    learning_rate        | 0.00151     |\n",
      "|    loss                 | -0.00536    |\n",
      "|    n_updates            | 1970        |\n",
      "|    policy_gradient_loss | -0.0061     |\n",
      "|    std                  | 0.642       |\n",
      "|    value_loss           | 1.13e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.191   |\n",
      "| time/              |          |\n",
      "|    fps             | 744      |\n",
      "|    iterations      | 198      |\n",
      "|    time_elapsed    | 2682     |\n",
      "|    total_timesteps | 1995840  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.193      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 745         |\n",
      "|    iterations           | 200         |\n",
      "|    time_elapsed         | 2705        |\n",
      "|    total_timesteps      | 2016000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011571469 |\n",
      "|    clip_fraction        | 0.109       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -36.7       |\n",
      "|    explained_variance   | 0.17        |\n",
      "|    learning_rate        | 0.0015      |\n",
      "|    loss                 | -0.0107     |\n",
      "|    n_updates            | 1990        |\n",
      "|    policy_gradient_loss | -0.0067     |\n",
      "|    std                  | 0.64        |\n",
      "|    value_loss           | 7.88e-07    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.186      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 746         |\n",
      "|    iterations           | 202         |\n",
      "|    time_elapsed         | 2728        |\n",
      "|    total_timesteps      | 2036160     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008337424 |\n",
      "|    clip_fraction        | 0.0995      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -36.2       |\n",
      "|    explained_variance   | 0.318       |\n",
      "|    learning_rate        | 0.00149     |\n",
      "|    loss                 | -0.00808    |\n",
      "|    n_updates            | 2010        |\n",
      "|    policy_gradient_loss | -0.00762    |\n",
      "|    std                  | 0.634       |\n",
      "|    value_loss           | 8.9e-08     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2056320, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0244     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2056320     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009789216 |\n",
      "|    clip_fraction        | 0.0895      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -36         |\n",
      "|    explained_variance   | 0.208       |\n",
      "|    learning_rate        | 0.00148     |\n",
      "|    loss                 | -0.00828    |\n",
      "|    n_updates            | 2030        |\n",
      "|    policy_gradient_loss | -0.00531    |\n",
      "|    std                  | 0.632       |\n",
      "|    value_loss           | 4.31e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.186   |\n",
      "| time/              |          |\n",
      "|    fps             | 745      |\n",
      "|    iterations      | 204      |\n",
      "|    time_elapsed    | 2758     |\n",
      "|    total_timesteps | 2056320  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.188      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 746         |\n",
      "|    iterations           | 206         |\n",
      "|    time_elapsed         | 2781        |\n",
      "|    total_timesteps      | 2076480     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010970056 |\n",
      "|    clip_fraction        | 0.116       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -35.8       |\n",
      "|    explained_variance   | 0.161       |\n",
      "|    learning_rate        | 0.00147     |\n",
      "|    loss                 | -0.00688    |\n",
      "|    n_updates            | 2050        |\n",
      "|    policy_gradient_loss | -0.00581    |\n",
      "|    std                  | 0.629       |\n",
      "|    value_loss           | 7.91e-07    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.26e+03     |\n",
      "|    ep_rew_mean          | -0.186       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 747          |\n",
      "|    iterations           | 208          |\n",
      "|    time_elapsed         | 2804         |\n",
      "|    total_timesteps      | 2096640      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0095558725 |\n",
      "|    clip_fraction        | 0.0842       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -35.8        |\n",
      "|    explained_variance   | 0.0634       |\n",
      "|    learning_rate        | 0.00146      |\n",
      "|    loss                 | -0.00573     |\n",
      "|    n_updates            | 2070         |\n",
      "|    policy_gradient_loss | -0.0043      |\n",
      "|    std                  | 0.627        |\n",
      "|    value_loss           | 3.31e-06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2116800, episode_reward=-0.03 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0267      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2116800      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0090505285 |\n",
      "|    clip_fraction        | 0.0853       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -35.5        |\n",
      "|    explained_variance   | 0.139        |\n",
      "|    learning_rate        | 0.00145      |\n",
      "|    loss                 | -0.0046      |\n",
      "|    n_updates            | 2090         |\n",
      "|    policy_gradient_loss | -0.00529     |\n",
      "|    std                  | 0.623        |\n",
      "|    value_loss           | 2.74e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.183   |\n",
      "| time/              |          |\n",
      "|    fps             | 747      |\n",
      "|    iterations      | 210      |\n",
      "|    time_elapsed    | 2833     |\n",
      "|    total_timesteps | 2116800  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.178      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 748         |\n",
      "|    iterations           | 212         |\n",
      "|    time_elapsed         | 2856        |\n",
      "|    total_timesteps      | 2136960     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008565345 |\n",
      "|    clip_fraction        | 0.0781      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -35.2       |\n",
      "|    explained_variance   | 0.0907      |\n",
      "|    learning_rate        | 0.00144     |\n",
      "|    loss                 | -0.00619    |\n",
      "|    n_updates            | 2110        |\n",
      "|    policy_gradient_loss | -0.00494    |\n",
      "|    std                  | 0.619       |\n",
      "|    value_loss           | 1.65e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.178      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 749         |\n",
      "|    iterations           | 214         |\n",
      "|    time_elapsed         | 2879        |\n",
      "|    total_timesteps      | 2157120     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011244491 |\n",
      "|    clip_fraction        | 0.0991      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -34.9       |\n",
      "|    explained_variance   | 0.143       |\n",
      "|    learning_rate        | 0.00143     |\n",
      "|    loss                 | -0.00602    |\n",
      "|    n_updates            | 2130        |\n",
      "|    policy_gradient_loss | -0.005      |\n",
      "|    std                  | 0.617       |\n",
      "|    value_loss           | 1.15e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2177280, episode_reward=-0.03 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0289     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2177280     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011106085 |\n",
      "|    clip_fraction        | 0.124       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -34.6       |\n",
      "|    explained_variance   | 0.128       |\n",
      "|    learning_rate        | 0.00142     |\n",
      "|    loss                 | -0.00719    |\n",
      "|    n_updates            | 2150        |\n",
      "|    policy_gradient_loss | -0.00626    |\n",
      "|    std                  | 0.612       |\n",
      "|    value_loss           | 1.26e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.178   |\n",
      "| time/              |          |\n",
      "|    fps             | 748      |\n",
      "|    iterations      | 216      |\n",
      "|    time_elapsed    | 2908     |\n",
      "|    total_timesteps | 2177280  |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.26e+03   |\n",
      "|    ep_rew_mean          | -0.178     |\n",
      "| time/                   |            |\n",
      "|    fps                  | 749        |\n",
      "|    iterations           | 218        |\n",
      "|    time_elapsed         | 2931       |\n",
      "|    total_timesteps      | 2197440    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01189189 |\n",
      "|    clip_fraction        | 0.123      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -34.3      |\n",
      "|    explained_variance   | 0.079      |\n",
      "|    learning_rate        | 0.00141    |\n",
      "|    loss                 | -0.00637   |\n",
      "|    n_updates            | 2170       |\n",
      "|    policy_gradient_loss | -0.00562   |\n",
      "|    std                  | 0.609      |\n",
      "|    value_loss           | 1.67e-06   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.174      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 750         |\n",
      "|    iterations           | 220         |\n",
      "|    time_elapsed         | 2954        |\n",
      "|    total_timesteps      | 2217600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009152266 |\n",
      "|    clip_fraction        | 0.0936      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -34.2       |\n",
      "|    explained_variance   | 0.275       |\n",
      "|    learning_rate        | 0.0014      |\n",
      "|    loss                 | -0.0047     |\n",
      "|    n_updates            | 2190        |\n",
      "|    policy_gradient_loss | -0.00609    |\n",
      "|    std                  | 0.608       |\n",
      "|    value_loss           | 1.28e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2237760, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0216     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2237760     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008234078 |\n",
      "|    clip_fraction        | 0.0852      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -34         |\n",
      "|    explained_variance   | 0.392       |\n",
      "|    learning_rate        | 0.00139     |\n",
      "|    loss                 | -0.00962    |\n",
      "|    n_updates            | 2210        |\n",
      "|    policy_gradient_loss | -0.00528    |\n",
      "|    std                  | 0.605       |\n",
      "|    value_loss           | 5.87e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.176   |\n",
      "| time/              |          |\n",
      "|    fps             | 749      |\n",
      "|    iterations      | 222      |\n",
      "|    time_elapsed    | 2983     |\n",
      "|    total_timesteps | 2237760  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.175      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 750         |\n",
      "|    iterations           | 224         |\n",
      "|    time_elapsed         | 3007        |\n",
      "|    total_timesteps      | 2257920     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011671792 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -33.7       |\n",
      "|    explained_variance   | 0.164       |\n",
      "|    learning_rate        | 0.00138     |\n",
      "|    loss                 | -0.00828    |\n",
      "|    n_updates            | 2230        |\n",
      "|    policy_gradient_loss | -0.00529    |\n",
      "|    std                  | 0.6         |\n",
      "|    value_loss           | 7.55e-07    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.26e+03     |\n",
      "|    ep_rew_mean          | -0.176       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 751          |\n",
      "|    iterations           | 226          |\n",
      "|    time_elapsed         | 3030         |\n",
      "|    total_timesteps      | 2278080      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0096517345 |\n",
      "|    clip_fraction        | 0.0995       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -33.4        |\n",
      "|    explained_variance   | 0.338        |\n",
      "|    learning_rate        | 0.00137      |\n",
      "|    loss                 | -0.00689     |\n",
      "|    n_updates            | 2250         |\n",
      "|    policy_gradient_loss | -0.00657     |\n",
      "|    std                  | 0.597        |\n",
      "|    value_loss           | 1.91e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2298240, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0213     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2298240     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010868998 |\n",
      "|    clip_fraction        | 0.114       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -33.1       |\n",
      "|    explained_variance   | 0.135       |\n",
      "|    learning_rate        | 0.00136     |\n",
      "|    loss                 | -0.0102     |\n",
      "|    n_updates            | 2270        |\n",
      "|    policy_gradient_loss | -0.0063     |\n",
      "|    std                  | 0.592       |\n",
      "|    value_loss           | 6.19e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.174   |\n",
      "| time/              |          |\n",
      "|    fps             | 751      |\n",
      "|    iterations      | 228      |\n",
      "|    time_elapsed    | 3059     |\n",
      "|    total_timesteps | 2298240  |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.26e+03     |\n",
      "|    ep_rew_mean          | -0.17        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 752          |\n",
      "|    iterations           | 230          |\n",
      "|    time_elapsed         | 3082         |\n",
      "|    total_timesteps      | 2318400      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0099725295 |\n",
      "|    clip_fraction        | 0.0895       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -32.7        |\n",
      "|    explained_variance   | 0.329        |\n",
      "|    learning_rate        | 0.00135      |\n",
      "|    loss                 | -0.00614     |\n",
      "|    n_updates            | 2290         |\n",
      "|    policy_gradient_loss | -0.00488     |\n",
      "|    std                  | 0.588        |\n",
      "|    value_loss           | 3.74e-07     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.171      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 753         |\n",
      "|    iterations           | 232         |\n",
      "|    time_elapsed         | 3105        |\n",
      "|    total_timesteps      | 2338560     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008812724 |\n",
      "|    clip_fraction        | 0.0958      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -32.4       |\n",
      "|    explained_variance   | 0.272       |\n",
      "|    learning_rate        | 0.00134     |\n",
      "|    loss                 | -0.0102     |\n",
      "|    n_updates            | 2310        |\n",
      "|    policy_gradient_loss | -0.00664    |\n",
      "|    std                  | 0.584       |\n",
      "|    value_loss           | 1.85e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2358720, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.018      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2358720     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010848945 |\n",
      "|    clip_fraction        | 0.109       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -32.1       |\n",
      "|    explained_variance   | 0.136       |\n",
      "|    learning_rate        | 0.00133     |\n",
      "|    loss                 | -0.00831    |\n",
      "|    n_updates            | 2330        |\n",
      "|    policy_gradient_loss | -0.00601    |\n",
      "|    std                  | 0.58        |\n",
      "|    value_loss           | 3.59e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.169   |\n",
      "| time/              |          |\n",
      "|    fps             | 752      |\n",
      "|    iterations      | 234      |\n",
      "|    time_elapsed    | 3134     |\n",
      "|    total_timesteps | 2358720  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.17       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 753         |\n",
      "|    iterations           | 236         |\n",
      "|    time_elapsed         | 3157        |\n",
      "|    total_timesteps      | 2378880     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009153506 |\n",
      "|    clip_fraction        | 0.107       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -31.8       |\n",
      "|    explained_variance   | 0.0219      |\n",
      "|    learning_rate        | 0.00132     |\n",
      "|    loss                 | -0.00722    |\n",
      "|    n_updates            | 2350        |\n",
      "|    policy_gradient_loss | -0.00454    |\n",
      "|    std                  | 0.576       |\n",
      "|    value_loss           | 8.64e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.166      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 754         |\n",
      "|    iterations           | 238         |\n",
      "|    time_elapsed         | 3180        |\n",
      "|    total_timesteps      | 2399040     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008449031 |\n",
      "|    clip_fraction        | 0.0876      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -31.6       |\n",
      "|    explained_variance   | 0.102       |\n",
      "|    learning_rate        | 0.00131     |\n",
      "|    loss                 | -0.00815    |\n",
      "|    n_updates            | 2370        |\n",
      "|    policy_gradient_loss | -0.00574    |\n",
      "|    std                  | 0.574       |\n",
      "|    value_loss           | 2e-07       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2419200, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0194      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2419200      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0101874955 |\n",
      "|    clip_fraction        | 0.119        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -31.4        |\n",
      "|    explained_variance   | 0.0281       |\n",
      "|    learning_rate        | 0.0013       |\n",
      "|    loss                 | -0.00719     |\n",
      "|    n_updates            | 2390         |\n",
      "|    policy_gradient_loss | -0.00503     |\n",
      "|    std                  | 0.571        |\n",
      "|    value_loss           | 1.86e-06     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.163   |\n",
      "| time/              |          |\n",
      "|    fps             | 753      |\n",
      "|    iterations      | 240      |\n",
      "|    time_elapsed    | 3209     |\n",
      "|    total_timesteps | 2419200  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.163      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 754         |\n",
      "|    iterations           | 242         |\n",
      "|    time_elapsed         | 3232        |\n",
      "|    total_timesteps      | 2439360     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008431279 |\n",
      "|    clip_fraction        | 0.087       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -31.2       |\n",
      "|    explained_variance   | 0.258       |\n",
      "|    learning_rate        | 0.00129     |\n",
      "|    loss                 | -0.00846    |\n",
      "|    n_updates            | 2410        |\n",
      "|    policy_gradient_loss | -0.00645    |\n",
      "|    std                  | 0.569       |\n",
      "|    value_loss           | 1.11e-07    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.162      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 755         |\n",
      "|    iterations           | 244         |\n",
      "|    time_elapsed         | 3255        |\n",
      "|    total_timesteps      | 2459520     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010671852 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -30.8       |\n",
      "|    explained_variance   | 0.213       |\n",
      "|    learning_rate        | 0.00128     |\n",
      "|    loss                 | -0.0127     |\n",
      "|    n_updates            | 2430        |\n",
      "|    policy_gradient_loss | -0.00718    |\n",
      "|    std                  | 0.563       |\n",
      "|    value_loss           | 2.58e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2479680, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0165     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2479680     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012782475 |\n",
      "|    clip_fraction        | 0.13        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -30.5       |\n",
      "|    explained_variance   | 0.0963      |\n",
      "|    learning_rate        | 0.00127     |\n",
      "|    loss                 | -0.0093     |\n",
      "|    n_updates            | 2450        |\n",
      "|    policy_gradient_loss | -0.00635    |\n",
      "|    std                  | 0.562       |\n",
      "|    value_loss           | 1.72e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.163   |\n",
      "| time/              |          |\n",
      "|    fps             | 754      |\n",
      "|    iterations      | 246      |\n",
      "|    time_elapsed    | 3285     |\n",
      "|    total_timesteps | 2479680  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.16       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 755         |\n",
      "|    iterations           | 248         |\n",
      "|    time_elapsed         | 3308        |\n",
      "|    total_timesteps      | 2499840     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008636702 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -30.2       |\n",
      "|    explained_variance   | 0.319       |\n",
      "|    learning_rate        | 0.00126     |\n",
      "|    loss                 | -0.0119     |\n",
      "|    n_updates            | 2470        |\n",
      "|    policy_gradient_loss | -0.00761    |\n",
      "|    std                  | 0.557       |\n",
      "|    value_loss           | 8.24e-08    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.159      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 756         |\n",
      "|    iterations           | 250         |\n",
      "|    time_elapsed         | 3331        |\n",
      "|    total_timesteps      | 2520000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011512496 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -30         |\n",
      "|    explained_variance   | 0.0508      |\n",
      "|    learning_rate        | 0.00125     |\n",
      "|    loss                 | -0.00374    |\n",
      "|    n_updates            | 2490        |\n",
      "|    policy_gradient_loss | -0.00505    |\n",
      "|    std                  | 0.558       |\n",
      "|    value_loss           | 2.08e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2540160, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.016      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2540160     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008120077 |\n",
      "|    clip_fraction        | 0.0899      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -29.8       |\n",
      "|    explained_variance   | 0.292       |\n",
      "|    learning_rate        | 0.00123     |\n",
      "|    loss                 | -0.00661    |\n",
      "|    n_updates            | 2510        |\n",
      "|    policy_gradient_loss | -0.00684    |\n",
      "|    std                  | 0.554       |\n",
      "|    value_loss           | 7.03e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.158   |\n",
      "| time/              |          |\n",
      "|    fps             | 755      |\n",
      "|    iterations      | 252      |\n",
      "|    time_elapsed    | 3360     |\n",
      "|    total_timesteps | 2540160  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.159      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 756         |\n",
      "|    iterations           | 254         |\n",
      "|    time_elapsed         | 3383        |\n",
      "|    total_timesteps      | 2560320     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007867468 |\n",
      "|    clip_fraction        | 0.089       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -29.8       |\n",
      "|    explained_variance   | 0.0471      |\n",
      "|    learning_rate        | 0.00122     |\n",
      "|    loss                 | -0.00615    |\n",
      "|    n_updates            | 2530        |\n",
      "|    policy_gradient_loss | -0.00516    |\n",
      "|    std                  | 0.554       |\n",
      "|    value_loss           | 1.13e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.16       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 757         |\n",
      "|    iterations           | 256         |\n",
      "|    time_elapsed         | 3406        |\n",
      "|    total_timesteps      | 2580480     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010174401 |\n",
      "|    clip_fraction        | 0.098       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -29.5       |\n",
      "|    explained_variance   | 0.0355      |\n",
      "|    learning_rate        | 0.00121     |\n",
      "|    loss                 | -0.00887    |\n",
      "|    n_updates            | 2550        |\n",
      "|    policy_gradient_loss | -0.00485    |\n",
      "|    std                  | 0.552       |\n",
      "|    value_loss           | 3.46e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2600640, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0175     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2600640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010169062 |\n",
      "|    clip_fraction        | 0.111       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -29.4       |\n",
      "|    explained_variance   | 0.0912      |\n",
      "|    learning_rate        | 0.0012      |\n",
      "|    loss                 | -0.00846    |\n",
      "|    n_updates            | 2570        |\n",
      "|    policy_gradient_loss | -0.00546    |\n",
      "|    std                  | 0.551       |\n",
      "|    value_loss           | 1.55e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.159   |\n",
      "| time/              |          |\n",
      "|    fps             | 756      |\n",
      "|    iterations      | 258      |\n",
      "|    time_elapsed    | 3435     |\n",
      "|    total_timesteps | 2600640  |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.26e+03     |\n",
      "|    ep_rew_mean          | -0.159       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 757          |\n",
      "|    iterations           | 260          |\n",
      "|    time_elapsed         | 3458         |\n",
      "|    total_timesteps      | 2620800      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077437246 |\n",
      "|    clip_fraction        | 0.0771       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -29          |\n",
      "|    explained_variance   | 0.303        |\n",
      "|    learning_rate        | 0.00119      |\n",
      "|    loss                 | -0.00532     |\n",
      "|    n_updates            | 2590         |\n",
      "|    policy_gradient_loss | -0.00622     |\n",
      "|    std                  | 0.544        |\n",
      "|    value_loss           | 5.52e-08     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.154      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 758         |\n",
      "|    iterations           | 262         |\n",
      "|    time_elapsed         | 3481        |\n",
      "|    total_timesteps      | 2640960     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008788208 |\n",
      "|    clip_fraction        | 0.0854      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -28.6       |\n",
      "|    explained_variance   | 0.304       |\n",
      "|    learning_rate        | 0.00118     |\n",
      "|    loss                 | -0.00851    |\n",
      "|    n_updates            | 2610        |\n",
      "|    policy_gradient_loss | -0.00637    |\n",
      "|    std                  | 0.54        |\n",
      "|    value_loss           | 6.91e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2661120, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0187     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2661120     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010238221 |\n",
      "|    clip_fraction        | 0.0944      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -28.2       |\n",
      "|    explained_variance   | 0.148       |\n",
      "|    learning_rate        | 0.00117     |\n",
      "|    loss                 | -0.00659    |\n",
      "|    n_updates            | 2630        |\n",
      "|    policy_gradient_loss | -0.0045     |\n",
      "|    std                  | 0.537       |\n",
      "|    value_loss           | 5.84e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.153   |\n",
      "| time/              |          |\n",
      "|    fps             | 757      |\n",
      "|    iterations      | 264      |\n",
      "|    time_elapsed    | 3510     |\n",
      "|    total_timesteps | 2661120  |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.26e+03   |\n",
      "|    ep_rew_mean          | -0.151     |\n",
      "| time/                   |            |\n",
      "|    fps                  | 758        |\n",
      "|    iterations           | 266        |\n",
      "|    time_elapsed         | 3533       |\n",
      "|    total_timesteps      | 2681280    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01194963 |\n",
      "|    clip_fraction        | 0.104      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -28        |\n",
      "|    explained_variance   | 0.00429    |\n",
      "|    learning_rate        | 0.00116    |\n",
      "|    loss                 | -0.00767   |\n",
      "|    n_updates            | 2650       |\n",
      "|    policy_gradient_loss | -0.00444   |\n",
      "|    std                  | 0.534      |\n",
      "|    value_loss           | 4.22e-06   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.15       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 759         |\n",
      "|    iterations           | 268         |\n",
      "|    time_elapsed         | 3556        |\n",
      "|    total_timesteps      | 2701440     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009113688 |\n",
      "|    clip_fraction        | 0.0942      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -27.7       |\n",
      "|    explained_variance   | 0.346       |\n",
      "|    learning_rate        | 0.00115     |\n",
      "|    loss                 | -0.00565    |\n",
      "|    n_updates            | 2670        |\n",
      "|    policy_gradient_loss | -0.00615    |\n",
      "|    std                  | 0.531       |\n",
      "|    value_loss           | 1.66e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2721600, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.018      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2721600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009399488 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -27.4       |\n",
      "|    explained_variance   | 0.214       |\n",
      "|    learning_rate        | 0.00114     |\n",
      "|    loss                 | -0.00865    |\n",
      "|    n_updates            | 2690        |\n",
      "|    policy_gradient_loss | -0.00693    |\n",
      "|    std                  | 0.528       |\n",
      "|    value_loss           | 1.05e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.145   |\n",
      "| time/              |          |\n",
      "|    fps             | 758      |\n",
      "|    iterations      | 270      |\n",
      "|    time_elapsed    | 3585     |\n",
      "|    total_timesteps | 2721600  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.143      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 759         |\n",
      "|    iterations           | 272         |\n",
      "|    time_elapsed         | 3608        |\n",
      "|    total_timesteps      | 2741760     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008476069 |\n",
      "|    clip_fraction        | 0.0801      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -27         |\n",
      "|    explained_variance   | 0.469       |\n",
      "|    learning_rate        | 0.00113     |\n",
      "|    loss                 | -0.00711    |\n",
      "|    n_updates            | 2710        |\n",
      "|    policy_gradient_loss | -0.00585    |\n",
      "|    std                  | 0.522       |\n",
      "|    value_loss           | 7.52e-08    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.146      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 760         |\n",
      "|    iterations           | 274         |\n",
      "|    time_elapsed         | 3631        |\n",
      "|    total_timesteps      | 2761920     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009345524 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -26.5       |\n",
      "|    explained_variance   | 0.397       |\n",
      "|    learning_rate        | 0.00112     |\n",
      "|    loss                 | -0.0081     |\n",
      "|    n_updates            | 2730        |\n",
      "|    policy_gradient_loss | -0.00718    |\n",
      "|    std                  | 0.517       |\n",
      "|    value_loss           | 1.03e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2782080, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0153     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2782080     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009269355 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -26.2       |\n",
      "|    explained_variance   | 0.374       |\n",
      "|    learning_rate        | 0.00111     |\n",
      "|    loss                 | -0.00991    |\n",
      "|    n_updates            | 2750        |\n",
      "|    policy_gradient_loss | -0.00785    |\n",
      "|    std                  | 0.515       |\n",
      "|    value_loss           | 1.63e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.147   |\n",
      "| time/              |          |\n",
      "|    fps             | 759      |\n",
      "|    iterations      | 276      |\n",
      "|    time_elapsed    | 3661     |\n",
      "|    total_timesteps | 2782080  |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.26e+03     |\n",
      "|    ep_rew_mean          | -0.146       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 760          |\n",
      "|    iterations           | 278          |\n",
      "|    time_elapsed         | 3684         |\n",
      "|    total_timesteps      | 2802240      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076840483 |\n",
      "|    clip_fraction        | 0.0923       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -26          |\n",
      "|    explained_variance   | 0.117        |\n",
      "|    learning_rate        | 0.0011       |\n",
      "|    loss                 | -0.00632     |\n",
      "|    n_updates            | 2770         |\n",
      "|    policy_gradient_loss | -0.00557     |\n",
      "|    std                  | 0.514        |\n",
      "|    value_loss           | 4.44e-07     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.149      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 761         |\n",
      "|    iterations           | 280         |\n",
      "|    time_elapsed         | 3706        |\n",
      "|    total_timesteps      | 2822400     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010657454 |\n",
      "|    clip_fraction        | 0.094       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -25.8       |\n",
      "|    explained_variance   | 0.186       |\n",
      "|    learning_rate        | 0.00109     |\n",
      "|    loss                 | -0.00833    |\n",
      "|    n_updates            | 2790        |\n",
      "|    policy_gradient_loss | -0.0058     |\n",
      "|    std                  | 0.512       |\n",
      "|    value_loss           | 8.39e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2842560, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0198     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2842560     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009473028 |\n",
      "|    clip_fraction        | 0.0938      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -25.7       |\n",
      "|    explained_variance   | 0.0531      |\n",
      "|    learning_rate        | 0.00108     |\n",
      "|    loss                 | -0.0058     |\n",
      "|    n_updates            | 2810        |\n",
      "|    policy_gradient_loss | -0.00553    |\n",
      "|    std                  | 0.51        |\n",
      "|    value_loss           | 1.06e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.149   |\n",
      "| time/              |          |\n",
      "|    fps             | 760      |\n",
      "|    iterations      | 282      |\n",
      "|    time_elapsed    | 3735     |\n",
      "|    total_timesteps | 2842560  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.15       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 761         |\n",
      "|    iterations           | 284         |\n",
      "|    time_elapsed         | 3758        |\n",
      "|    total_timesteps      | 2862720     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009486095 |\n",
      "|    clip_fraction        | 0.119       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -25.6       |\n",
      "|    explained_variance   | 0.0233      |\n",
      "|    learning_rate        | 0.00107     |\n",
      "|    loss                 | -0.00644    |\n",
      "|    n_updates            | 2830        |\n",
      "|    policy_gradient_loss | -0.00479    |\n",
      "|    std                  | 0.509       |\n",
      "|    value_loss           | 2.32e-06    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.26e+03     |\n",
      "|    ep_rew_mean          | -0.149       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 762          |\n",
      "|    iterations           | 286          |\n",
      "|    time_elapsed         | 3781         |\n",
      "|    total_timesteps      | 2882880      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0101852305 |\n",
      "|    clip_fraction        | 0.107        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -25.2        |\n",
      "|    explained_variance   | 0.0376       |\n",
      "|    learning_rate        | 0.00106      |\n",
      "|    loss                 | -0.00786     |\n",
      "|    n_updates            | 2850         |\n",
      "|    policy_gradient_loss | -0.00507     |\n",
      "|    std                  | 0.504        |\n",
      "|    value_loss           | 3.59e-06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2903040, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0222     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2903040     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010830917 |\n",
      "|    clip_fraction        | 0.112       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -25         |\n",
      "|    explained_variance   | 0.0983      |\n",
      "|    learning_rate        | 0.00105     |\n",
      "|    loss                 | -0.00963    |\n",
      "|    n_updates            | 2870        |\n",
      "|    policy_gradient_loss | -0.00599    |\n",
      "|    std                  | 0.502       |\n",
      "|    value_loss           | 4.84e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.147   |\n",
      "| time/              |          |\n",
      "|    fps             | 761      |\n",
      "|    iterations      | 288      |\n",
      "|    time_elapsed    | 3810     |\n",
      "|    total_timesteps | 2903040  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.145      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 762         |\n",
      "|    iterations           | 290         |\n",
      "|    time_elapsed         | 3834        |\n",
      "|    total_timesteps      | 2923200     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009039996 |\n",
      "|    clip_fraction        | 0.101       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -24.8       |\n",
      "|    explained_variance   | 0.353       |\n",
      "|    learning_rate        | 0.00104     |\n",
      "|    loss                 | -0.00578    |\n",
      "|    n_updates            | 2890        |\n",
      "|    policy_gradient_loss | -0.00586    |\n",
      "|    std                  | 0.5         |\n",
      "|    value_loss           | 1.64e-07    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.142      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 763         |\n",
      "|    iterations           | 292         |\n",
      "|    time_elapsed         | 3857        |\n",
      "|    total_timesteps      | 2943360     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009978627 |\n",
      "|    clip_fraction        | 0.1         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -24.3       |\n",
      "|    explained_variance   | 0.222       |\n",
      "|    learning_rate        | 0.00103     |\n",
      "|    loss                 | -0.00569    |\n",
      "|    n_updates            | 2910        |\n",
      "|    policy_gradient_loss | -0.00624    |\n",
      "|    std                  | 0.493       |\n",
      "|    value_loss           | 3.66e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2963520, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0186     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2963520     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010105242 |\n",
      "|    clip_fraction        | 0.0987      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -24         |\n",
      "|    explained_variance   | 0.151       |\n",
      "|    learning_rate        | 0.00102     |\n",
      "|    loss                 | -0.00795    |\n",
      "|    n_updates            | 2930        |\n",
      "|    policy_gradient_loss | -0.00602    |\n",
      "|    std                  | 0.489       |\n",
      "|    value_loss           | 2.3e-07     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.141   |\n",
      "| time/              |          |\n",
      "|    fps             | 762      |\n",
      "|    iterations      | 294      |\n",
      "|    time_elapsed    | 3886     |\n",
      "|    total_timesteps | 2963520  |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.26e+03   |\n",
      "|    ep_rew_mean          | -0.138     |\n",
      "| time/                   |            |\n",
      "|    fps                  | 763        |\n",
      "|    iterations           | 296        |\n",
      "|    time_elapsed         | 3909       |\n",
      "|    total_timesteps      | 2983680    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00927068 |\n",
      "|    clip_fraction        | 0.0968     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -23.6      |\n",
      "|    explained_variance   | 0.443      |\n",
      "|    learning_rate        | 0.00101    |\n",
      "|    loss                 | -0.0105    |\n",
      "|    n_updates            | 2950       |\n",
      "|    policy_gradient_loss | -0.00615   |\n",
      "|    std                  | 0.486      |\n",
      "|    value_loss           | 1.89e-07   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.137      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 763         |\n",
      "|    iterations           | 298         |\n",
      "|    time_elapsed         | 3932        |\n",
      "|    total_timesteps      | 3003840     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009280367 |\n",
      "|    clip_fraction        | 0.0785      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -23.2       |\n",
      "|    explained_variance   | 0.213       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | -0.00773    |\n",
      "|    n_updates            | 2970        |\n",
      "|    policy_gradient_loss | -0.00488    |\n",
      "|    std                  | 0.48        |\n",
      "|    value_loss           | 2.33e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3024000, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0203     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3024000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009473773 |\n",
      "|    clip_fraction        | 0.0859      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -23         |\n",
      "|    explained_variance   | 0.0875      |\n",
      "|    learning_rate        | 0.000993    |\n",
      "|    loss                 | -0.00807    |\n",
      "|    n_updates            | 2990        |\n",
      "|    policy_gradient_loss | -0.00509    |\n",
      "|    std                  | 0.478       |\n",
      "|    value_loss           | 5.94e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.136   |\n",
      "| time/              |          |\n",
      "|    fps             | 763      |\n",
      "|    iterations      | 300      |\n",
      "|    time_elapsed    | 3961     |\n",
      "|    total_timesteps | 3024000  |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.26e+03     |\n",
      "|    ep_rew_mean          | -0.136       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 764          |\n",
      "|    iterations           | 302          |\n",
      "|    time_elapsed         | 3984         |\n",
      "|    total_timesteps      | 3044160      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0097605735 |\n",
      "|    clip_fraction        | 0.0932       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -22.9        |\n",
      "|    explained_variance   | 0.242        |\n",
      "|    learning_rate        | 0.000983     |\n",
      "|    loss                 | -0.00955     |\n",
      "|    n_updates            | 3010         |\n",
      "|    policy_gradient_loss | -0.00659     |\n",
      "|    std                  | 0.478        |\n",
      "|    value_loss           | 1.09e-07     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.134      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 764         |\n",
      "|    iterations           | 304         |\n",
      "|    time_elapsed         | 4007        |\n",
      "|    total_timesteps      | 3064320     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010119569 |\n",
      "|    clip_fraction        | 0.0972      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -22.4       |\n",
      "|    explained_variance   | 0.265       |\n",
      "|    learning_rate        | 0.000973    |\n",
      "|    loss                 | -0.00797    |\n",
      "|    n_updates            | 3030        |\n",
      "|    policy_gradient_loss | -0.0064     |\n",
      "|    std                  | 0.473       |\n",
      "|    value_loss           | 2.44e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3084480, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0164     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3084480     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010223232 |\n",
      "|    clip_fraction        | 0.0985      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -22.2       |\n",
      "|    explained_variance   | 0.0787      |\n",
      "|    learning_rate        | 0.000963    |\n",
      "|    loss                 | -0.00617    |\n",
      "|    n_updates            | 3050        |\n",
      "|    policy_gradient_loss | -0.00591    |\n",
      "|    std                  | 0.472       |\n",
      "|    value_loss           | 8.46e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.136   |\n",
      "| time/              |          |\n",
      "|    fps             | 764      |\n",
      "|    iterations      | 306      |\n",
      "|    time_elapsed    | 4037     |\n",
      "|    total_timesteps | 3084480  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.135      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 764         |\n",
      "|    iterations           | 308         |\n",
      "|    time_elapsed         | 4060        |\n",
      "|    total_timesteps      | 3104640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010165095 |\n",
      "|    clip_fraction        | 0.0774      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -22.1       |\n",
      "|    explained_variance   | 0.121       |\n",
      "|    learning_rate        | 0.000953    |\n",
      "|    loss                 | -0.00662    |\n",
      "|    n_updates            | 3070        |\n",
      "|    policy_gradient_loss | -0.00524    |\n",
      "|    std                  | 0.47        |\n",
      "|    value_loss           | 7.74e-07    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.26e+03   |\n",
      "|    ep_rew_mean          | -0.135     |\n",
      "| time/                   |            |\n",
      "|    fps                  | 765        |\n",
      "|    iterations           | 310        |\n",
      "|    time_elapsed         | 4083       |\n",
      "|    total_timesteps      | 3124800    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00890405 |\n",
      "|    clip_fraction        | 0.0832     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -21.7      |\n",
      "|    explained_variance   | 0.178      |\n",
      "|    learning_rate        | 0.000943   |\n",
      "|    loss                 | -0.00906   |\n",
      "|    n_updates            | 3090       |\n",
      "|    policy_gradient_loss | -0.00532   |\n",
      "|    std                  | 0.467      |\n",
      "|    value_loss           | 3.87e-07   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=3144960, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0206     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3144960     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009756663 |\n",
      "|    clip_fraction        | 0.101       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -21.6       |\n",
      "|    explained_variance   | 0.0875      |\n",
      "|    learning_rate        | 0.000933    |\n",
      "|    loss                 | -0.00615    |\n",
      "|    n_updates            | 3110        |\n",
      "|    policy_gradient_loss | -0.00569    |\n",
      "|    std                  | 0.466       |\n",
      "|    value_loss           | 3.24e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.132   |\n",
      "| time/              |          |\n",
      "|    fps             | 764      |\n",
      "|    iterations      | 312      |\n",
      "|    time_elapsed    | 4112     |\n",
      "|    total_timesteps | 3144960  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.131      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 765         |\n",
      "|    iterations           | 314         |\n",
      "|    time_elapsed         | 4135        |\n",
      "|    total_timesteps      | 3165120     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011812863 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -21.3       |\n",
      "|    explained_variance   | 0.184       |\n",
      "|    learning_rate        | 0.000922    |\n",
      "|    loss                 | -0.0109     |\n",
      "|    n_updates            | 3130        |\n",
      "|    policy_gradient_loss | -0.00569    |\n",
      "|    std                  | 0.464       |\n",
      "|    value_loss           | 3.04e-07    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.26e+03   |\n",
      "|    ep_rew_mean          | -0.133     |\n",
      "| time/                   |            |\n",
      "|    fps                  | 766        |\n",
      "|    iterations           | 316        |\n",
      "|    time_elapsed         | 4158       |\n",
      "|    total_timesteps      | 3185280    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01041966 |\n",
      "|    clip_fraction        | 0.093      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -21.1      |\n",
      "|    explained_variance   | 0.0718     |\n",
      "|    learning_rate        | 0.000912   |\n",
      "|    loss                 | -0.00638   |\n",
      "|    n_updates            | 3150       |\n",
      "|    policy_gradient_loss | -0.00469   |\n",
      "|    std                  | 0.462      |\n",
      "|    value_loss           | 1.09e-06   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=3205440, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.016      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3205440     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011228286 |\n",
      "|    clip_fraction        | 0.105       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -20.8       |\n",
      "|    explained_variance   | 0.0682      |\n",
      "|    learning_rate        | 0.000902    |\n",
      "|    loss                 | -0.00828    |\n",
      "|    n_updates            | 3170        |\n",
      "|    policy_gradient_loss | -0.00593    |\n",
      "|    std                  | 0.458       |\n",
      "|    value_loss           | 8.92e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.132   |\n",
      "| time/              |          |\n",
      "|    fps             | 765      |\n",
      "|    iterations      | 318      |\n",
      "|    time_elapsed    | 4187     |\n",
      "|    total_timesteps | 3205440  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.129      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 765         |\n",
      "|    iterations           | 320         |\n",
      "|    time_elapsed         | 4210        |\n",
      "|    total_timesteps      | 3225600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008311509 |\n",
      "|    clip_fraction        | 0.0855      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -20.4       |\n",
      "|    explained_variance   | 0.415       |\n",
      "|    learning_rate        | 0.000892    |\n",
      "|    loss                 | -0.0068     |\n",
      "|    n_updates            | 3190        |\n",
      "|    policy_gradient_loss | -0.00685    |\n",
      "|    std                  | 0.454       |\n",
      "|    value_loss           | 4.34e-08    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.13       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 766         |\n",
      "|    iterations           | 322         |\n",
      "|    time_elapsed         | 4233        |\n",
      "|    total_timesteps      | 3245760     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007894272 |\n",
      "|    clip_fraction        | 0.101       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -20.2       |\n",
      "|    explained_variance   | 0.125       |\n",
      "|    learning_rate        | 0.000882    |\n",
      "|    loss                 | -0.00769    |\n",
      "|    n_updates            | 3210        |\n",
      "|    policy_gradient_loss | -0.00507    |\n",
      "|    std                  | 0.453       |\n",
      "|    value_loss           | 6.57e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3265920, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0134     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3265920     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011149128 |\n",
      "|    clip_fraction        | 0.0988      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -19.8       |\n",
      "|    explained_variance   | 0.268       |\n",
      "|    learning_rate        | 0.000872    |\n",
      "|    loss                 | -0.0113     |\n",
      "|    n_updates            | 3230        |\n",
      "|    policy_gradient_loss | -0.00825    |\n",
      "|    std                  | 0.448       |\n",
      "|    value_loss           | 6.89e-08    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.128   |\n",
      "| time/              |          |\n",
      "|    fps             | 766      |\n",
      "|    iterations      | 324      |\n",
      "|    time_elapsed    | 4262     |\n",
      "|    total_timesteps | 3265920  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.125      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 766         |\n",
      "|    iterations           | 326         |\n",
      "|    time_elapsed         | 4286        |\n",
      "|    total_timesteps      | 3286080     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008496948 |\n",
      "|    clip_fraction        | 0.0782      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -19.3       |\n",
      "|    explained_variance   | 0.392       |\n",
      "|    learning_rate        | 0.000862    |\n",
      "|    loss                 | -0.00965    |\n",
      "|    n_updates            | 3250        |\n",
      "|    policy_gradient_loss | -0.00679    |\n",
      "|    std                  | 0.444       |\n",
      "|    value_loss           | 4.26e-08    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.121      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 767         |\n",
      "|    iterations           | 328         |\n",
      "|    time_elapsed         | 4308        |\n",
      "|    total_timesteps      | 3306240     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008002102 |\n",
      "|    clip_fraction        | 0.0713      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -19         |\n",
      "|    explained_variance   | 0.27        |\n",
      "|    learning_rate        | 0.000852    |\n",
      "|    loss                 | -0.00777    |\n",
      "|    n_updates            | 3270        |\n",
      "|    policy_gradient_loss | -0.00588    |\n",
      "|    std                  | 0.44        |\n",
      "|    value_loss           | 1.19e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3326400, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.015     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3326400    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00821667 |\n",
      "|    clip_fraction        | 0.0782     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -18.6      |\n",
      "|    explained_variance   | 0.387      |\n",
      "|    learning_rate        | 0.000842   |\n",
      "|    loss                 | -0.00727   |\n",
      "|    n_updates            | 3290       |\n",
      "|    policy_gradient_loss | -0.00587   |\n",
      "|    std                  | 0.437      |\n",
      "|    value_loss           | 6.46e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.117   |\n",
      "| time/              |          |\n",
      "|    fps             | 766      |\n",
      "|    iterations      | 330      |\n",
      "|    time_elapsed    | 4337     |\n",
      "|    total_timesteps | 3326400  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.119      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 767         |\n",
      "|    iterations           | 332         |\n",
      "|    time_elapsed         | 4360        |\n",
      "|    total_timesteps      | 3346560     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009799616 |\n",
      "|    clip_fraction        | 0.0977      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -18.3       |\n",
      "|    explained_variance   | 0.22        |\n",
      "|    learning_rate        | 0.000832    |\n",
      "|    loss                 | -0.0117     |\n",
      "|    n_updates            | 3310        |\n",
      "|    policy_gradient_loss | -0.00661    |\n",
      "|    std                  | 0.434       |\n",
      "|    value_loss           | 5.7e-07     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.119      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 767         |\n",
      "|    iterations           | 334         |\n",
      "|    time_elapsed         | 4384        |\n",
      "|    total_timesteps      | 3366720     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010577142 |\n",
      "|    clip_fraction        | 0.102       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -18         |\n",
      "|    explained_variance   | 0.126       |\n",
      "|    learning_rate        | 0.000822    |\n",
      "|    loss                 | -0.00993    |\n",
      "|    n_updates            | 3330        |\n",
      "|    policy_gradient_loss | -0.006      |\n",
      "|    std                  | 0.432       |\n",
      "|    value_loss           | 4.92e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3386880, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0177     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3386880     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011045991 |\n",
      "|    clip_fraction        | 0.126       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -17.9       |\n",
      "|    explained_variance   | 0.0794      |\n",
      "|    learning_rate        | 0.000812    |\n",
      "|    loss                 | -0.00691    |\n",
      "|    n_updates            | 3350        |\n",
      "|    policy_gradient_loss | -0.00604    |\n",
      "|    std                  | 0.431       |\n",
      "|    value_loss           | 1.06e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.12    |\n",
      "| time/              |          |\n",
      "|    fps             | 767      |\n",
      "|    iterations      | 336      |\n",
      "|    time_elapsed    | 4413     |\n",
      "|    total_timesteps | 3386880  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.121      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 767         |\n",
      "|    iterations           | 338         |\n",
      "|    time_elapsed         | 4436        |\n",
      "|    total_timesteps      | 3407040     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008028895 |\n",
      "|    clip_fraction        | 0.0872      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -17.6       |\n",
      "|    explained_variance   | 0.184       |\n",
      "|    learning_rate        | 0.000802    |\n",
      "|    loss                 | -0.00368    |\n",
      "|    n_updates            | 3370        |\n",
      "|    policy_gradient_loss | -0.00489    |\n",
      "|    std                  | 0.429       |\n",
      "|    value_loss           | 3.92e-07    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.121      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 768         |\n",
      "|    iterations           | 340         |\n",
      "|    time_elapsed         | 4459        |\n",
      "|    total_timesteps      | 3427200     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008377961 |\n",
      "|    clip_fraction        | 0.0695      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -17         |\n",
      "|    explained_variance   | 0.315       |\n",
      "|    learning_rate        | 0.000791    |\n",
      "|    loss                 | -0.00673    |\n",
      "|    n_updates            | 3390        |\n",
      "|    policy_gradient_loss | -0.00623    |\n",
      "|    std                  | 0.422       |\n",
      "|    value_loss           | 5.5e-08     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3447360, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0137     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3447360     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007966229 |\n",
      "|    clip_fraction        | 0.0833      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -16.5       |\n",
      "|    explained_variance   | 0.0551      |\n",
      "|    learning_rate        | 0.000781    |\n",
      "|    loss                 | -0.00893    |\n",
      "|    n_updates            | 3410        |\n",
      "|    policy_gradient_loss | -0.00462    |\n",
      "|    std                  | 0.419       |\n",
      "|    value_loss           | 9.69e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.121   |\n",
      "| time/              |          |\n",
      "|    fps             | 768      |\n",
      "|    iterations      | 342      |\n",
      "|    time_elapsed    | 4488     |\n",
      "|    total_timesteps | 3447360  |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.26e+03     |\n",
      "|    ep_rew_mean          | -0.118       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 768          |\n",
      "|    iterations           | 344          |\n",
      "|    time_elapsed         | 4511         |\n",
      "|    total_timesteps      | 3467520      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077503454 |\n",
      "|    clip_fraction        | 0.0747       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -16.3        |\n",
      "|    explained_variance   | 0.476        |\n",
      "|    learning_rate        | 0.000771     |\n",
      "|    loss                 | -0.00781     |\n",
      "|    n_updates            | 3430         |\n",
      "|    policy_gradient_loss | -0.00651     |\n",
      "|    std                  | 0.416        |\n",
      "|    value_loss           | 3.48e-08     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.115      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 769         |\n",
      "|    iterations           | 346         |\n",
      "|    time_elapsed         | 4534        |\n",
      "|    total_timesteps      | 3487680     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008487075 |\n",
      "|    clip_fraction        | 0.0963      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -16         |\n",
      "|    explained_variance   | -0.00175    |\n",
      "|    learning_rate        | 0.000761    |\n",
      "|    loss                 | -0.00382    |\n",
      "|    n_updates            | 3450        |\n",
      "|    policy_gradient_loss | -0.00561    |\n",
      "|    std                  | 0.415       |\n",
      "|    value_loss           | 4.28e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3507840, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0143      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3507840      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074815275 |\n",
      "|    clip_fraction        | 0.0698       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -15.8        |\n",
      "|    explained_variance   | 0.235        |\n",
      "|    learning_rate        | 0.000751     |\n",
      "|    loss                 | -0.0116      |\n",
      "|    n_updates            | 3470         |\n",
      "|    policy_gradient_loss | -0.00563     |\n",
      "|    std                  | 0.413        |\n",
      "|    value_loss           | 1.02e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.113   |\n",
      "| time/              |          |\n",
      "|    fps             | 768      |\n",
      "|    iterations      | 348      |\n",
      "|    time_elapsed    | 4564     |\n",
      "|    total_timesteps | 3507840  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.113      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 769         |\n",
      "|    iterations           | 350         |\n",
      "|    time_elapsed         | 4587        |\n",
      "|    total_timesteps      | 3528000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010497976 |\n",
      "|    clip_fraction        | 0.0855      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -15.5       |\n",
      "|    explained_variance   | 0.216       |\n",
      "|    learning_rate        | 0.000741    |\n",
      "|    loss                 | -0.00666    |\n",
      "|    n_updates            | 3490        |\n",
      "|    policy_gradient_loss | -0.00476    |\n",
      "|    std                  | 0.411       |\n",
      "|    value_loss           | 8.57e-07    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.26e+03     |\n",
      "|    ep_rew_mean          | -0.112       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 769          |\n",
      "|    iterations           | 352          |\n",
      "|    time_elapsed         | 4610         |\n",
      "|    total_timesteps      | 3548160      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0086984765 |\n",
      "|    clip_fraction        | 0.0957       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -15.3        |\n",
      "|    explained_variance   | 0.152        |\n",
      "|    learning_rate        | 0.000731     |\n",
      "|    loss                 | -0.00844     |\n",
      "|    n_updates            | 3510         |\n",
      "|    policy_gradient_loss | -0.00556     |\n",
      "|    std                  | 0.409        |\n",
      "|    value_loss           | 3.33e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3568320, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0177     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3568320     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008860283 |\n",
      "|    clip_fraction        | 0.0784      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -14.8       |\n",
      "|    explained_variance   | 0.316       |\n",
      "|    learning_rate        | 0.000721    |\n",
      "|    loss                 | -0.00951    |\n",
      "|    n_updates            | 3530        |\n",
      "|    policy_gradient_loss | -0.00643    |\n",
      "|    std                  | 0.404       |\n",
      "|    value_loss           | 6.02e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.109   |\n",
      "| time/              |          |\n",
      "|    fps             | 769      |\n",
      "|    iterations      | 354      |\n",
      "|    time_elapsed    | 4639     |\n",
      "|    total_timesteps | 3568320  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.109      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 769         |\n",
      "|    iterations           | 356         |\n",
      "|    time_elapsed         | 4662        |\n",
      "|    total_timesteps      | 3588480     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008290552 |\n",
      "|    clip_fraction        | 0.0682      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -14.5       |\n",
      "|    explained_variance   | 0.648       |\n",
      "|    learning_rate        | 0.000711    |\n",
      "|    loss                 | -0.00772    |\n",
      "|    n_updates            | 3550        |\n",
      "|    policy_gradient_loss | -0.00584    |\n",
      "|    std                  | 0.402       |\n",
      "|    value_loss           | 4.25e-08    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.109      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 770         |\n",
      "|    iterations           | 358         |\n",
      "|    time_elapsed         | 4685        |\n",
      "|    total_timesteps      | 3608640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009932882 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -14.3       |\n",
      "|    explained_variance   | 0.0648      |\n",
      "|    learning_rate        | 0.000701    |\n",
      "|    loss                 | -0.00829    |\n",
      "|    n_updates            | 3570        |\n",
      "|    policy_gradient_loss | -0.00526    |\n",
      "|    std                  | 0.4         |\n",
      "|    value_loss           | 1.11e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3628800, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0148      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3628800      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074340883 |\n",
      "|    clip_fraction        | 0.0649       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -14          |\n",
      "|    explained_variance   | 0.317        |\n",
      "|    learning_rate        | 0.000691     |\n",
      "|    loss                 | -0.00787     |\n",
      "|    n_updates            | 3590         |\n",
      "|    policy_gradient_loss | -0.00564     |\n",
      "|    std                  | 0.396        |\n",
      "|    value_loss           | 9.39e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.107   |\n",
      "| time/              |          |\n",
      "|    fps             | 769      |\n",
      "|    iterations      | 360      |\n",
      "|    time_elapsed    | 4714     |\n",
      "|    total_timesteps | 3628800  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.106      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 770         |\n",
      "|    iterations           | 362         |\n",
      "|    time_elapsed         | 4737        |\n",
      "|    total_timesteps      | 3648960     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009659813 |\n",
      "|    clip_fraction        | 0.0899      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -13.7       |\n",
      "|    explained_variance   | 0.103       |\n",
      "|    learning_rate        | 0.000681    |\n",
      "|    loss                 | -0.00953    |\n",
      "|    n_updates            | 3610        |\n",
      "|    policy_gradient_loss | -0.00536    |\n",
      "|    std                  | 0.394       |\n",
      "|    value_loss           | 2.14e-07    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.26e+03     |\n",
      "|    ep_rew_mean          | -0.105       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 770          |\n",
      "|    iterations           | 364          |\n",
      "|    time_elapsed         | 4760         |\n",
      "|    total_timesteps      | 3669120      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0082390765 |\n",
      "|    clip_fraction        | 0.0704       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -13.4        |\n",
      "|    explained_variance   | 0.501        |\n",
      "|    learning_rate        | 0.00067      |\n",
      "|    loss                 | -0.00694     |\n",
      "|    n_updates            | 3630         |\n",
      "|    policy_gradient_loss | -0.00557     |\n",
      "|    std                  | 0.391        |\n",
      "|    value_loss           | 8.5e-08      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3689280, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0177      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3689280      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077037616 |\n",
      "|    clip_fraction        | 0.0674       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -13.2        |\n",
      "|    explained_variance   | 0.284        |\n",
      "|    learning_rate        | 0.00066      |\n",
      "|    loss                 | -0.0116      |\n",
      "|    n_updates            | 3650         |\n",
      "|    policy_gradient_loss | -0.00611     |\n",
      "|    std                  | 0.388        |\n",
      "|    value_loss           | 3.37e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.105   |\n",
      "| time/              |          |\n",
      "|    fps             | 770      |\n",
      "|    iterations      | 366      |\n",
      "|    time_elapsed    | 4789     |\n",
      "|    total_timesteps | 3689280  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.104      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 770         |\n",
      "|    iterations           | 368         |\n",
      "|    time_elapsed         | 4812        |\n",
      "|    total_timesteps      | 3709440     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008882162 |\n",
      "|    clip_fraction        | 0.0743      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -12.8       |\n",
      "|    explained_variance   | 0.427       |\n",
      "|    learning_rate        | 0.00065     |\n",
      "|    loss                 | -0.00777    |\n",
      "|    n_updates            | 3670        |\n",
      "|    policy_gradient_loss | -0.00577    |\n",
      "|    std                  | 0.385       |\n",
      "|    value_loss           | 8.49e-08    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.101      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 771         |\n",
      "|    iterations           | 370         |\n",
      "|    time_elapsed         | 4835        |\n",
      "|    total_timesteps      | 3729600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007419228 |\n",
      "|    clip_fraction        | 0.057       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -12.5       |\n",
      "|    explained_variance   | 0.117       |\n",
      "|    learning_rate        | 0.00064     |\n",
      "|    loss                 | -0.00624    |\n",
      "|    n_updates            | 3690        |\n",
      "|    policy_gradient_loss | -0.00496    |\n",
      "|    std                  | 0.383       |\n",
      "|    value_loss           | 1.21e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3749760, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0151      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3749760      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0086961575 |\n",
      "|    clip_fraction        | 0.061        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -12.2        |\n",
      "|    explained_variance   | 0.165        |\n",
      "|    learning_rate        | 0.00063      |\n",
      "|    loss                 | -0.00678     |\n",
      "|    n_updates            | 3710         |\n",
      "|    policy_gradient_loss | -0.00445     |\n",
      "|    std                  | 0.38         |\n",
      "|    value_loss           | 4.78e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.103   |\n",
      "| time/              |          |\n",
      "|    fps             | 770      |\n",
      "|    iterations      | 372      |\n",
      "|    time_elapsed    | 4864     |\n",
      "|    total_timesteps | 3749760  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.103      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 771         |\n",
      "|    iterations           | 374         |\n",
      "|    time_elapsed         | 4888        |\n",
      "|    total_timesteps      | 3769920     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009164872 |\n",
      "|    clip_fraction        | 0.0879      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -11.8       |\n",
      "|    explained_variance   | 0.179       |\n",
      "|    learning_rate        | 0.00062     |\n",
      "|    loss                 | -0.00648    |\n",
      "|    n_updates            | 3730        |\n",
      "|    policy_gradient_loss | -0.00655    |\n",
      "|    std                  | 0.377       |\n",
      "|    value_loss           | 1.79e-07    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.103      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 771         |\n",
      "|    iterations           | 376         |\n",
      "|    time_elapsed         | 4911        |\n",
      "|    total_timesteps      | 3790080     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009112294 |\n",
      "|    clip_fraction        | 0.0981      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -11.7       |\n",
      "|    explained_variance   | 0.118       |\n",
      "|    learning_rate        | 0.00061     |\n",
      "|    loss                 | -0.0108     |\n",
      "|    n_updates            | 3750        |\n",
      "|    policy_gradient_loss | -0.00585    |\n",
      "|    std                  | 0.376       |\n",
      "|    value_loss           | 4.82e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3810240, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.015     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3810240    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00907038 |\n",
      "|    clip_fraction        | 0.0768     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -11.5      |\n",
      "|    explained_variance   | 0.18       |\n",
      "|    learning_rate        | 0.0006     |\n",
      "|    loss                 | -0.00832   |\n",
      "|    n_updates            | 3770       |\n",
      "|    policy_gradient_loss | -0.0051    |\n",
      "|    std                  | 0.373      |\n",
      "|    value_loss           | 2.08e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.103   |\n",
      "| time/              |          |\n",
      "|    fps             | 771      |\n",
      "|    iterations      | 378      |\n",
      "|    time_elapsed    | 4940     |\n",
      "|    total_timesteps | 3810240  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.101      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 771         |\n",
      "|    iterations           | 380         |\n",
      "|    time_elapsed         | 4963        |\n",
      "|    total_timesteps      | 3830400     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008239571 |\n",
      "|    clip_fraction        | 0.0641      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -11.1       |\n",
      "|    explained_variance   | 0.356       |\n",
      "|    learning_rate        | 0.00059     |\n",
      "|    loss                 | -0.0089     |\n",
      "|    n_updates            | 3790        |\n",
      "|    policy_gradient_loss | -0.00536    |\n",
      "|    std                  | 0.371       |\n",
      "|    value_loss           | 9.75e-08    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.104      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 772         |\n",
      "|    iterations           | 382         |\n",
      "|    time_elapsed         | 4986        |\n",
      "|    total_timesteps      | 3850560     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008847756 |\n",
      "|    clip_fraction        | 0.0725      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -10.9       |\n",
      "|    explained_variance   | 0.18        |\n",
      "|    learning_rate        | 0.00058     |\n",
      "|    loss                 | -0.00431    |\n",
      "|    n_updates            | 3810        |\n",
      "|    policy_gradient_loss | -0.00423    |\n",
      "|    std                  | 0.369       |\n",
      "|    value_loss           | 4.13e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3870720, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0178     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3870720     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007637059 |\n",
      "|    clip_fraction        | 0.0652      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -10.7       |\n",
      "|    explained_variance   | 0.304       |\n",
      "|    learning_rate        | 0.00057     |\n",
      "|    loss                 | -0.00601    |\n",
      "|    n_updates            | 3830        |\n",
      "|    policy_gradient_loss | -0.0054     |\n",
      "|    std                  | 0.368       |\n",
      "|    value_loss           | 5.08e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.102   |\n",
      "| time/              |          |\n",
      "|    fps             | 771      |\n",
      "|    iterations      | 384      |\n",
      "|    time_elapsed    | 5015     |\n",
      "|    total_timesteps | 3870720  |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.26e+03   |\n",
      "|    ep_rew_mean          | -0.101     |\n",
      "| time/                   |            |\n",
      "|    fps                  | 772        |\n",
      "|    iterations           | 386        |\n",
      "|    time_elapsed         | 5038       |\n",
      "|    total_timesteps      | 3890880    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00865881 |\n",
      "|    clip_fraction        | 0.0699     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -10.3      |\n",
      "|    explained_variance   | 0.379      |\n",
      "|    learning_rate        | 0.00056    |\n",
      "|    loss                 | -0.0141    |\n",
      "|    n_updates            | 3850       |\n",
      "|    policy_gradient_loss | -0.0068    |\n",
      "|    std                  | 0.365      |\n",
      "|    value_loss           | 2.63e-08   |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.26e+03     |\n",
      "|    ep_rew_mean          | -0.0997      |\n",
      "| time/                   |              |\n",
      "|    fps                  | 772          |\n",
      "|    iterations           | 388          |\n",
      "|    time_elapsed         | 5061         |\n",
      "|    total_timesteps      | 3911040      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066142073 |\n",
      "|    clip_fraction        | 0.0609       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -10          |\n",
      "|    explained_variance   | 0.0693       |\n",
      "|    learning_rate        | 0.00055      |\n",
      "|    loss                 | -0.00563     |\n",
      "|    n_updates            | 3870         |\n",
      "|    policy_gradient_loss | -0.00413     |\n",
      "|    std                  | 0.363        |\n",
      "|    value_loss           | 5.02e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3931200, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0173      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3931200      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076195975 |\n",
      "|    clip_fraction        | 0.0654       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -9.85        |\n",
      "|    explained_variance   | 0.184        |\n",
      "|    learning_rate        | 0.000539     |\n",
      "|    loss                 | -0.00812     |\n",
      "|    n_updates            | 3890         |\n",
      "|    policy_gradient_loss | -0.00514     |\n",
      "|    std                  | 0.362        |\n",
      "|    value_loss           | 2.25e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0984  |\n",
      "| time/              |          |\n",
      "|    fps             | 772      |\n",
      "|    iterations      | 390      |\n",
      "|    time_elapsed    | 5091     |\n",
      "|    total_timesteps | 3931200  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0978     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 772         |\n",
      "|    iterations           | 392         |\n",
      "|    time_elapsed         | 5114        |\n",
      "|    total_timesteps      | 3951360     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007906204 |\n",
      "|    clip_fraction        | 0.0699      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -9.57       |\n",
      "|    explained_variance   | 0.178       |\n",
      "|    learning_rate        | 0.000529    |\n",
      "|    loss                 | -0.00655    |\n",
      "|    n_updates            | 3910        |\n",
      "|    policy_gradient_loss | -0.0052     |\n",
      "|    std                  | 0.36        |\n",
      "|    value_loss           | 9.19e-08    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0956     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 772         |\n",
      "|    iterations           | 394         |\n",
      "|    time_elapsed         | 5137        |\n",
      "|    total_timesteps      | 3971520     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008153327 |\n",
      "|    clip_fraction        | 0.0699      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -9.45       |\n",
      "|    explained_variance   | 0.219       |\n",
      "|    learning_rate        | 0.000519    |\n",
      "|    loss                 | -0.00867    |\n",
      "|    n_updates            | 3930        |\n",
      "|    policy_gradient_loss | -0.00482    |\n",
      "|    std                  | 0.358       |\n",
      "|    value_loss           | 8.5e-08     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3991680, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0211      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3991680      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074160257 |\n",
      "|    clip_fraction        | 0.0568       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -9.23        |\n",
      "|    explained_variance   | 0.126        |\n",
      "|    learning_rate        | 0.000509     |\n",
      "|    loss                 | -0.00763     |\n",
      "|    n_updates            | 3950         |\n",
      "|    policy_gradient_loss | -0.0041      |\n",
      "|    std                  | 0.356        |\n",
      "|    value_loss           | 8.79e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0943  |\n",
      "| time/              |          |\n",
      "|    fps             | 772      |\n",
      "|    iterations      | 396      |\n",
      "|    time_elapsed    | 5166     |\n",
      "|    total_timesteps | 3991680  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0944     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 772         |\n",
      "|    iterations           | 398         |\n",
      "|    time_elapsed         | 5190        |\n",
      "|    total_timesteps      | 4011840     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009778038 |\n",
      "|    clip_fraction        | 0.0763      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -8.94       |\n",
      "|    explained_variance   | 0.0559      |\n",
      "|    learning_rate        | 0.000499    |\n",
      "|    loss                 | -0.0064     |\n",
      "|    n_updates            | 3970        |\n",
      "|    policy_gradient_loss | -0.00433    |\n",
      "|    std                  | 0.354       |\n",
      "|    value_loss           | 5.57e-07    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0956     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 773         |\n",
      "|    iterations           | 400         |\n",
      "|    time_elapsed         | 5212        |\n",
      "|    total_timesteps      | 4032000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009534051 |\n",
      "|    clip_fraction        | 0.0794      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -8.75       |\n",
      "|    explained_variance   | 0.0565      |\n",
      "|    learning_rate        | 0.000489    |\n",
      "|    loss                 | -0.00627    |\n",
      "|    n_updates            | 3990        |\n",
      "|    policy_gradient_loss | -0.00453    |\n",
      "|    std                  | 0.353       |\n",
      "|    value_loss           | 9.6e-07     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4052160, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0171     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4052160     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009095514 |\n",
      "|    clip_fraction        | 0.0845      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -8.58       |\n",
      "|    explained_variance   | 0.0736      |\n",
      "|    learning_rate        | 0.000479    |\n",
      "|    loss                 | -0.00913    |\n",
      "|    n_updates            | 4010        |\n",
      "|    policy_gradient_loss | -0.00529    |\n",
      "|    std                  | 0.351       |\n",
      "|    value_loss           | 4.9e-07     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0942  |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 402      |\n",
      "|    time_elapsed    | 5242     |\n",
      "|    total_timesteps | 4052160  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.097      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 773         |\n",
      "|    iterations           | 404         |\n",
      "|    time_elapsed         | 5265        |\n",
      "|    total_timesteps      | 4072320     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009911629 |\n",
      "|    clip_fraction        | 0.065       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -8.45       |\n",
      "|    explained_variance   | 0.0945      |\n",
      "|    learning_rate        | 0.000469    |\n",
      "|    loss                 | -0.00704    |\n",
      "|    n_updates            | 4030        |\n",
      "|    policy_gradient_loss | -0.00426    |\n",
      "|    std                  | 0.35        |\n",
      "|    value_loss           | 1.09e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0987     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 773         |\n",
      "|    iterations           | 406         |\n",
      "|    time_elapsed         | 5288        |\n",
      "|    total_timesteps      | 4092480     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008122638 |\n",
      "|    clip_fraction        | 0.0626      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -8.35       |\n",
      "|    explained_variance   | 0.104       |\n",
      "|    learning_rate        | 0.000459    |\n",
      "|    loss                 | -0.0075     |\n",
      "|    n_updates            | 4050        |\n",
      "|    policy_gradient_loss | -0.00469    |\n",
      "|    std                  | 0.35        |\n",
      "|    value_loss           | 3.12e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4112640, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0155     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4112640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008234097 |\n",
      "|    clip_fraction        | 0.0547      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.0785      |\n",
      "|    learning_rate        | 0.000449    |\n",
      "|    loss                 | -0.00745    |\n",
      "|    n_updates            | 4070        |\n",
      "|    policy_gradient_loss | -0.00382    |\n",
      "|    std                  | 0.349       |\n",
      "|    value_loss           | 7.48e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.101   |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 408      |\n",
      "|    time_elapsed    | 5317     |\n",
      "|    total_timesteps | 4112640  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.101      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 773         |\n",
      "|    iterations           | 410         |\n",
      "|    time_elapsed         | 5340        |\n",
      "|    total_timesteps      | 4132800     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007552854 |\n",
      "|    clip_fraction        | 0.0583      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -8.04       |\n",
      "|    explained_variance   | 0.373       |\n",
      "|    learning_rate        | 0.000439    |\n",
      "|    loss                 | -0.00791    |\n",
      "|    n_updates            | 4090        |\n",
      "|    policy_gradient_loss | -0.00568    |\n",
      "|    std                  | 0.347       |\n",
      "|    value_loss           | 2.91e-08    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.097      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 774         |\n",
      "|    iterations           | 412         |\n",
      "|    time_elapsed         | 5363        |\n",
      "|    total_timesteps      | 4152960     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007538392 |\n",
      "|    clip_fraction        | 0.0549      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.79       |\n",
      "|    explained_variance   | 0.201       |\n",
      "|    learning_rate        | 0.000429    |\n",
      "|    loss                 | -0.0101     |\n",
      "|    n_updates            | 4110        |\n",
      "|    policy_gradient_loss | -0.00541    |\n",
      "|    std                  | 0.344       |\n",
      "|    value_loss           | 3.3e-08     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4173120, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0144     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4173120     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008309955 |\n",
      "|    clip_fraction        | 0.066       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.49       |\n",
      "|    explained_variance   | 0.274       |\n",
      "|    learning_rate        | 0.000418    |\n",
      "|    loss                 | -0.00906    |\n",
      "|    n_updates            | 4130        |\n",
      "|    policy_gradient_loss | -0.00482    |\n",
      "|    std                  | 0.343       |\n",
      "|    value_loss           | 1.3e-07     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0963  |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 414      |\n",
      "|    time_elapsed    | 5392     |\n",
      "|    total_timesteps | 4173120  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0947     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 774         |\n",
      "|    iterations           | 416         |\n",
      "|    time_elapsed         | 5415        |\n",
      "|    total_timesteps      | 4193280     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009339706 |\n",
      "|    clip_fraction        | 0.0705      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.3        |\n",
      "|    explained_variance   | 0.161       |\n",
      "|    learning_rate        | 0.000408    |\n",
      "|    loss                 | -0.00654    |\n",
      "|    n_updates            | 4150        |\n",
      "|    policy_gradient_loss | -0.00465    |\n",
      "|    std                  | 0.341       |\n",
      "|    value_loss           | 3.49e-07    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0936     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 774         |\n",
      "|    iterations           | 418         |\n",
      "|    time_elapsed         | 5438        |\n",
      "|    total_timesteps      | 4213440     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008452106 |\n",
      "|    clip_fraction        | 0.0559      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.14       |\n",
      "|    explained_variance   | 0.0504      |\n",
      "|    learning_rate        | 0.000398    |\n",
      "|    loss                 | -0.00484    |\n",
      "|    n_updates            | 4170        |\n",
      "|    policy_gradient_loss | -0.00314    |\n",
      "|    std                  | 0.34        |\n",
      "|    value_loss           | 7.74e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4233600, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.014       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4233600      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074945344 |\n",
      "|    clip_fraction        | 0.0563       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.02        |\n",
      "|    explained_variance   | 0.165        |\n",
      "|    learning_rate        | 0.000388     |\n",
      "|    loss                 | -0.00744     |\n",
      "|    n_updates            | 4190         |\n",
      "|    policy_gradient_loss | -0.0045      |\n",
      "|    std                  | 0.339        |\n",
      "|    value_loss           | 3.42e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0909  |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 420      |\n",
      "|    time_elapsed    | 5468     |\n",
      "|    total_timesteps | 4233600  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0935     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 774         |\n",
      "|    iterations           | 422         |\n",
      "|    time_elapsed         | 5491        |\n",
      "|    total_timesteps      | 4253760     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010709842 |\n",
      "|    clip_fraction        | 0.0631      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.95       |\n",
      "|    explained_variance   | 0.0218      |\n",
      "|    learning_rate        | 0.000378    |\n",
      "|    loss                 | -0.00386    |\n",
      "|    n_updates            | 4210        |\n",
      "|    policy_gradient_loss | -0.00362    |\n",
      "|    std                  | 0.339       |\n",
      "|    value_loss           | 3.79e-06    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.26e+03     |\n",
      "|    ep_rew_mean          | -0.0942      |\n",
      "| time/                   |              |\n",
      "|    fps                  | 775          |\n",
      "|    iterations           | 424          |\n",
      "|    time_elapsed         | 5514         |\n",
      "|    total_timesteps      | 4273920      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071377195 |\n",
      "|    clip_fraction        | 0.0559       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -6.76        |\n",
      "|    explained_variance   | 0.207        |\n",
      "|    learning_rate        | 0.000368     |\n",
      "|    loss                 | -0.00904     |\n",
      "|    n_updates            | 4230         |\n",
      "|    policy_gradient_loss | -0.00546     |\n",
      "|    std                  | 0.337        |\n",
      "|    value_loss           | 1.24e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4294080, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.017      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4294080     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011316087 |\n",
      "|    clip_fraction        | 0.0744      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.55       |\n",
      "|    explained_variance   | 0.0618      |\n",
      "|    learning_rate        | 0.000358    |\n",
      "|    loss                 | -0.00749    |\n",
      "|    n_updates            | 4250        |\n",
      "|    policy_gradient_loss | -0.00499    |\n",
      "|    std                  | 0.336       |\n",
      "|    value_loss           | 5.55e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.095   |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 426      |\n",
      "|    time_elapsed    | 5543     |\n",
      "|    total_timesteps | 4294080  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0961     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 775         |\n",
      "|    iterations           | 428         |\n",
      "|    time_elapsed         | 5566        |\n",
      "|    total_timesteps      | 4314240     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007939824 |\n",
      "|    clip_fraction        | 0.06        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.45       |\n",
      "|    explained_variance   | 0.0696      |\n",
      "|    learning_rate        | 0.000348    |\n",
      "|    loss                 | -0.00705    |\n",
      "|    n_updates            | 4270        |\n",
      "|    policy_gradient_loss | -0.00485    |\n",
      "|    std                  | 0.335       |\n",
      "|    value_loss           | 7.48e-07    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.096      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 775         |\n",
      "|    iterations           | 430         |\n",
      "|    time_elapsed         | 5589        |\n",
      "|    total_timesteps      | 4334400     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007014443 |\n",
      "|    clip_fraction        | 0.0644      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.37       |\n",
      "|    explained_variance   | 0.0424      |\n",
      "|    learning_rate        | 0.000338    |\n",
      "|    loss                 | -0.00112    |\n",
      "|    n_updates            | 4290        |\n",
      "|    policy_gradient_loss | -0.00394    |\n",
      "|    std                  | 0.334       |\n",
      "|    value_loss           | 7.76e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4354560, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0193      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4354560      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077715437 |\n",
      "|    clip_fraction        | 0.0631       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -6.19        |\n",
      "|    explained_variance   | 0.121        |\n",
      "|    learning_rate        | 0.000328     |\n",
      "|    loss                 | -0.00718     |\n",
      "|    n_updates            | 4310         |\n",
      "|    policy_gradient_loss | -0.00425     |\n",
      "|    std                  | 0.332        |\n",
      "|    value_loss           | 9.72e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0948  |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 432      |\n",
      "|    time_elapsed    | 5618     |\n",
      "|    total_timesteps | 4354560  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0925     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 775         |\n",
      "|    iterations           | 434         |\n",
      "|    time_elapsed         | 5642        |\n",
      "|    total_timesteps      | 4374720     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006020096 |\n",
      "|    clip_fraction        | 0.0449      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.99       |\n",
      "|    explained_variance   | 0.294       |\n",
      "|    learning_rate        | 0.000318    |\n",
      "|    loss                 | -0.0116     |\n",
      "|    n_updates            | 4330        |\n",
      "|    policy_gradient_loss | -0.00508    |\n",
      "|    std                  | 0.331       |\n",
      "|    value_loss           | 3.34e-08    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0889     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 775         |\n",
      "|    iterations           | 436         |\n",
      "|    time_elapsed         | 5665        |\n",
      "|    total_timesteps      | 4394880     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007879887 |\n",
      "|    clip_fraction        | 0.0559      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.73       |\n",
      "|    explained_variance   | 0.419       |\n",
      "|    learning_rate        | 0.000308    |\n",
      "|    loss                 | -0.00937    |\n",
      "|    n_updates            | 4350        |\n",
      "|    policy_gradient_loss | -0.0059     |\n",
      "|    std                  | 0.328       |\n",
      "|    value_loss           | 2.52e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4415040, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0163     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4415040     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009049155 |\n",
      "|    clip_fraction        | 0.0507      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.56       |\n",
      "|    explained_variance   | 0.131       |\n",
      "|    learning_rate        | 0.000298    |\n",
      "|    loss                 | -0.00641    |\n",
      "|    n_updates            | 4370        |\n",
      "|    policy_gradient_loss | -0.00364    |\n",
      "|    std                  | 0.327       |\n",
      "|    value_loss           | 4.64e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0881  |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 438      |\n",
      "|    time_elapsed    | 5694     |\n",
      "|    total_timesteps | 4415040  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0871     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 775         |\n",
      "|    iterations           | 440         |\n",
      "|    time_elapsed         | 5717        |\n",
      "|    total_timesteps      | 4435200     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008978356 |\n",
      "|    clip_fraction        | 0.0603      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.37       |\n",
      "|    explained_variance   | 0.257       |\n",
      "|    learning_rate        | 0.000287    |\n",
      "|    loss                 | -0.0103     |\n",
      "|    n_updates            | 4390        |\n",
      "|    policy_gradient_loss | -0.00427    |\n",
      "|    std                  | 0.326       |\n",
      "|    value_loss           | 1.27e-07    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0842     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 776         |\n",
      "|    iterations           | 442         |\n",
      "|    time_elapsed         | 5740        |\n",
      "|    total_timesteps      | 4455360     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006935195 |\n",
      "|    clip_fraction        | 0.0513      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.24       |\n",
      "|    explained_variance   | 0.341       |\n",
      "|    learning_rate        | 0.000277    |\n",
      "|    loss                 | -0.00844    |\n",
      "|    n_updates            | 4410        |\n",
      "|    policy_gradient_loss | -0.00512    |\n",
      "|    std                  | 0.325       |\n",
      "|    value_loss           | 2.53e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4475520, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0143     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4475520     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006197013 |\n",
      "|    clip_fraction        | 0.0356      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.01       |\n",
      "|    explained_variance   | 0.362       |\n",
      "|    learning_rate        | 0.000267    |\n",
      "|    loss                 | -0.00472    |\n",
      "|    n_updates            | 4430        |\n",
      "|    policy_gradient_loss | -0.00443    |\n",
      "|    std                  | 0.323       |\n",
      "|    value_loss           | 2.32e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0846  |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 444      |\n",
      "|    time_elapsed    | 5769     |\n",
      "|    total_timesteps | 4475520  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0858     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 776         |\n",
      "|    iterations           | 446         |\n",
      "|    time_elapsed         | 5792        |\n",
      "|    total_timesteps      | 4495680     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004889959 |\n",
      "|    clip_fraction        | 0.0208      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.87       |\n",
      "|    explained_variance   | 0.107       |\n",
      "|    learning_rate        | 0.000257    |\n",
      "|    loss                 | -0.00438    |\n",
      "|    n_updates            | 4450        |\n",
      "|    policy_gradient_loss | -0.00281    |\n",
      "|    std                  | 0.323       |\n",
      "|    value_loss           | 1.75e-07    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0867     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 776         |\n",
      "|    iterations           | 448         |\n",
      "|    time_elapsed         | 5815        |\n",
      "|    total_timesteps      | 4515840     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007464667 |\n",
      "|    clip_fraction        | 0.0435      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.78       |\n",
      "|    explained_variance   | 0.11        |\n",
      "|    learning_rate        | 0.000247    |\n",
      "|    loss                 | -0.00926    |\n",
      "|    n_updates            | 4470        |\n",
      "|    policy_gradient_loss | -0.00396    |\n",
      "|    std                  | 0.322       |\n",
      "|    value_loss           | 1.38e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4536000, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0161      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4536000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058165826 |\n",
      "|    clip_fraction        | 0.0395       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4.64        |\n",
      "|    explained_variance   | 0.276        |\n",
      "|    learning_rate        | 0.000237     |\n",
      "|    loss                 | -0.00854     |\n",
      "|    n_updates            | 4490         |\n",
      "|    policy_gradient_loss | -0.00402     |\n",
      "|    std                  | 0.321        |\n",
      "|    value_loss           | 2.94e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0853  |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 450      |\n",
      "|    time_elapsed    | 5845     |\n",
      "|    total_timesteps | 4536000  |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.26e+03   |\n",
      "|    ep_rew_mean          | -0.0842    |\n",
      "| time/                   |            |\n",
      "|    fps                  | 776        |\n",
      "|    iterations           | 452        |\n",
      "|    time_elapsed         | 5868       |\n",
      "|    total_timesteps      | 4556160    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00683157 |\n",
      "|    clip_fraction        | 0.045      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -4.52      |\n",
      "|    explained_variance   | 0.255      |\n",
      "|    learning_rate        | 0.000227   |\n",
      "|    loss                 | -0.00846   |\n",
      "|    n_updates            | 4510       |\n",
      "|    policy_gradient_loss | -0.00488   |\n",
      "|    std                  | 0.321      |\n",
      "|    value_loss           | 2.99e-08   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0856     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 776         |\n",
      "|    iterations           | 454         |\n",
      "|    time_elapsed         | 5891        |\n",
      "|    total_timesteps      | 4576320     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006518133 |\n",
      "|    clip_fraction        | 0.0362      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.39       |\n",
      "|    explained_variance   | 0.18        |\n",
      "|    learning_rate        | 0.000217    |\n",
      "|    loss                 | -0.00502    |\n",
      "|    n_updates            | 4530        |\n",
      "|    policy_gradient_loss | -0.00381    |\n",
      "|    std                  | 0.319       |\n",
      "|    value_loss           | 1.12e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4596480, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.018       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4596480      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074226866 |\n",
      "|    clip_fraction        | 0.0388       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4.26        |\n",
      "|    explained_variance   | 0.497        |\n",
      "|    learning_rate        | 0.000207     |\n",
      "|    loss                 | -0.00783     |\n",
      "|    n_updates            | 4550         |\n",
      "|    policy_gradient_loss | -0.0041      |\n",
      "|    std                  | 0.318        |\n",
      "|    value_loss           | 6.36e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0864  |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 456      |\n",
      "|    time_elapsed    | 5920     |\n",
      "|    total_timesteps | 4596480  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0849     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 776         |\n",
      "|    iterations           | 458         |\n",
      "|    time_elapsed         | 5943        |\n",
      "|    total_timesteps      | 4616640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007942002 |\n",
      "|    clip_fraction        | 0.0389      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.27       |\n",
      "|    explained_variance   | 0.0451      |\n",
      "|    learning_rate        | 0.000197    |\n",
      "|    loss                 | -0.00594    |\n",
      "|    n_updates            | 4570        |\n",
      "|    policy_gradient_loss | -0.00342    |\n",
      "|    std                  | 0.319       |\n",
      "|    value_loss           | 5.7e-07     |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.26e+03     |\n",
      "|    ep_rew_mean          | -0.0857      |\n",
      "| time/                   |              |\n",
      "|    fps                  | 777          |\n",
      "|    iterations           | 460          |\n",
      "|    time_elapsed         | 5966         |\n",
      "|    total_timesteps      | 4636800      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067006545 |\n",
      "|    clip_fraction        | 0.0389       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4.22        |\n",
      "|    explained_variance   | 0.211        |\n",
      "|    learning_rate        | 0.000187     |\n",
      "|    loss                 | -0.00287     |\n",
      "|    n_updates            | 4590         |\n",
      "|    policy_gradient_loss | -0.00352     |\n",
      "|    std                  | 0.318        |\n",
      "|    value_loss           | 1.79e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4656960, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0195      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4656960      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068137604 |\n",
      "|    clip_fraction        | 0.0229       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4.17        |\n",
      "|    explained_variance   | 0.358        |\n",
      "|    learning_rate        | 0.000177     |\n",
      "|    loss                 | -0.00914     |\n",
      "|    n_updates            | 4610         |\n",
      "|    policy_gradient_loss | -0.0044      |\n",
      "|    std                  | 0.318        |\n",
      "|    value_loss           | 1.55e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0868  |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 462      |\n",
      "|    time_elapsed    | 5996     |\n",
      "|    total_timesteps | 4656960  |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.26e+03     |\n",
      "|    ep_rew_mean          | -0.0867      |\n",
      "| time/                   |              |\n",
      "|    fps                  | 776          |\n",
      "|    iterations           | 464          |\n",
      "|    time_elapsed         | 6019         |\n",
      "|    total_timesteps      | 4677120      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051585236 |\n",
      "|    clip_fraction        | 0.0136       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4.13        |\n",
      "|    explained_variance   | 0.434        |\n",
      "|    learning_rate        | 0.000166     |\n",
      "|    loss                 | -0.00587     |\n",
      "|    n_updates            | 4630         |\n",
      "|    policy_gradient_loss | -0.00296     |\n",
      "|    std                  | 0.318        |\n",
      "|    value_loss           | 4.25e-08     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0869     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 777         |\n",
      "|    iterations           | 466         |\n",
      "|    time_elapsed         | 6042        |\n",
      "|    total_timesteps      | 4697280     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005641125 |\n",
      "|    clip_fraction        | 0.0133      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.02       |\n",
      "|    explained_variance   | 0.116       |\n",
      "|    learning_rate        | 0.000156    |\n",
      "|    loss                 | -0.00554    |\n",
      "|    n_updates            | 4650        |\n",
      "|    policy_gradient_loss | -0.00257    |\n",
      "|    std                  | 0.317       |\n",
      "|    value_loss           | 2.04e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4717440, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0148     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4717440     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004962253 |\n",
      "|    clip_fraction        | 0.0276      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.99       |\n",
      "|    explained_variance   | 0.187       |\n",
      "|    learning_rate        | 0.000146    |\n",
      "|    loss                 | -0.00413    |\n",
      "|    n_updates            | 4670        |\n",
      "|    policy_gradient_loss | -0.0023     |\n",
      "|    std                  | 0.317       |\n",
      "|    value_loss           | 3.24e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0885  |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 468      |\n",
      "|    time_elapsed    | 6071     |\n",
      "|    total_timesteps | 4717440  |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.26e+03     |\n",
      "|    ep_rew_mean          | -0.0857      |\n",
      "| time/                   |              |\n",
      "|    fps                  | 777          |\n",
      "|    iterations           | 470          |\n",
      "|    time_elapsed         | 6095         |\n",
      "|    total_timesteps      | 4737600      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044733062 |\n",
      "|    clip_fraction        | 0.0171       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.94        |\n",
      "|    explained_variance   | 0.34         |\n",
      "|    learning_rate        | 0.000136     |\n",
      "|    loss                 | -0.00504     |\n",
      "|    n_updates            | 4690         |\n",
      "|    policy_gradient_loss | -0.0031      |\n",
      "|    std                  | 0.316        |\n",
      "|    value_loss           | 2.07e-08     |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.26e+03   |\n",
      "|    ep_rew_mean          | -0.0834    |\n",
      "| time/                   |            |\n",
      "|    fps                  | 777        |\n",
      "|    iterations           | 472        |\n",
      "|    time_elapsed         | 6118       |\n",
      "|    total_timesteps      | 4757760    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00512188 |\n",
      "|    clip_fraction        | 0.0177     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -3.8       |\n",
      "|    explained_variance   | 0.43       |\n",
      "|    learning_rate        | 0.000126   |\n",
      "|    loss                 | -0.00629   |\n",
      "|    n_updates            | 4710       |\n",
      "|    policy_gradient_loss | -0.00337   |\n",
      "|    std                  | 0.315      |\n",
      "|    value_loss           | 2.07e-08   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=4777920, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0164      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4777920      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053734654 |\n",
      "|    clip_fraction        | 0.00917      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.72        |\n",
      "|    explained_variance   | 0.1          |\n",
      "|    learning_rate        | 0.000116     |\n",
      "|    loss                 | -0.0023      |\n",
      "|    n_updates            | 4730         |\n",
      "|    policy_gradient_loss | -0.0028      |\n",
      "|    std                  | 0.315        |\n",
      "|    value_loss           | 2.78e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0836  |\n",
      "| time/              |          |\n",
      "|    fps             | 777      |\n",
      "|    iterations      | 474      |\n",
      "|    time_elapsed    | 6147     |\n",
      "|    total_timesteps | 4777920  |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.26e+03     |\n",
      "|    ep_rew_mean          | -0.0833      |\n",
      "| time/                   |              |\n",
      "|    fps                  | 777          |\n",
      "|    iterations           | 476          |\n",
      "|    time_elapsed         | 6170         |\n",
      "|    total_timesteps      | 4798080      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053785346 |\n",
      "|    clip_fraction        | 0.0366       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.69        |\n",
      "|    explained_variance   | 0.408        |\n",
      "|    learning_rate        | 0.000106     |\n",
      "|    loss                 | -0.00728     |\n",
      "|    n_updates            | 4750         |\n",
      "|    policy_gradient_loss | -0.00447     |\n",
      "|    std                  | 0.315        |\n",
      "|    value_loss           | 6.15e-08     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.26e+03     |\n",
      "|    ep_rew_mean          | -0.083       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 777          |\n",
      "|    iterations           | 478          |\n",
      "|    time_elapsed         | 6193         |\n",
      "|    total_timesteps      | 4818240      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038133275 |\n",
      "|    clip_fraction        | 0.00643      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.66        |\n",
      "|    explained_variance   | 0.174        |\n",
      "|    learning_rate        | 9.59e-05     |\n",
      "|    loss                 | -0.00334     |\n",
      "|    n_updates            | 4770         |\n",
      "|    policy_gradient_loss | -0.00235     |\n",
      "|    std                  | 0.315        |\n",
      "|    value_loss           | 1.56e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4838400, episode_reward=-0.03 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0266      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4838400      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038096362 |\n",
      "|    clip_fraction        | 0.00898      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.61        |\n",
      "|    explained_variance   | 0.122        |\n",
      "|    learning_rate        | 8.58e-05     |\n",
      "|    loss                 | -0.00351     |\n",
      "|    n_updates            | 4790         |\n",
      "|    policy_gradient_loss | -0.00203     |\n",
      "|    std                  | 0.314        |\n",
      "|    value_loss           | 9.32e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.08    |\n",
      "| time/              |          |\n",
      "|    fps             | 777      |\n",
      "|    iterations      | 480      |\n",
      "|    time_elapsed    | 6222     |\n",
      "|    total_timesteps | 4838400  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0805     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 777         |\n",
      "|    iterations           | 482         |\n",
      "|    time_elapsed         | 6245        |\n",
      "|    total_timesteps      | 4858560     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004809376 |\n",
      "|    clip_fraction        | 0.00707     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.56       |\n",
      "|    explained_variance   | 0.336       |\n",
      "|    learning_rate        | 7.58e-05    |\n",
      "|    loss                 | -0.00589    |\n",
      "|    n_updates            | 4810        |\n",
      "|    policy_gradient_loss | -0.0026     |\n",
      "|    std                  | 0.314       |\n",
      "|    value_loss           | 1.05e-07    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.26e+03     |\n",
      "|    ep_rew_mean          | -0.0808      |\n",
      "| time/                   |              |\n",
      "|    fps                  | 778          |\n",
      "|    iterations           | 484          |\n",
      "|    time_elapsed         | 6268         |\n",
      "|    total_timesteps      | 4878720      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044928715 |\n",
      "|    clip_fraction        | 0.0208       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.53        |\n",
      "|    explained_variance   | 0.0786       |\n",
      "|    learning_rate        | 6.57e-05     |\n",
      "|    loss                 | -0.00617     |\n",
      "|    n_updates            | 4830         |\n",
      "|    policy_gradient_loss | -0.00223     |\n",
      "|    std                  | 0.314        |\n",
      "|    value_loss           | 1.5e-07      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4898880, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0183      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4898880      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034552806 |\n",
      "|    clip_fraction        | 0.0288       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.49        |\n",
      "|    explained_variance   | 0.215        |\n",
      "|    learning_rate        | 5.56e-05     |\n",
      "|    loss                 | -0.00608     |\n",
      "|    n_updates            | 4850         |\n",
      "|    policy_gradient_loss | -0.0021      |\n",
      "|    std                  | 0.313        |\n",
      "|    value_loss           | 1.68e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0818  |\n",
      "| time/              |          |\n",
      "|    fps             | 777      |\n",
      "|    iterations      | 486      |\n",
      "|    time_elapsed    | 6297     |\n",
      "|    total_timesteps | 4898880  |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1.26e+03      |\n",
      "|    ep_rew_mean          | -0.0803       |\n",
      "| time/                   |               |\n",
      "|    fps                  | 778           |\n",
      "|    iterations           | 488           |\n",
      "|    time_elapsed         | 6320          |\n",
      "|    total_timesteps      | 4919040       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00095502695 |\n",
      "|    clip_fraction        | 9.92e-06      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -3.46         |\n",
      "|    explained_variance   | 0.459         |\n",
      "|    learning_rate        | 4.55e-05      |\n",
      "|    loss                 | -0.000966     |\n",
      "|    n_updates            | 4870          |\n",
      "|    policy_gradient_loss | -0.00109      |\n",
      "|    std                  | 0.313         |\n",
      "|    value_loss           | 2.15e-08      |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0795     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 778         |\n",
      "|    iterations           | 490         |\n",
      "|    time_elapsed         | 6343        |\n",
      "|    total_timesteps      | 4939200     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002164894 |\n",
      "|    clip_fraction        | 0.00254     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.43       |\n",
      "|    explained_variance   | 0.432       |\n",
      "|    learning_rate        | 3.54e-05    |\n",
      "|    loss                 | -0.00417    |\n",
      "|    n_updates            | 4890        |\n",
      "|    policy_gradient_loss | -0.00143    |\n",
      "|    std                  | 0.313       |\n",
      "|    value_loss           | 1.6e-08     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4959360, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0172     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4959360     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003984851 |\n",
      "|    clip_fraction        | 0.00674     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.41       |\n",
      "|    explained_variance   | 0.275       |\n",
      "|    learning_rate        | 2.54e-05    |\n",
      "|    loss                 | -0.000128   |\n",
      "|    n_updates            | 4910        |\n",
      "|    policy_gradient_loss | -0.00116    |\n",
      "|    std                  | 0.313       |\n",
      "|    value_loss           | 1.12e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.081   |\n",
      "| time/              |          |\n",
      "|    fps             | 778      |\n",
      "|    iterations      | 492      |\n",
      "|    time_elapsed    | 6373     |\n",
      "|    total_timesteps | 4959360  |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.26e+03     |\n",
      "|    ep_rew_mean          | -0.081       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 778          |\n",
      "|    iterations           | 494          |\n",
      "|    time_elapsed         | 6396         |\n",
      "|    total_timesteps      | 4979520      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007988292 |\n",
      "|    clip_fraction        | 1.98e-05     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.4         |\n",
      "|    explained_variance   | 0.084        |\n",
      "|    learning_rate        | 1.53e-05     |\n",
      "|    loss                 | -0.000649    |\n",
      "|    n_updates            | 4930         |\n",
      "|    policy_gradient_loss | -0.000383    |\n",
      "|    std                  | 0.313        |\n",
      "|    value_loss           | 3.81e-07     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1.26e+03      |\n",
      "|    ep_rew_mean          | -0.0809       |\n",
      "| time/                   |               |\n",
      "|    fps                  | 778           |\n",
      "|    iterations           | 496           |\n",
      "|    time_elapsed         | 6419          |\n",
      "|    total_timesteps      | 4999680       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00030121394 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -3.4          |\n",
      "|    explained_variance   | 0.263         |\n",
      "|    learning_rate        | 5.2e-06       |\n",
      "|    loss                 | -0.000934     |\n",
      "|    n_updates            | 4950          |\n",
      "|    policy_gradient_loss | -0.000271     |\n",
      "|    std                  | 0.313         |\n",
      "|    value_loss           | 3.24e-08      |\n",
      "-------------------------------------------\n",
      "Using cpu device\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -7.74       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1031        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 19          |\n",
      "|    total_timesteps      | 20160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010821441 |\n",
      "|    clip_fraction        | 0.0537      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.19       |\n",
      "|    explained_variance   | 0.0144      |\n",
      "|    learning_rate        | 0.00249     |\n",
      "|    loss                 | -0.00619    |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00415    |\n",
      "|    std                  | 0.96        |\n",
      "|    value_loss           | 0.00148     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -7.13       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 959         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 42          |\n",
      "|    total_timesteps      | 40320       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012774747 |\n",
      "|    clip_fraction        | 0.0489      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.92       |\n",
      "|    explained_variance   | 0.0355      |\n",
      "|    learning_rate        | 0.00248     |\n",
      "|    loss                 | -0.00736    |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00579    |\n",
      "|    std                  | 0.877       |\n",
      "|    value_loss           | 0.000987    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=60480, episode_reward=-0.65 +/- 0.17\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.649       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 60480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062061464 |\n",
      "|    clip_fraction        | 0.0241       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.65        |\n",
      "|    explained_variance   | 0.0802       |\n",
      "|    learning_rate        | 0.00247      |\n",
      "|    loss                 | 0.00359      |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00208     |\n",
      "|    std                  | 0.814        |\n",
      "|    value_loss           | 0.00524      |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -7.16    |\n",
      "| time/              |          |\n",
      "|    fps             | 857      |\n",
      "|    iterations      | 6        |\n",
      "|    time_elapsed    | 70       |\n",
      "|    total_timesteps | 60480    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -6.66       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 863         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 93          |\n",
      "|    total_timesteps      | 80640       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016764568 |\n",
      "|    clip_fraction        | 0.0644      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.34       |\n",
      "|    explained_variance   | 0.3         |\n",
      "|    learning_rate        | 0.00246     |\n",
      "|    loss                 | -0.0104     |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00814    |\n",
      "|    std                  | 0.737       |\n",
      "|    value_loss           | 0.000192    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -6.47       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 867         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 116         |\n",
      "|    total_timesteps      | 100800      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014822283 |\n",
      "|    clip_fraction        | 0.0613      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3          |\n",
      "|    explained_variance   | 0.377       |\n",
      "|    learning_rate        | 0.00245     |\n",
      "|    loss                 | -0.0131     |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00739    |\n",
      "|    std                  | 0.674       |\n",
      "|    value_loss           | 0.000426    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=120960, episode_reward=-0.16 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.156      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 120960      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021560242 |\n",
      "|    clip_fraction        | 0.0957      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.68       |\n",
      "|    explained_variance   | 0.464       |\n",
      "|    learning_rate        | 0.00244     |\n",
      "|    loss                 | -0.0202     |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0115     |\n",
      "|    std                  | 0.612       |\n",
      "|    value_loss           | 0.000129    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -6.18    |\n",
      "| time/              |          |\n",
      "|    fps             | 835      |\n",
      "|    iterations      | 12       |\n",
      "|    time_elapsed    | 144      |\n",
      "|    total_timesteps | 120960   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -5.71       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 842         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 167         |\n",
      "|    total_timesteps      | 141120      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012828616 |\n",
      "|    clip_fraction        | 0.0462      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.33       |\n",
      "|    explained_variance   | 0.085       |\n",
      "|    learning_rate        | 0.00243     |\n",
      "|    loss                 | -0.00707    |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.00489    |\n",
      "|    std                  | 0.553       |\n",
      "|    value_loss           | 0.00265     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -5.18       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 849         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 189         |\n",
      "|    total_timesteps      | 161280      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009971023 |\n",
      "|    clip_fraction        | 0.0437      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.99       |\n",
      "|    explained_variance   | 0.0625      |\n",
      "|    learning_rate        | 0.00242     |\n",
      "|    loss                 | -0.00874    |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0032     |\n",
      "|    std                  | 0.509       |\n",
      "|    value_loss           | 0.0013      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=181440, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.00657    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 181440      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013520876 |\n",
      "|    clip_fraction        | 0.0688      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.66       |\n",
      "|    explained_variance   | 0.185       |\n",
      "|    learning_rate        | 0.00241     |\n",
      "|    loss                 | -0.012      |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.00727    |\n",
      "|    std                  | 0.47        |\n",
      "|    value_loss           | 0.000138    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -4.55    |\n",
      "| time/              |          |\n",
      "|    fps             | 830      |\n",
      "|    iterations      | 18       |\n",
      "|    time_elapsed    | 218      |\n",
      "|    total_timesteps | 181440   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.26e+03     |\n",
      "|    ep_rew_mean          | -4.17        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 836          |\n",
      "|    iterations           | 20           |\n",
      "|    time_elapsed         | 241          |\n",
      "|    total_timesteps      | 201600       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0082846815 |\n",
      "|    clip_fraction        | 0.0348       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.227        |\n",
      "|    learning_rate        | 0.0024       |\n",
      "|    loss                 | -0.00551     |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.00407     |\n",
      "|    std                  | 0.454        |\n",
      "|    value_loss           | 0.000293     |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.26e+03   |\n",
      "|    ep_rew_mean          | -3.57      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 841        |\n",
      "|    iterations           | 22         |\n",
      "|    time_elapsed         | 263        |\n",
      "|    total_timesteps      | 221760     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01892896 |\n",
      "|    clip_fraction        | 0.0767     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.07      |\n",
      "|    explained_variance   | 0.371      |\n",
      "|    learning_rate        | 0.00239    |\n",
      "|    loss                 | -0.0102    |\n",
      "|    n_updates            | 210        |\n",
      "|    policy_gradient_loss | -0.00867   |\n",
      "|    std                  | 0.414      |\n",
      "|    value_loss           | 4.69e-05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=241920, episode_reward=-0.18 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.176      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 241920      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009638391 |\n",
      "|    clip_fraction        | 0.0374      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.727      |\n",
      "|    explained_variance   | 0.301       |\n",
      "|    learning_rate        | 0.00238     |\n",
      "|    loss                 | -0.00823    |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.00432    |\n",
      "|    std                  | 0.387       |\n",
      "|    value_loss           | 0.000108    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -3.15    |\n",
      "| time/              |          |\n",
      "|    fps             | 827      |\n",
      "|    iterations      | 24       |\n",
      "|    time_elapsed    | 292      |\n",
      "|    total_timesteps | 241920   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.26e+03     |\n",
      "|    ep_rew_mean          | -2.57        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 831          |\n",
      "|    iterations           | 26           |\n",
      "|    time_elapsed         | 315          |\n",
      "|    total_timesteps      | 262080       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064974986 |\n",
      "|    clip_fraction        | 0.0357       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.435       |\n",
      "|    explained_variance   | 0.121        |\n",
      "|    learning_rate        | 0.00237      |\n",
      "|    loss                 | -0.00513     |\n",
      "|    n_updates            | 250          |\n",
      "|    policy_gradient_loss | -0.00228     |\n",
      "|    std                  | 0.369        |\n",
      "|    value_loss           | 0.000228     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -2.21       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 836         |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 337         |\n",
      "|    total_timesteps      | 282240      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008626499 |\n",
      "|    clip_fraction        | 0.0318      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.146      |\n",
      "|    explained_variance   | 0.244       |\n",
      "|    learning_rate        | 0.00236     |\n",
      "|    loss                 | -0.00674    |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.00379    |\n",
      "|    std                  | 0.353       |\n",
      "|    value_loss           | 5.58e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=302400, episode_reward=-0.10 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0959      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 302400       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0104414495 |\n",
      "|    clip_fraction        | 0.0321       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 0.198        |\n",
      "|    explained_variance   | 0.141        |\n",
      "|    learning_rate        | 0.00235      |\n",
      "|    loss                 | -0.00602     |\n",
      "|    n_updates            | 290          |\n",
      "|    policy_gradient_loss | -0.00416     |\n",
      "|    std                  | 0.326        |\n",
      "|    value_loss           | 0.000119     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 826      |\n",
      "|    iterations      | 30       |\n",
      "|    time_elapsed    | 365      |\n",
      "|    total_timesteps | 302400   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -1.58       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 830         |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 388         |\n",
      "|    total_timesteps      | 322560      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006247782 |\n",
      "|    clip_fraction        | 0.0244      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.494       |\n",
      "|    explained_variance   | 0.113       |\n",
      "|    learning_rate        | 0.00234     |\n",
      "|    loss                 | -0.00242    |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.00251    |\n",
      "|    std                  | 0.317       |\n",
      "|    value_loss           | 9.2e-05     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -1.35       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 833         |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 411         |\n",
      "|    total_timesteps      | 342720      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018326232 |\n",
      "|    clip_fraction        | 0.0719      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.844       |\n",
      "|    explained_variance   | 0.498       |\n",
      "|    learning_rate        | 0.00233     |\n",
      "|    loss                 | -0.0148     |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.0104     |\n",
      "|    std                  | 0.291       |\n",
      "|    value_loss           | 8.29e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=362880, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0489     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 362880      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009941417 |\n",
      "|    clip_fraction        | 0.0377      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.21        |\n",
      "|    explained_variance   | 0.11        |\n",
      "|    learning_rate        | 0.00232     |\n",
      "|    loss                 | -0.0124     |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.00495    |\n",
      "|    std                  | 0.278       |\n",
      "|    value_loss           | 2e-05       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.16    |\n",
      "| time/              |          |\n",
      "|    fps             | 825      |\n",
      "|    iterations      | 36       |\n",
      "|    time_elapsed    | 439      |\n",
      "|    total_timesteps | 362880   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -1.01       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 828         |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 462         |\n",
      "|    total_timesteps      | 383040      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012092238 |\n",
      "|    clip_fraction        | 0.0549      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.53        |\n",
      "|    explained_variance   | 0.157       |\n",
      "|    learning_rate        | 0.00231     |\n",
      "|    loss                 | -0.0127     |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.00751    |\n",
      "|    std                  | 0.275       |\n",
      "|    value_loss           | 3.93e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.895      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 831         |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 485         |\n",
      "|    total_timesteps      | 403200      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005461909 |\n",
      "|    clip_fraction        | 0.0294      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.83        |\n",
      "|    explained_variance   | 0.0784      |\n",
      "|    learning_rate        | 0.0023      |\n",
      "|    loss                 | -0.00669    |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.00292    |\n",
      "|    std                  | 0.256       |\n",
      "|    value_loss           | 3.19e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=423360, episode_reward=-0.03 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0305      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 423360       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077517806 |\n",
      "|    clip_fraction        | 0.0451       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 1.99         |\n",
      "|    explained_variance   | 0.143        |\n",
      "|    learning_rate        | 0.00229      |\n",
      "|    loss                 | -0.00497     |\n",
      "|    n_updates            | 410          |\n",
      "|    policy_gradient_loss | -0.00389     |\n",
      "|    std                  | 0.255        |\n",
      "|    value_loss           | 9.43e-06     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.77    |\n",
      "| time/              |          |\n",
      "|    fps             | 824      |\n",
      "|    iterations      | 42       |\n",
      "|    time_elapsed    | 513      |\n",
      "|    total_timesteps | 423360   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.656      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 827         |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 536         |\n",
      "|    total_timesteps      | 443520      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016124872 |\n",
      "|    clip_fraction        | 0.0599      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.29        |\n",
      "|    explained_variance   | 0.536       |\n",
      "|    learning_rate        | 0.00228     |\n",
      "|    loss                 | -0.021      |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.0104     |\n",
      "|    std                  | 0.248       |\n",
      "|    value_loss           | 1.19e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.578      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 829         |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 558         |\n",
      "|    total_timesteps      | 463680      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008930564 |\n",
      "|    clip_fraction        | 0.0368      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.59        |\n",
      "|    explained_variance   | 0.174       |\n",
      "|    learning_rate        | 0.00227     |\n",
      "|    loss                 | -0.006      |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.00406    |\n",
      "|    std                  | 0.237       |\n",
      "|    value_loss           | 4.89e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=483840, episode_reward=-0.03 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.034      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 483840      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013649238 |\n",
      "|    clip_fraction        | 0.0515      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.92        |\n",
      "|    explained_variance   | 0.299       |\n",
      "|    learning_rate        | 0.00226     |\n",
      "|    loss                 | -0.0145     |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.00878    |\n",
      "|    std                  | 0.227       |\n",
      "|    value_loss           | 9.52e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.502   |\n",
      "| time/              |          |\n",
      "|    fps             | 823      |\n",
      "|    iterations      | 48       |\n",
      "|    time_elapsed    | 587      |\n",
      "|    total_timesteps | 483840   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.26e+03     |\n",
      "|    ep_rew_mean          | -0.458       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 826          |\n",
      "|    iterations           | 50           |\n",
      "|    time_elapsed         | 609          |\n",
      "|    total_timesteps      | 504000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0085266065 |\n",
      "|    clip_fraction        | 0.045        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 3.21         |\n",
      "|    explained_variance   | 0.289        |\n",
      "|    learning_rate        | 0.00225      |\n",
      "|    loss                 | -0.00647     |\n",
      "|    n_updates            | 490          |\n",
      "|    policy_gradient_loss | -0.00516     |\n",
      "|    std                  | 0.215        |\n",
      "|    value_loss           | 1.73e-06     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.39       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 828         |\n",
      "|    iterations           | 52          |\n",
      "|    time_elapsed         | 632         |\n",
      "|    total_timesteps      | 524160      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007243394 |\n",
      "|    clip_fraction        | 0.0484      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.39        |\n",
      "|    explained_variance   | 0.306       |\n",
      "|    learning_rate        | 0.00224     |\n",
      "|    loss                 | -0.00926    |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.0063     |\n",
      "|    std                  | 0.211       |\n",
      "|    value_loss           | 6.14e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=544320, episode_reward=-0.03 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0253      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 544320       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0085605495 |\n",
      "|    clip_fraction        | 0.047        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 3.61         |\n",
      "|    explained_variance   | 0.47         |\n",
      "|    learning_rate        | 0.00223      |\n",
      "|    loss                 | -0.0166      |\n",
      "|    n_updates            | 530          |\n",
      "|    policy_gradient_loss | -0.00825     |\n",
      "|    std                  | 0.21         |\n",
      "|    value_loss           | 2.85e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.346   |\n",
      "| time/              |          |\n",
      "|    fps             | 823      |\n",
      "|    iterations      | 54       |\n",
      "|    time_elapsed    | 661      |\n",
      "|    total_timesteps | 544320   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1.26e+03  |\n",
      "|    ep_rew_mean          | -0.308    |\n",
      "| time/                   |           |\n",
      "|    fps                  | 825       |\n",
      "|    iterations           | 56        |\n",
      "|    time_elapsed         | 683       |\n",
      "|    total_timesteps      | 564480    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0117069 |\n",
      "|    clip_fraction        | 0.0474    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | 3.82      |\n",
      "|    explained_variance   | 0.398     |\n",
      "|    learning_rate        | 0.00222   |\n",
      "|    loss                 | -0.0116   |\n",
      "|    n_updates            | 550       |\n",
      "|    policy_gradient_loss | -0.00652  |\n",
      "|    std                  | 0.202     |\n",
      "|    value_loss           | 4.13e-07  |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.274      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 827         |\n",
      "|    iterations           | 58          |\n",
      "|    time_elapsed         | 706         |\n",
      "|    total_timesteps      | 584640      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006962925 |\n",
      "|    clip_fraction        | 0.0678      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.03        |\n",
      "|    explained_variance   | 0.235       |\n",
      "|    learning_rate        | 0.00221     |\n",
      "|    loss                 | -0.00982    |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | -0.00448    |\n",
      "|    std                  | 0.199       |\n",
      "|    value_loss           | 5.19e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=604800, episode_reward=-0.03 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0258      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 604800       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045871846 |\n",
      "|    clip_fraction        | 0.0294       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.19         |\n",
      "|    explained_variance   | 0.126        |\n",
      "|    learning_rate        | 0.0022       |\n",
      "|    loss                 | -0.00597     |\n",
      "|    n_updates            | 590          |\n",
      "|    policy_gradient_loss | -0.00212     |\n",
      "|    std                  | 0.191        |\n",
      "|    value_loss           | 1.64e-06     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.254   |\n",
      "| time/              |          |\n",
      "|    fps             | 822      |\n",
      "|    iterations      | 60       |\n",
      "|    time_elapsed    | 734      |\n",
      "|    total_timesteps | 604800   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.227      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 824         |\n",
      "|    iterations           | 62          |\n",
      "|    time_elapsed         | 757         |\n",
      "|    total_timesteps      | 624960      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006938491 |\n",
      "|    clip_fraction        | 0.0614      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.38        |\n",
      "|    explained_variance   | 0.376       |\n",
      "|    learning_rate        | 0.00219     |\n",
      "|    loss                 | -0.00773    |\n",
      "|    n_updates            | 610         |\n",
      "|    policy_gradient_loss | -0.00377    |\n",
      "|    std                  | 0.189       |\n",
      "|    value_loss           | 4.01e-07    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.26e+03     |\n",
      "|    ep_rew_mean          | -0.211       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 826          |\n",
      "|    iterations           | 64           |\n",
      "|    time_elapsed         | 780          |\n",
      "|    total_timesteps      | 645120       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039701397 |\n",
      "|    clip_fraction        | 0.0379       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.54         |\n",
      "|    explained_variance   | 0.431        |\n",
      "|    learning_rate        | 0.00218      |\n",
      "|    loss                 | -0.00467     |\n",
      "|    n_updates            | 630          |\n",
      "|    policy_gradient_loss | -0.00234     |\n",
      "|    std                  | 0.183        |\n",
      "|    value_loss           | 2.75e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=665280, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0187     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 665280      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006992065 |\n",
      "|    clip_fraction        | 0.0426      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.73        |\n",
      "|    explained_variance   | 0.333       |\n",
      "|    learning_rate        | 0.00217     |\n",
      "|    loss                 | -0.0057     |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | -0.00293    |\n",
      "|    std                  | 0.178       |\n",
      "|    value_loss           | 3.94e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.197   |\n",
      "| time/              |          |\n",
      "|    fps             | 822      |\n",
      "|    iterations      | 66       |\n",
      "|    time_elapsed    | 808      |\n",
      "|    total_timesteps | 665280   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.26e+03     |\n",
      "|    ep_rew_mean          | -0.182       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 824          |\n",
      "|    iterations           | 68           |\n",
      "|    time_elapsed         | 831          |\n",
      "|    total_timesteps      | 685440       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064335186 |\n",
      "|    clip_fraction        | 0.0836       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.89         |\n",
      "|    explained_variance   | 0.126        |\n",
      "|    learning_rate        | 0.00216      |\n",
      "|    loss                 | -0.00238     |\n",
      "|    n_updates            | 670          |\n",
      "|    policy_gradient_loss | -0.00255     |\n",
      "|    std                  | 0.168        |\n",
      "|    value_loss           | 1.17e-06     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.171      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 826         |\n",
      "|    iterations           | 70          |\n",
      "|    time_elapsed         | 854         |\n",
      "|    total_timesteps      | 705600      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006093953 |\n",
      "|    clip_fraction        | 0.11        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5           |\n",
      "|    explained_variance   | 0.263       |\n",
      "|    learning_rate        | 0.00215     |\n",
      "|    loss                 | -0.00368    |\n",
      "|    n_updates            | 690         |\n",
      "|    policy_gradient_loss | -0.000809   |\n",
      "|    std                  | 0.166       |\n",
      "|    value_loss           | 8.97e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=725760, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00942     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 725760       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064641284 |\n",
      "|    clip_fraction        | 0.0479       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.16         |\n",
      "|    explained_variance   | 0.393        |\n",
      "|    learning_rate        | 0.00214      |\n",
      "|    loss                 | -0.00612     |\n",
      "|    n_updates            | 710          |\n",
      "|    policy_gradient_loss | -0.00359     |\n",
      "|    std                  | 0.159        |\n",
      "|    value_loss           | 3.65e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.161   |\n",
      "| time/              |          |\n",
      "|    fps             | 822      |\n",
      "|    iterations      | 72       |\n",
      "|    time_elapsed    | 882      |\n",
      "|    total_timesteps | 725760   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.154      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 823         |\n",
      "|    iterations           | 74          |\n",
      "|    time_elapsed         | 905         |\n",
      "|    total_timesteps      | 745920      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008983672 |\n",
      "|    clip_fraction        | 0.0401      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.36        |\n",
      "|    explained_variance   | 0.343       |\n",
      "|    learning_rate        | 0.00213     |\n",
      "|    loss                 | -0.0026     |\n",
      "|    n_updates            | 730         |\n",
      "|    policy_gradient_loss | -0.00476    |\n",
      "|    std                  | 0.153       |\n",
      "|    value_loss           | 2.88e-07    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.26e+03     |\n",
      "|    ep_rew_mean          | -0.147       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 825          |\n",
      "|    iterations           | 76           |\n",
      "|    time_elapsed         | 927          |\n",
      "|    total_timesteps      | 766080       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071803667 |\n",
      "|    clip_fraction        | 0.0569       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.47         |\n",
      "|    explained_variance   | 0.116        |\n",
      "|    learning_rate        | 0.00212      |\n",
      "|    loss                 | -0.000193    |\n",
      "|    n_updates            | 750          |\n",
      "|    policy_gradient_loss | -0.000666    |\n",
      "|    std                  | 0.151        |\n",
      "|    value_loss           | 1.7e-06      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=786240, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0162     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 786240      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007452347 |\n",
      "|    clip_fraction        | 0.0542      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.58        |\n",
      "|    explained_variance   | 0.414       |\n",
      "|    learning_rate        | 0.00211     |\n",
      "|    loss                 | -0.0117     |\n",
      "|    n_updates            | 770         |\n",
      "|    policy_gradient_loss | -0.00693    |\n",
      "|    std                  | 0.145       |\n",
      "|    value_loss           | 8.71e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.137   |\n",
      "| time/              |          |\n",
      "|    fps             | 822      |\n",
      "|    iterations      | 78       |\n",
      "|    time_elapsed    | 956      |\n",
      "|    total_timesteps | 786240   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.13       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 823         |\n",
      "|    iterations           | 80          |\n",
      "|    time_elapsed         | 978         |\n",
      "|    total_timesteps      | 806400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007266865 |\n",
      "|    clip_fraction        | 0.0433      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.78        |\n",
      "|    explained_variance   | 0.137       |\n",
      "|    learning_rate        | 0.0021      |\n",
      "|    loss                 | -0.005      |\n",
      "|    n_updates            | 790         |\n",
      "|    policy_gradient_loss | -0.00208    |\n",
      "|    std                  | 0.135       |\n",
      "|    value_loss           | 1.2e-06     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.122      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 825         |\n",
      "|    iterations           | 82          |\n",
      "|    time_elapsed         | 1001        |\n",
      "|    total_timesteps      | 826560      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006666083 |\n",
      "|    clip_fraction        | 0.0356      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.95        |\n",
      "|    explained_variance   | 0.347       |\n",
      "|    learning_rate        | 0.00209     |\n",
      "|    loss                 | -0.00224    |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | -0.00358    |\n",
      "|    std                  | 0.131       |\n",
      "|    value_loss           | 5.3e-08     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=846720, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0108      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 846720       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072451513 |\n",
      "|    clip_fraction        | 0.0839       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.13         |\n",
      "|    explained_variance   | 0.321        |\n",
      "|    learning_rate        | 0.00208      |\n",
      "|    loss                 | -0.00209     |\n",
      "|    n_updates            | 830          |\n",
      "|    policy_gradient_loss | -0.00181     |\n",
      "|    std                  | 0.126        |\n",
      "|    value_loss           | 1.38e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.115   |\n",
      "| time/              |          |\n",
      "|    fps             | 821      |\n",
      "|    iterations      | 84       |\n",
      "|    time_elapsed    | 1030     |\n",
      "|    total_timesteps | 846720   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.105      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 823         |\n",
      "|    iterations           | 86          |\n",
      "|    time_elapsed         | 1053        |\n",
      "|    total_timesteps      | 866880      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008198387 |\n",
      "|    clip_fraction        | 0.0393      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.28        |\n",
      "|    explained_variance   | 0.497       |\n",
      "|    learning_rate        | 0.00207     |\n",
      "|    loss                 | -0.0123     |\n",
      "|    n_updates            | 850         |\n",
      "|    policy_gradient_loss | -0.00623    |\n",
      "|    std                  | 0.12        |\n",
      "|    value_loss           | 3.44e-08    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0962     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 824         |\n",
      "|    iterations           | 88          |\n",
      "|    time_elapsed         | 1075        |\n",
      "|    total_timesteps      | 887040      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011278202 |\n",
      "|    clip_fraction        | 0.08        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.48        |\n",
      "|    explained_variance   | 0.334       |\n",
      "|    learning_rate        | 0.00206     |\n",
      "|    loss                 | 0.00625     |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | -0.000779   |\n",
      "|    std                  | 0.115       |\n",
      "|    value_loss           | 1.31e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=907200, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00717     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 907200       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058687353 |\n",
      "|    clip_fraction        | 0.0653       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.62         |\n",
      "|    explained_variance   | 0.0586       |\n",
      "|    learning_rate        | 0.00205      |\n",
      "|    loss                 | -0.00268     |\n",
      "|    n_updates            | 890          |\n",
      "|    policy_gradient_loss | -0.000702    |\n",
      "|    std                  | 0.11         |\n",
      "|    value_loss           | 9.27e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.093   |\n",
      "| time/              |          |\n",
      "|    fps             | 821      |\n",
      "|    iterations      | 90       |\n",
      "|    time_elapsed    | 1104     |\n",
      "|    total_timesteps | 907200   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0856     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 822         |\n",
      "|    iterations           | 92          |\n",
      "|    time_elapsed         | 1126        |\n",
      "|    total_timesteps      | 927360      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004859539 |\n",
      "|    clip_fraction        | 0.0531      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.71        |\n",
      "|    explained_variance   | 0.535       |\n",
      "|    learning_rate        | 0.00204     |\n",
      "|    loss                 | -0.00736    |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | -0.00176    |\n",
      "|    std                  | 0.106       |\n",
      "|    value_loss           | 4e-08       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.26e+03     |\n",
      "|    ep_rew_mean          | -0.0819      |\n",
      "| time/                   |              |\n",
      "|    fps                  | 824          |\n",
      "|    iterations           | 94           |\n",
      "|    time_elapsed         | 1149         |\n",
      "|    total_timesteps      | 947520       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058207796 |\n",
      "|    clip_fraction        | 0.175        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.84         |\n",
      "|    explained_variance   | 0.119        |\n",
      "|    learning_rate        | 0.00203      |\n",
      "|    loss                 | -0.000244    |\n",
      "|    n_updates            | 930          |\n",
      "|    policy_gradient_loss | 0.00218      |\n",
      "|    std                  | 0.104        |\n",
      "|    value_loss           | 3.36e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=967680, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.015      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 967680      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004664962 |\n",
      "|    clip_fraction        | 0.0927      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.94        |\n",
      "|    explained_variance   | 0.427       |\n",
      "|    learning_rate        | 0.00202     |\n",
      "|    loss                 | -0.00394    |\n",
      "|    n_updates            | 950         |\n",
      "|    policy_gradient_loss | -0.000314   |\n",
      "|    std                  | 0.101       |\n",
      "|    value_loss           | 4.76e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0784  |\n",
      "| time/              |          |\n",
      "|    fps             | 821      |\n",
      "|    iterations      | 96       |\n",
      "|    time_elapsed    | 1177     |\n",
      "|    total_timesteps | 967680   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0753     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 822         |\n",
      "|    iterations           | 98          |\n",
      "|    time_elapsed         | 1201        |\n",
      "|    total_timesteps      | 987840      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012929994 |\n",
      "|    clip_fraction        | 0.112       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.03        |\n",
      "|    explained_variance   | 0.434       |\n",
      "|    learning_rate        | 0.00201     |\n",
      "|    loss                 | -0.00254    |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | -0.000345   |\n",
      "|    std                  | 0.0984      |\n",
      "|    value_loss           | 5.62e-08    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.26e+03     |\n",
      "|    ep_rew_mean          | -0.075       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 823          |\n",
      "|    iterations           | 100          |\n",
      "|    time_elapsed         | 1223         |\n",
      "|    total_timesteps      | 1008000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027440768 |\n",
      "|    clip_fraction        | 0.0984       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 7.12         |\n",
      "|    explained_variance   | 0.195        |\n",
      "|    learning_rate        | 0.002        |\n",
      "|    loss                 | -0.00194     |\n",
      "|    n_updates            | 990          |\n",
      "|    policy_gradient_loss | 0.00105      |\n",
      "|    std                  | 0.0947       |\n",
      "|    value_loss           | 1.47e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1028160, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.00714    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1028160     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009466728 |\n",
      "|    clip_fraction        | 0.114       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.24        |\n",
      "|    explained_variance   | 0.345       |\n",
      "|    learning_rate        | 0.00199     |\n",
      "|    loss                 | -0.00436    |\n",
      "|    n_updates            | 1010        |\n",
      "|    policy_gradient_loss | 0.00142     |\n",
      "|    std                  | 0.0916      |\n",
      "|    value_loss           | 3.18e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0697  |\n",
      "| time/              |          |\n",
      "|    fps             | 821      |\n",
      "|    iterations      | 102      |\n",
      "|    time_elapsed    | 1252     |\n",
      "|    total_timesteps | 1028160  |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.26e+03     |\n",
      "|    ep_rew_mean          | -0.067       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 822          |\n",
      "|    iterations           | 104          |\n",
      "|    time_elapsed         | 1274         |\n",
      "|    total_timesteps      | 1048320      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076452103 |\n",
      "|    clip_fraction        | 0.139        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 7.33         |\n",
      "|    explained_variance   | 0.423        |\n",
      "|    learning_rate        | 0.00198      |\n",
      "|    loss                 | -0.00558     |\n",
      "|    n_updates            | 1030         |\n",
      "|    policy_gradient_loss | 0.0037       |\n",
      "|    std                  | 0.0892       |\n",
      "|    value_loss           | 2.51e-08     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0654     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 823         |\n",
      "|    iterations           | 106         |\n",
      "|    time_elapsed         | 1297        |\n",
      "|    total_timesteps      | 1068480     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016218904 |\n",
      "|    clip_fraction        | 0.22        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.44        |\n",
      "|    explained_variance   | 0.0892      |\n",
      "|    learning_rate        | 0.00197     |\n",
      "|    loss                 | -0.000171   |\n",
      "|    n_updates            | 1050        |\n",
      "|    policy_gradient_loss | 0.00192     |\n",
      "|    std                  | 0.0874      |\n",
      "|    value_loss           | 3.02e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1088640, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0139    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1088640    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01609694 |\n",
      "|    clip_fraction        | 0.149      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 7.51       |\n",
      "|    explained_variance   | 0.117      |\n",
      "|    learning_rate        | 0.00196    |\n",
      "|    loss                 | -0.00164   |\n",
      "|    n_updates            | 1070       |\n",
      "|    policy_gradient_loss | 0.00104    |\n",
      "|    std                  | 0.085      |\n",
      "|    value_loss           | 1.83e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0644  |\n",
      "| time/              |          |\n",
      "|    fps             | 820      |\n",
      "|    iterations      | 108      |\n",
      "|    time_elapsed    | 1325     |\n",
      "|    total_timesteps | 1088640  |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.26e+03   |\n",
      "|    ep_rew_mean          | -0.0638    |\n",
      "| time/                   |            |\n",
      "|    fps                  | 821        |\n",
      "|    iterations           | 110        |\n",
      "|    time_elapsed         | 1349       |\n",
      "|    total_timesteps      | 1108800    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01282748 |\n",
      "|    clip_fraction        | 0.0812     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 7.55       |\n",
      "|    explained_variance   | 0.266      |\n",
      "|    learning_rate        | 0.00195    |\n",
      "|    loss                 | -0.00178   |\n",
      "|    n_updates            | 1090       |\n",
      "|    policy_gradient_loss | -0.000506  |\n",
      "|    std                  | 0.0843     |\n",
      "|    value_loss           | 6.33e-08   |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.26e+03   |\n",
      "|    ep_rew_mean          | -0.061     |\n",
      "| time/                   |            |\n",
      "|    fps                  | 822        |\n",
      "|    iterations           | 112        |\n",
      "|    time_elapsed         | 1371       |\n",
      "|    total_timesteps      | 1128960    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02265757 |\n",
      "|    clip_fraction        | 0.232      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 7.61       |\n",
      "|    explained_variance   | 0.261      |\n",
      "|    learning_rate        | 0.00194    |\n",
      "|    loss                 | 0.0136     |\n",
      "|    n_updates            | 1110       |\n",
      "|    policy_gradient_loss | 0.00489    |\n",
      "|    std                  | 0.0832     |\n",
      "|    value_loss           | 6.48e-08   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=1149120, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0216     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1149120     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017972648 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.67        |\n",
      "|    explained_variance   | 0.171       |\n",
      "|    learning_rate        | 0.00193     |\n",
      "|    loss                 | 0.00666     |\n",
      "|    n_updates            | 1130        |\n",
      "|    policy_gradient_loss | 0.00197     |\n",
      "|    std                  | 0.0809      |\n",
      "|    value_loss           | 6.54e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0607  |\n",
      "| time/              |          |\n",
      "|    fps             | 820      |\n",
      "|    iterations      | 114      |\n",
      "|    time_elapsed    | 1401     |\n",
      "|    total_timesteps | 1149120  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0603     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 821         |\n",
      "|    iterations           | 116         |\n",
      "|    time_elapsed         | 1423        |\n",
      "|    total_timesteps      | 1169280     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009476482 |\n",
      "|    clip_fraction        | 0.206       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.72        |\n",
      "|    explained_variance   | 0.291       |\n",
      "|    learning_rate        | 0.00192     |\n",
      "|    loss                 | 0.0126      |\n",
      "|    n_updates            | 1150        |\n",
      "|    policy_gradient_loss | 0.00464     |\n",
      "|    std                  | 0.0797      |\n",
      "|    value_loss           | 1.63e-08    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.26e+03     |\n",
      "|    ep_rew_mean          | -0.0584      |\n",
      "| time/                   |              |\n",
      "|    fps                  | 822          |\n",
      "|    iterations           | 118          |\n",
      "|    time_elapsed         | 1446         |\n",
      "|    total_timesteps      | 1189440      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072195707 |\n",
      "|    clip_fraction        | 0.109        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 7.79         |\n",
      "|    explained_variance   | 0.348        |\n",
      "|    learning_rate        | 0.00191      |\n",
      "|    loss                 | -0.00453     |\n",
      "|    n_updates            | 1170         |\n",
      "|    policy_gradient_loss | -0.00287     |\n",
      "|    std                  | 0.0778       |\n",
      "|    value_loss           | 1.95e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1209600, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0094     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1209600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005187291 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.89        |\n",
      "|    explained_variance   | 0.433       |\n",
      "|    learning_rate        | 0.0019      |\n",
      "|    loss                 | -0.00288    |\n",
      "|    n_updates            | 1190        |\n",
      "|    policy_gradient_loss | 0.000596    |\n",
      "|    std                  | 0.0749      |\n",
      "|    value_loss           | 1.56e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.057   |\n",
      "| time/              |          |\n",
      "|    fps             | 820      |\n",
      "|    iterations      | 120      |\n",
      "|    time_elapsed    | 1474     |\n",
      "|    total_timesteps | 1209600  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0546     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 821         |\n",
      "|    iterations           | 122         |\n",
      "|    time_elapsed         | 1497        |\n",
      "|    total_timesteps      | 1229760     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010226746 |\n",
      "|    clip_fraction        | 0.139       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.97        |\n",
      "|    explained_variance   | 0.612       |\n",
      "|    learning_rate        | 0.00189     |\n",
      "|    loss                 | -0.000633   |\n",
      "|    n_updates            | 1210        |\n",
      "|    policy_gradient_loss | 0.00117     |\n",
      "|    std                  | 0.0731      |\n",
      "|    value_loss           | 1.45e-08    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0529     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 822         |\n",
      "|    iterations           | 124         |\n",
      "|    time_elapsed         | 1519        |\n",
      "|    total_timesteps      | 1249920     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006916199 |\n",
      "|    clip_fraction        | 0.119       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.07        |\n",
      "|    explained_variance   | 0.205       |\n",
      "|    learning_rate        | 0.00188     |\n",
      "|    loss                 | -0.0022     |\n",
      "|    n_updates            | 1230        |\n",
      "|    policy_gradient_loss | 0.00136     |\n",
      "|    std                  | 0.0724      |\n",
      "|    value_loss           | 7.81e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1270080, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00317     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1270080      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051110233 |\n",
      "|    clip_fraction        | 0.0636       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 8.16         |\n",
      "|    explained_variance   | 0.376        |\n",
      "|    learning_rate        | 0.00187      |\n",
      "|    loss                 | -0.00598     |\n",
      "|    n_updates            | 1250         |\n",
      "|    policy_gradient_loss | -0.000784    |\n",
      "|    std                  | 0.0701       |\n",
      "|    value_loss           | 3e-08        |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0502  |\n",
      "| time/              |          |\n",
      "|    fps             | 820      |\n",
      "|    iterations      | 126      |\n",
      "|    time_elapsed    | 1548     |\n",
      "|    total_timesteps | 1270080  |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.26e+03     |\n",
      "|    ep_rew_mean          | -0.0484      |\n",
      "| time/                   |              |\n",
      "|    fps                  | 821          |\n",
      "|    iterations           | 128          |\n",
      "|    time_elapsed         | 1571         |\n",
      "|    total_timesteps      | 1290240      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057240943 |\n",
      "|    clip_fraction        | 0.207        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 8.25         |\n",
      "|    explained_variance   | 0.343        |\n",
      "|    learning_rate        | 0.00186      |\n",
      "|    loss                 | -0.00038     |\n",
      "|    n_updates            | 1270         |\n",
      "|    policy_gradient_loss | 0.00659      |\n",
      "|    std                  | 0.0685       |\n",
      "|    value_loss           | 2.05e-08     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.049      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 822         |\n",
      "|    iterations           | 130         |\n",
      "|    time_elapsed         | 1594        |\n",
      "|    total_timesteps      | 1310400     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008950067 |\n",
      "|    clip_fraction        | 0.204       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.28        |\n",
      "|    explained_variance   | 0.0331      |\n",
      "|    learning_rate        | 0.00185     |\n",
      "|    loss                 | -0.000295   |\n",
      "|    n_updates            | 1290        |\n",
      "|    policy_gradient_loss | 0.00158     |\n",
      "|    std                  | 0.0684      |\n",
      "|    value_loss           | 4.6e-07     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1330560, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0228     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1330560     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.051897265 |\n",
      "|    clip_fraction        | 0.209       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.33        |\n",
      "|    explained_variance   | 0.119       |\n",
      "|    learning_rate        | 0.00184     |\n",
      "|    loss                 | 0.047       |\n",
      "|    n_updates            | 1310        |\n",
      "|    policy_gradient_loss | 0.00585     |\n",
      "|    std                  | 0.0674      |\n",
      "|    value_loss           | 7.47e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0485  |\n",
      "| time/              |          |\n",
      "|    fps             | 819      |\n",
      "|    iterations      | 132      |\n",
      "|    time_elapsed    | 1622     |\n",
      "|    total_timesteps | 1330560  |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.26e+03     |\n",
      "|    ep_rew_mean          | -0.0483      |\n",
      "| time/                   |              |\n",
      "|    fps                  | 820          |\n",
      "|    iterations           | 134          |\n",
      "|    time_elapsed         | 1645         |\n",
      "|    total_timesteps      | 1350720      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042102346 |\n",
      "|    clip_fraction        | 0.128        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 8.38         |\n",
      "|    explained_variance   | 0.474        |\n",
      "|    learning_rate        | 0.00183      |\n",
      "|    loss                 | -0.00834     |\n",
      "|    n_updates            | 1330         |\n",
      "|    policy_gradient_loss | -0.0006      |\n",
      "|    std                  | 0.0665       |\n",
      "|    value_loss           | 1.13e-08     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0467     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 821         |\n",
      "|    iterations           | 136         |\n",
      "|    time_elapsed         | 1668        |\n",
      "|    total_timesteps      | 1370880     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022823114 |\n",
      "|    clip_fraction        | 0.189       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.44        |\n",
      "|    explained_variance   | 0.296       |\n",
      "|    learning_rate        | 0.00182     |\n",
      "|    loss                 | 0.0106      |\n",
      "|    n_updates            | 1350        |\n",
      "|    policy_gradient_loss | 0.00687     |\n",
      "|    std                  | 0.0653      |\n",
      "|    value_loss           | 2.37e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1391040, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.00333    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1391040     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008341693 |\n",
      "|    clip_fraction        | 0.158       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.51        |\n",
      "|    explained_variance   | 0.0826      |\n",
      "|    learning_rate        | 0.00181     |\n",
      "|    loss                 | 0.00162     |\n",
      "|    n_updates            | 1370        |\n",
      "|    policy_gradient_loss | 8.22e-05    |\n",
      "|    std                  | 0.0641      |\n",
      "|    value_loss           | 1.43e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0456  |\n",
      "| time/              |          |\n",
      "|    fps             | 819      |\n",
      "|    iterations      | 138      |\n",
      "|    time_elapsed    | 1696     |\n",
      "|    total_timesteps | 1391040  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0443     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 820         |\n",
      "|    iterations           | 140         |\n",
      "|    time_elapsed         | 1719        |\n",
      "|    total_timesteps      | 1411200     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006626142 |\n",
      "|    clip_fraction        | 0.0787      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.61        |\n",
      "|    explained_variance   | 0.244       |\n",
      "|    learning_rate        | 0.0018      |\n",
      "|    loss                 | -0.00722    |\n",
      "|    n_updates            | 1390        |\n",
      "|    policy_gradient_loss | -0.00148    |\n",
      "|    std                  | 0.0613      |\n",
      "|    value_loss           | 2.07e-08    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0424     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 821         |\n",
      "|    iterations           | 142         |\n",
      "|    time_elapsed         | 1742        |\n",
      "|    total_timesteps      | 1431360     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021682367 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.74        |\n",
      "|    explained_variance   | 0.285       |\n",
      "|    learning_rate        | 0.00179     |\n",
      "|    loss                 | -0.00452    |\n",
      "|    n_updates            | 1410        |\n",
      "|    policy_gradient_loss | 0.00106     |\n",
      "|    std                  | 0.0588      |\n",
      "|    value_loss           | 1.9e-08     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1451520, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00643     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1451520      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046403706 |\n",
      "|    clip_fraction        | 0.132        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 8.86         |\n",
      "|    explained_variance   | 0.376        |\n",
      "|    learning_rate        | 0.00178      |\n",
      "|    loss                 | -0.0106      |\n",
      "|    n_updates            | 1430         |\n",
      "|    policy_gradient_loss | -0.00321     |\n",
      "|    std                  | 0.0567       |\n",
      "|    value_loss           | 1.26e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0409  |\n",
      "| time/              |          |\n",
      "|    fps             | 819      |\n",
      "|    iterations      | 144      |\n",
      "|    time_elapsed    | 1770     |\n",
      "|    total_timesteps | 1451520  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.04       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 820         |\n",
      "|    iterations           | 146         |\n",
      "|    time_elapsed         | 1793        |\n",
      "|    total_timesteps      | 1471680     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011325713 |\n",
      "|    clip_fraction        | 0.197       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.95        |\n",
      "|    explained_variance   | 0.214       |\n",
      "|    learning_rate        | 0.00177     |\n",
      "|    loss                 | 0.00674     |\n",
      "|    n_updates            | 1450        |\n",
      "|    policy_gradient_loss | 0.00207     |\n",
      "|    std                  | 0.0553      |\n",
      "|    value_loss           | 1.32e-07    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0379     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 821         |\n",
      "|    iterations           | 148         |\n",
      "|    time_elapsed         | 1816        |\n",
      "|    total_timesteps      | 1491840     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012532946 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.04        |\n",
      "|    explained_variance   | 0.517       |\n",
      "|    learning_rate        | 0.00176     |\n",
      "|    loss                 | 0.00432     |\n",
      "|    n_updates            | 1470        |\n",
      "|    policy_gradient_loss | -0.00338    |\n",
      "|    std                  | 0.0536      |\n",
      "|    value_loss           | 7.05e-09    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1512000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0109     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1512000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006185077 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.13        |\n",
      "|    explained_variance   | 0.121       |\n",
      "|    learning_rate        | 0.00175     |\n",
      "|    loss                 | -0.00485    |\n",
      "|    n_updates            | 1490        |\n",
      "|    policy_gradient_loss | -0.000799   |\n",
      "|    std                  | 0.0519      |\n",
      "|    value_loss           | 2.27e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0365  |\n",
      "| time/              |          |\n",
      "|    fps             | 819      |\n",
      "|    iterations      | 150      |\n",
      "|    time_elapsed    | 1844     |\n",
      "|    total_timesteps | 1512000  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0355     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 820         |\n",
      "|    iterations           | 152         |\n",
      "|    time_elapsed         | 1867        |\n",
      "|    total_timesteps      | 1532160     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009934374 |\n",
      "|    clip_fraction        | 0.257       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.19        |\n",
      "|    explained_variance   | 0.267       |\n",
      "|    learning_rate        | 0.00174     |\n",
      "|    loss                 | -0.00216    |\n",
      "|    n_updates            | 1510        |\n",
      "|    policy_gradient_loss | 0.0066      |\n",
      "|    std                  | 0.0511      |\n",
      "|    value_loss           | 2.74e-08    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.035      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 821         |\n",
      "|    iterations           | 154         |\n",
      "|    time_elapsed         | 1889        |\n",
      "|    total_timesteps      | 1552320     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009449152 |\n",
      "|    clip_fraction        | 0.084       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.26        |\n",
      "|    explained_variance   | 0.224       |\n",
      "|    learning_rate        | 0.00173     |\n",
      "|    loss                 | 0.00599     |\n",
      "|    n_updates            | 1530        |\n",
      "|    policy_gradient_loss | -0.000664   |\n",
      "|    std                  | 0.05        |\n",
      "|    value_loss           | 9.72e-09    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1572480, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.00363   |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1572480    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02826298 |\n",
      "|    clip_fraction        | 0.328      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 9.36       |\n",
      "|    explained_variance   | 0.0806     |\n",
      "|    learning_rate        | 0.00172    |\n",
      "|    loss                 | 0.000253   |\n",
      "|    n_updates            | 1550       |\n",
      "|    policy_gradient_loss | 0.00769    |\n",
      "|    std                  | 0.0488     |\n",
      "|    value_loss           | 1.19e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0336  |\n",
      "| time/              |          |\n",
      "|    fps             | 819      |\n",
      "|    iterations      | 156      |\n",
      "|    time_elapsed    | 1918     |\n",
      "|    total_timesteps | 1572480  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0325     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 820         |\n",
      "|    iterations           | 158         |\n",
      "|    time_elapsed         | 1941        |\n",
      "|    total_timesteps      | 1592640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022552706 |\n",
      "|    clip_fraction        | 0.25        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.43        |\n",
      "|    explained_variance   | 0.353       |\n",
      "|    learning_rate        | 0.00171     |\n",
      "|    loss                 | -0.00598    |\n",
      "|    n_updates            | 1570        |\n",
      "|    policy_gradient_loss | 0.01        |\n",
      "|    std                  | 0.0476      |\n",
      "|    value_loss           | 8.5e-09     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0316     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 821         |\n",
      "|    iterations           | 160         |\n",
      "|    time_elapsed         | 1964        |\n",
      "|    total_timesteps      | 1612800     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007478019 |\n",
      "|    clip_fraction        | 0.179       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.49        |\n",
      "|    explained_variance   | 0.285       |\n",
      "|    learning_rate        | 0.0017      |\n",
      "|    loss                 | -0.0027     |\n",
      "|    n_updates            | 1590        |\n",
      "|    policy_gradient_loss | 0.0026      |\n",
      "|    std                  | 0.0466      |\n",
      "|    value_loss           | 2.1e-08     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1632960, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.00841    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1632960     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005672974 |\n",
      "|    clip_fraction        | 0.179       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.54        |\n",
      "|    explained_variance   | 0.14        |\n",
      "|    learning_rate        | 0.00169     |\n",
      "|    loss                 | -0.00288    |\n",
      "|    n_updates            | 1610        |\n",
      "|    policy_gradient_loss | 0.00345     |\n",
      "|    std                  | 0.0459      |\n",
      "|    value_loss           | 2.79e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0305  |\n",
      "| time/              |          |\n",
      "|    fps             | 819      |\n",
      "|    iterations      | 162      |\n",
      "|    time_elapsed    | 1992     |\n",
      "|    total_timesteps | 1632960  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0304     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 820         |\n",
      "|    iterations           | 164         |\n",
      "|    time_elapsed         | 2015        |\n",
      "|    total_timesteps      | 1653120     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005661842 |\n",
      "|    clip_fraction        | 0.299       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.59        |\n",
      "|    explained_variance   | 0.0047      |\n",
      "|    learning_rate        | 0.00168     |\n",
      "|    loss                 | -0.000175   |\n",
      "|    n_updates            | 1630        |\n",
      "|    policy_gradient_loss | 0.00891     |\n",
      "|    std                  | 0.0451      |\n",
      "|    value_loss           | 4.02e-08    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0299     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 821         |\n",
      "|    iterations           | 166         |\n",
      "|    time_elapsed         | 2038        |\n",
      "|    total_timesteps      | 1673280     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014598731 |\n",
      "|    clip_fraction        | 0.143       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.67        |\n",
      "|    explained_variance   | 0.0676      |\n",
      "|    learning_rate        | 0.00167     |\n",
      "|    loss                 | -0.00224    |\n",
      "|    n_updates            | 1650        |\n",
      "|    policy_gradient_loss | -0.000264   |\n",
      "|    std                  | 0.044       |\n",
      "|    value_loss           | 3.4e-08     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1693440, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0034     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1693440     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004407891 |\n",
      "|    clip_fraction        | 0.17        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.74        |\n",
      "|    explained_variance   | 0.166       |\n",
      "|    learning_rate        | 0.00166     |\n",
      "|    loss                 | -0.00382    |\n",
      "|    n_updates            | 1670        |\n",
      "|    policy_gradient_loss | 0.00302     |\n",
      "|    std                  | 0.0432      |\n",
      "|    value_loss           | 1.76e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0285  |\n",
      "| time/              |          |\n",
      "|    fps             | 819      |\n",
      "|    iterations      | 168      |\n",
      "|    time_elapsed    | 2066     |\n",
      "|    total_timesteps | 1693440  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0285     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 820         |\n",
      "|    iterations           | 170         |\n",
      "|    time_elapsed         | 2089        |\n",
      "|    total_timesteps      | 1713600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013584116 |\n",
      "|    clip_fraction        | 0.169       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.81        |\n",
      "|    explained_variance   | 0.162       |\n",
      "|    learning_rate        | 0.00165     |\n",
      "|    loss                 | 0.00739     |\n",
      "|    n_updates            | 1690        |\n",
      "|    policy_gradient_loss | 0.00382     |\n",
      "|    std                  | 0.0421      |\n",
      "|    value_loss           | 2.82e-08    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.26e+03     |\n",
      "|    ep_rew_mean          | -0.0276      |\n",
      "| time/                   |              |\n",
      "|    fps                  | 820          |\n",
      "|    iterations           | 172          |\n",
      "|    time_elapsed         | 2112         |\n",
      "|    total_timesteps      | 1733760      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063401423 |\n",
      "|    clip_fraction        | 0.154        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 9.89         |\n",
      "|    explained_variance   | 0.292        |\n",
      "|    learning_rate        | 0.00164      |\n",
      "|    loss                 | -1.87e-05    |\n",
      "|    n_updates            | 1710         |\n",
      "|    policy_gradient_loss | 0.00157      |\n",
      "|    std                  | 0.0413       |\n",
      "|    value_loss           | 1.27e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1753920, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.00439   |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1753920    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01962879 |\n",
      "|    clip_fraction        | 0.191      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 9.95       |\n",
      "|    explained_variance   | 0.149      |\n",
      "|    learning_rate        | 0.00163    |\n",
      "|    loss                 | -0.00241   |\n",
      "|    n_updates            | 1730       |\n",
      "|    policy_gradient_loss | 0.00388    |\n",
      "|    std                  | 0.0406     |\n",
      "|    value_loss           | 1.94e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0272  |\n",
      "| time/              |          |\n",
      "|    fps             | 819      |\n",
      "|    iterations      | 174      |\n",
      "|    time_elapsed    | 2140     |\n",
      "|    total_timesteps | 1753920  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0278     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 819         |\n",
      "|    iterations           | 176         |\n",
      "|    time_elapsed         | 2163        |\n",
      "|    total_timesteps      | 1774080     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016258055 |\n",
      "|    clip_fraction        | 0.205       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 10          |\n",
      "|    explained_variance   | 0.163       |\n",
      "|    learning_rate        | 0.00162     |\n",
      "|    loss                 | 0.00359     |\n",
      "|    n_updates            | 1750        |\n",
      "|    policy_gradient_loss | 0.00409     |\n",
      "|    std                  | 0.0397      |\n",
      "|    value_loss           | 6.37e-08    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.26e+03     |\n",
      "|    ep_rew_mean          | -0.0263      |\n",
      "| time/                   |              |\n",
      "|    fps                  | 820          |\n",
      "|    iterations           | 178          |\n",
      "|    time_elapsed         | 2186         |\n",
      "|    total_timesteps      | 1794240      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059164437 |\n",
      "|    clip_fraction        | 0.11         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 10.1         |\n",
      "|    explained_variance   | 0.375        |\n",
      "|    learning_rate        | 0.00161      |\n",
      "|    loss                 | 0.000755     |\n",
      "|    n_updates            | 1770         |\n",
      "|    policy_gradient_loss | 0.00152      |\n",
      "|    std                  | 0.0386       |\n",
      "|    value_loss           | 8.28e-09     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1814400, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.00319    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1814400     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026571862 |\n",
      "|    clip_fraction        | 0.198       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 10.2        |\n",
      "|    explained_variance   | 0.0518      |\n",
      "|    learning_rate        | 0.0016      |\n",
      "|    loss                 | 0.00293     |\n",
      "|    n_updates            | 1790        |\n",
      "|    policy_gradient_loss | 0.00804     |\n",
      "|    std                  | 0.0378      |\n",
      "|    value_loss           | 7.52e-09    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0251  |\n",
      "| time/              |          |\n",
      "|    fps             | 819      |\n",
      "|    iterations      | 180      |\n",
      "|    time_elapsed    | 2214     |\n",
      "|    total_timesteps | 1814400  |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.26e+03   |\n",
      "|    ep_rew_mean          | -0.0245    |\n",
      "| time/                   |            |\n",
      "|    fps                  | 819        |\n",
      "|    iterations           | 182        |\n",
      "|    time_elapsed         | 2237       |\n",
      "|    total_timesteps      | 1834560    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02233249 |\n",
      "|    clip_fraction        | 0.225      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 10.2       |\n",
      "|    explained_variance   | 0.238      |\n",
      "|    learning_rate        | 0.00159    |\n",
      "|    loss                 | -0.00196   |\n",
      "|    n_updates            | 1810       |\n",
      "|    policy_gradient_loss | 0.00879    |\n",
      "|    std                  | 0.0366     |\n",
      "|    value_loss           | 4.33e-09   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0238     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 820         |\n",
      "|    iterations           | 184         |\n",
      "|    time_elapsed         | 2260        |\n",
      "|    total_timesteps      | 1854720     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009055901 |\n",
      "|    clip_fraction        | 0.168       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 10.3        |\n",
      "|    explained_variance   | 0.304       |\n",
      "|    learning_rate        | 0.00158     |\n",
      "|    loss                 | -0.00355    |\n",
      "|    n_updates            | 1830        |\n",
      "|    policy_gradient_loss | 0.00352     |\n",
      "|    std                  | 0.0353      |\n",
      "|    value_loss           | 9.43e-09    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1874880, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.00909    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1874880     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010014815 |\n",
      "|    clip_fraction        | 0.191       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 10.4        |\n",
      "|    explained_variance   | 0.105       |\n",
      "|    learning_rate        | 0.00157     |\n",
      "|    loss                 | 0.0087      |\n",
      "|    n_updates            | 1850        |\n",
      "|    policy_gradient_loss | 0.0056      |\n",
      "|    std                  | 0.0345      |\n",
      "|    value_loss           | 1.2e-08     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0237  |\n",
      "| time/              |          |\n",
      "|    fps             | 819      |\n",
      "|    iterations      | 186      |\n",
      "|    time_elapsed    | 2289     |\n",
      "|    total_timesteps | 1874880  |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.26e+03   |\n",
      "|    ep_rew_mean          | -0.023     |\n",
      "| time/                   |            |\n",
      "|    fps                  | 819        |\n",
      "|    iterations           | 188        |\n",
      "|    time_elapsed         | 2311       |\n",
      "|    total_timesteps      | 1895040    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07276755 |\n",
      "|    clip_fraction        | 0.362      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 10.5       |\n",
      "|    explained_variance   | 0.0458     |\n",
      "|    learning_rate        | 0.00156    |\n",
      "|    loss                 | 0.016      |\n",
      "|    n_updates            | 1870       |\n",
      "|    policy_gradient_loss | 0.00932    |\n",
      "|    std                  | 0.0338     |\n",
      "|    value_loss           | 7.85e-08   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0226     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 820         |\n",
      "|    iterations           | 190         |\n",
      "|    time_elapsed         | 2334        |\n",
      "|    total_timesteps      | 1915200     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007664031 |\n",
      "|    clip_fraction        | 0.204       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 10.5        |\n",
      "|    explained_variance   | 0.376       |\n",
      "|    learning_rate        | 0.00155     |\n",
      "|    loss                 | -0.00625    |\n",
      "|    n_updates            | 1890        |\n",
      "|    policy_gradient_loss | 0.00255     |\n",
      "|    std                  | 0.0332      |\n",
      "|    value_loss           | 5.51e-09    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1935360, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.00652    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1935360     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019477334 |\n",
      "|    clip_fraction        | 0.168       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 10.6        |\n",
      "|    explained_variance   | 0.0966      |\n",
      "|    learning_rate        | 0.00154     |\n",
      "|    loss                 | 0.0164      |\n",
      "|    n_updates            | 1910        |\n",
      "|    policy_gradient_loss | -0.000481   |\n",
      "|    std                  | 0.0323      |\n",
      "|    value_loss           | 2.19e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0233  |\n",
      "| time/              |          |\n",
      "|    fps             | 819      |\n",
      "|    iterations      | 192      |\n",
      "|    time_elapsed    | 2362     |\n",
      "|    total_timesteps | 1935360  |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.26e+03     |\n",
      "|    ep_rew_mean          | -0.0232      |\n",
      "| time/                   |              |\n",
      "|    fps                  | 819          |\n",
      "|    iterations           | 194          |\n",
      "|    time_elapsed         | 2385         |\n",
      "|    total_timesteps      | 1955520      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045930347 |\n",
      "|    clip_fraction        | 0.167        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 10.7         |\n",
      "|    explained_variance   | 0.266        |\n",
      "|    learning_rate        | 0.00153      |\n",
      "|    loss                 | -0.00399     |\n",
      "|    n_updates            | 1930         |\n",
      "|    policy_gradient_loss | 0.00473      |\n",
      "|    std                  | 0.0314       |\n",
      "|    value_loss           | 9.68e-09     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0229     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 820         |\n",
      "|    iterations           | 196         |\n",
      "|    time_elapsed         | 2408        |\n",
      "|    total_timesteps      | 1975680     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015558347 |\n",
      "|    clip_fraction        | 0.156       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 10.7        |\n",
      "|    explained_variance   | 0.279       |\n",
      "|    learning_rate        | 0.00152     |\n",
      "|    loss                 | -0.000218   |\n",
      "|    n_updates            | 1950        |\n",
      "|    policy_gradient_loss | -0.000775   |\n",
      "|    std                  | 0.0307      |\n",
      "|    value_loss           | 7.06e-09    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1995840, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.00568    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1995840     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007080479 |\n",
      "|    clip_fraction        | 0.166       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 10.8        |\n",
      "|    explained_variance   | 0.15        |\n",
      "|    learning_rate        | 0.00151     |\n",
      "|    loss                 | -0.00339    |\n",
      "|    n_updates            | 1970        |\n",
      "|    policy_gradient_loss | 0.000981    |\n",
      "|    std                  | 0.03        |\n",
      "|    value_loss           | 1.14e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0235  |\n",
      "| time/              |          |\n",
      "|    fps             | 819      |\n",
      "|    iterations      | 198      |\n",
      "|    time_elapsed    | 2436     |\n",
      "|    total_timesteps | 1995840  |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.26e+03     |\n",
      "|    ep_rew_mean          | -0.0225      |\n",
      "| time/                   |              |\n",
      "|    fps                  | 819          |\n",
      "|    iterations           | 200          |\n",
      "|    time_elapsed         | 2459         |\n",
      "|    total_timesteps      | 2016000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074249124 |\n",
      "|    clip_fraction        | 0.272        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 10.8         |\n",
      "|    explained_variance   | 0.162        |\n",
      "|    learning_rate        | 0.0015       |\n",
      "|    loss                 | -0.00262     |\n",
      "|    n_updates            | 1990         |\n",
      "|    policy_gradient_loss | 0.00764      |\n",
      "|    std                  | 0.0293       |\n",
      "|    value_loss           | 1.17e-08     |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.26e+03   |\n",
      "|    ep_rew_mean          | -0.0224    |\n",
      "| time/                   |            |\n",
      "|    fps                  | 820        |\n",
      "|    iterations           | 202        |\n",
      "|    time_elapsed         | 2482       |\n",
      "|    total_timesteps      | 2036160    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02166945 |\n",
      "|    clip_fraction        | 0.18       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 10.9       |\n",
      "|    explained_variance   | 0.381      |\n",
      "|    learning_rate        | 0.00149    |\n",
      "|    loss                 | 0.0232     |\n",
      "|    n_updates            | 2010       |\n",
      "|    policy_gradient_loss | 0.0071     |\n",
      "|    std                  | 0.0289     |\n",
      "|    value_loss           | 3.79e-09   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=2056320, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.00924    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2056320     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034915525 |\n",
      "|    clip_fraction        | 0.271       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 10.9        |\n",
      "|    explained_variance   | 0.113       |\n",
      "|    learning_rate        | 0.00148     |\n",
      "|    loss                 | 0.0217      |\n",
      "|    n_updates            | 2030        |\n",
      "|    policy_gradient_loss | 0.00399     |\n",
      "|    std                  | 0.0285      |\n",
      "|    value_loss           | 7.14e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0227  |\n",
      "| time/              |          |\n",
      "|    fps             | 818      |\n",
      "|    iterations      | 204      |\n",
      "|    time_elapsed    | 2511     |\n",
      "|    total_timesteps | 2056320  |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.26e+03   |\n",
      "|    ep_rew_mean          | -0.0214    |\n",
      "| time/                   |            |\n",
      "|    fps                  | 819        |\n",
      "|    iterations           | 206        |\n",
      "|    time_elapsed         | 2533       |\n",
      "|    total_timesteps      | 2076480    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04292145 |\n",
      "|    clip_fraction        | 0.261      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 11         |\n",
      "|    explained_variance   | 0.326      |\n",
      "|    learning_rate        | 0.00147    |\n",
      "|    loss                 | 0.0172     |\n",
      "|    n_updates            | 2050       |\n",
      "|    policy_gradient_loss | 0.00418    |\n",
      "|    std                  | 0.0283     |\n",
      "|    value_loss           | 3.98e-09   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0214     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 820         |\n",
      "|    iterations           | 208         |\n",
      "|    time_elapsed         | 2556        |\n",
      "|    total_timesteps      | 2096640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020314397 |\n",
      "|    clip_fraction        | 0.157       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 11          |\n",
      "|    explained_variance   | 0.387       |\n",
      "|    learning_rate        | 0.00146     |\n",
      "|    loss                 | 0.0128      |\n",
      "|    n_updates            | 2070        |\n",
      "|    policy_gradient_loss | 0.000252    |\n",
      "|    std                  | 0.028       |\n",
      "|    value_loss           | 4.56e-09    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2116800, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.00363    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2116800     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006767524 |\n",
      "|    clip_fraction        | 0.197       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 11.1        |\n",
      "|    explained_variance   | 0.101       |\n",
      "|    learning_rate        | 0.00145     |\n",
      "|    loss                 | 0.0006      |\n",
      "|    n_updates            | 2090        |\n",
      "|    policy_gradient_loss | 0.00248     |\n",
      "|    std                  | 0.0278      |\n",
      "|    value_loss           | 3.83e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0198  |\n",
      "| time/              |          |\n",
      "|    fps             | 818      |\n",
      "|    iterations      | 210      |\n",
      "|    time_elapsed    | 2585     |\n",
      "|    total_timesteps | 2116800  |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.26e+03   |\n",
      "|    ep_rew_mean          | -0.0193    |\n",
      "| time/                   |            |\n",
      "|    fps                  | 819        |\n",
      "|    iterations           | 212        |\n",
      "|    time_elapsed         | 2607       |\n",
      "|    total_timesteps      | 2136960    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03028762 |\n",
      "|    clip_fraction        | 0.287      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 11.1       |\n",
      "|    explained_variance   | 0.206      |\n",
      "|    learning_rate        | 0.00144    |\n",
      "|    loss                 | 0.0148     |\n",
      "|    n_updates            | 2110       |\n",
      "|    policy_gradient_loss | 0.0107     |\n",
      "|    std                  | 0.0273     |\n",
      "|    value_loss           | 9.8e-09    |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0194     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 820         |\n",
      "|    iterations           | 214         |\n",
      "|    time_elapsed         | 2630        |\n",
      "|    total_timesteps      | 2157120     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016709426 |\n",
      "|    clip_fraction        | 0.276       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 11.2        |\n",
      "|    explained_variance   | 0.254       |\n",
      "|    learning_rate        | 0.00143     |\n",
      "|    loss                 | 0.0134      |\n",
      "|    n_updates            | 2130        |\n",
      "|    policy_gradient_loss | 0.0102      |\n",
      "|    std                  | 0.027       |\n",
      "|    value_loss           | 6.53e-09    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2177280, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.00667   |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2177280    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02643636 |\n",
      "|    clip_fraction        | 0.265      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 11.2       |\n",
      "|    explained_variance   | 0.0138     |\n",
      "|    learning_rate        | 0.00142    |\n",
      "|    loss                 | 0.00139    |\n",
      "|    n_updates            | 2150       |\n",
      "|    policy_gradient_loss | 0.00153    |\n",
      "|    std                  | 0.0267     |\n",
      "|    value_loss           | 7.28e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0183  |\n",
      "| time/              |          |\n",
      "|    fps             | 818      |\n",
      "|    iterations      | 216      |\n",
      "|    time_elapsed    | 2659     |\n",
      "|    total_timesteps | 2177280  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0178     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 819         |\n",
      "|    iterations           | 218         |\n",
      "|    time_elapsed         | 2682        |\n",
      "|    total_timesteps      | 2197440     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012157812 |\n",
      "|    clip_fraction        | 0.29        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 11.2        |\n",
      "|    explained_variance   | 0.143       |\n",
      "|    learning_rate        | 0.00141     |\n",
      "|    loss                 | -0.00193    |\n",
      "|    n_updates            | 2170        |\n",
      "|    policy_gradient_loss | 0.00873     |\n",
      "|    std                  | 0.0265      |\n",
      "|    value_loss           | 1.33e-08    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0172     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 819         |\n",
      "|    iterations           | 220         |\n",
      "|    time_elapsed         | 2704        |\n",
      "|    total_timesteps      | 2217600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009685119 |\n",
      "|    clip_fraction        | 0.223       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 11.3        |\n",
      "|    explained_variance   | 0.227       |\n",
      "|    learning_rate        | 0.0014      |\n",
      "|    loss                 | -0.005      |\n",
      "|    n_updates            | 2190        |\n",
      "|    policy_gradient_loss | 0.00769     |\n",
      "|    std                  | 0.026       |\n",
      "|    value_loss           | 2.7e-09     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2237760, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.00653    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2237760     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012227669 |\n",
      "|    clip_fraction        | 0.222       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 11.4        |\n",
      "|    explained_variance   | 0.0828      |\n",
      "|    learning_rate        | 0.00139     |\n",
      "|    loss                 | 0.000658    |\n",
      "|    n_updates            | 2210        |\n",
      "|    policy_gradient_loss | 0.00522     |\n",
      "|    std                  | 0.0255      |\n",
      "|    value_loss           | 1.11e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.017   |\n",
      "| time/              |          |\n",
      "|    fps             | 818      |\n",
      "|    iterations      | 222      |\n",
      "|    time_elapsed    | 2733     |\n",
      "|    total_timesteps | 2237760  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0168     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 819         |\n",
      "|    iterations           | 224         |\n",
      "|    time_elapsed         | 2755        |\n",
      "|    total_timesteps      | 2257920     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013246796 |\n",
      "|    clip_fraction        | 0.186       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 11.4        |\n",
      "|    explained_variance   | -0.0057     |\n",
      "|    learning_rate        | 0.00138     |\n",
      "|    loss                 | 0.00476     |\n",
      "|    n_updates            | 2230        |\n",
      "|    policy_gradient_loss | 0.00393     |\n",
      "|    std                  | 0.0254      |\n",
      "|    value_loss           | 1.67e-08    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0165     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 819         |\n",
      "|    iterations           | 226         |\n",
      "|    time_elapsed         | 2778        |\n",
      "|    total_timesteps      | 2278080     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029709568 |\n",
      "|    clip_fraction        | 0.336       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 11.4        |\n",
      "|    explained_variance   | -0.233      |\n",
      "|    learning_rate        | 0.00137     |\n",
      "|    loss                 | 0.0224      |\n",
      "|    n_updates            | 2250        |\n",
      "|    policy_gradient_loss | 0.0128      |\n",
      "|    std                  | 0.0251      |\n",
      "|    value_loss           | 2.08e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2298240, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.00971    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2298240     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019810926 |\n",
      "|    clip_fraction        | 0.2         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 11.5        |\n",
      "|    explained_variance   | -0.0217     |\n",
      "|    learning_rate        | 0.00136     |\n",
      "|    loss                 | 0.00204     |\n",
      "|    n_updates            | 2270        |\n",
      "|    policy_gradient_loss | 0.00112     |\n",
      "|    std                  | 0.0249      |\n",
      "|    value_loss           | 1.25e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0164  |\n",
      "| time/              |          |\n",
      "|    fps             | 818      |\n",
      "|    iterations      | 228      |\n",
      "|    time_elapsed    | 2807     |\n",
      "|    total_timesteps | 2298240  |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.26e+03     |\n",
      "|    ep_rew_mean          | -0.016       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 819          |\n",
      "|    iterations           | 230          |\n",
      "|    time_elapsed         | 2830         |\n",
      "|    total_timesteps      | 2318400      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055100475 |\n",
      "|    clip_fraction        | 0.225        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 11.5         |\n",
      "|    explained_variance   | 0.12         |\n",
      "|    learning_rate        | 0.00135      |\n",
      "|    loss                 | -0.00338     |\n",
      "|    n_updates            | 2290         |\n",
      "|    policy_gradient_loss | 0.00651      |\n",
      "|    std                  | 0.0246       |\n",
      "|    value_loss           | 6.11e-09     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0159     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 819         |\n",
      "|    iterations           | 232         |\n",
      "|    time_elapsed         | 2852        |\n",
      "|    total_timesteps      | 2338560     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024872283 |\n",
      "|    clip_fraction        | 0.187       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 11.6        |\n",
      "|    explained_variance   | -0.0625     |\n",
      "|    learning_rate        | 0.00134     |\n",
      "|    loss                 | 0.0143      |\n",
      "|    n_updates            | 2310        |\n",
      "|    policy_gradient_loss | 0.00594     |\n",
      "|    std                  | 0.0242      |\n",
      "|    value_loss           | 3.47e-09    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2358720, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.00858    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2358720     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.046032608 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 11.6        |\n",
      "|    explained_variance   | -0.0151     |\n",
      "|    learning_rate        | 0.00133     |\n",
      "|    loss                 | 0.0358      |\n",
      "|    n_updates            | 2330        |\n",
      "|    policy_gradient_loss | 0.0146      |\n",
      "|    std                  | 0.0238      |\n",
      "|    value_loss           | 1.03e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0157  |\n",
      "| time/              |          |\n",
      "|    fps             | 818      |\n",
      "|    iterations      | 234      |\n",
      "|    time_elapsed    | 2881     |\n",
      "|    total_timesteps | 2358720  |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.26e+03     |\n",
      "|    ep_rew_mean          | -0.0155      |\n",
      "| time/                   |              |\n",
      "|    fps                  | 819          |\n",
      "|    iterations           | 236          |\n",
      "|    time_elapsed         | 2903         |\n",
      "|    total_timesteps      | 2378880      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065438165 |\n",
      "|    clip_fraction        | 0.356        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 11.7         |\n",
      "|    explained_variance   | 0.173        |\n",
      "|    learning_rate        | 0.00132      |\n",
      "|    loss                 | -0.00398     |\n",
      "|    n_updates            | 2350         |\n",
      "|    policy_gradient_loss | 0.0225       |\n",
      "|    std                  | 0.0237       |\n",
      "|    value_loss           | 2.58e-09     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0152     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 819         |\n",
      "|    iterations           | 238         |\n",
      "|    time_elapsed         | 2926        |\n",
      "|    total_timesteps      | 2399040     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022417983 |\n",
      "|    clip_fraction        | 0.243       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 11.7        |\n",
      "|    explained_variance   | -0.0417     |\n",
      "|    learning_rate        | 0.00131     |\n",
      "|    loss                 | -0.00334    |\n",
      "|    n_updates            | 2370        |\n",
      "|    policy_gradient_loss | 0.00682     |\n",
      "|    std                  | 0.0235      |\n",
      "|    value_loss           | 7.32e-09    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2419200, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.00237    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2419200     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009516752 |\n",
      "|    clip_fraction        | 0.171       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 11.7        |\n",
      "|    explained_variance   | -0.211      |\n",
      "|    learning_rate        | 0.0013      |\n",
      "|    loss                 | -0.00517    |\n",
      "|    n_updates            | 2390        |\n",
      "|    policy_gradient_loss | 0.00457     |\n",
      "|    std                  | 0.0231      |\n",
      "|    value_loss           | 5.91e-09    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0148  |\n",
      "| time/              |          |\n",
      "|    fps             | 818      |\n",
      "|    iterations      | 240      |\n",
      "|    time_elapsed    | 2955     |\n",
      "|    total_timesteps | 2419200  |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.26e+03   |\n",
      "|    ep_rew_mean          | -0.0144    |\n",
      "| time/                   |            |\n",
      "|    fps                  | 819        |\n",
      "|    iterations           | 242        |\n",
      "|    time_elapsed         | 2978       |\n",
      "|    total_timesteps      | 2439360    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06106384 |\n",
      "|    clip_fraction        | 0.268      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 11.8       |\n",
      "|    explained_variance   | -0.0263    |\n",
      "|    learning_rate        | 0.00129    |\n",
      "|    loss                 | 0.0942     |\n",
      "|    n_updates            | 2410       |\n",
      "|    policy_gradient_loss | 0.0136     |\n",
      "|    std                  | 0.0228     |\n",
      "|    value_loss           | 2.3e-09    |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.26e+03   |\n",
      "|    ep_rew_mean          | -0.0142    |\n",
      "| time/                   |            |\n",
      "|    fps                  | 819        |\n",
      "|    iterations           | 244        |\n",
      "|    time_elapsed         | 3000       |\n",
      "|    total_timesteps      | 2459520    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02072808 |\n",
      "|    clip_fraction        | 0.299      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 11.8       |\n",
      "|    explained_variance   | -0.267     |\n",
      "|    learning_rate        | 0.00128    |\n",
      "|    loss                 | 0.00673    |\n",
      "|    n_updates            | 2430       |\n",
      "|    policy_gradient_loss | 0.0166     |\n",
      "|    std                  | 0.0224     |\n",
      "|    value_loss           | 2.86e-09   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=2479680, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.00259    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2479680     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021987323 |\n",
      "|    clip_fraction        | 0.292       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 11.9        |\n",
      "|    explained_variance   | 0.219       |\n",
      "|    learning_rate        | 0.00127     |\n",
      "|    loss                 | 0.0114      |\n",
      "|    n_updates            | 2450        |\n",
      "|    policy_gradient_loss | 0.0114      |\n",
      "|    std                  | 0.0222      |\n",
      "|    value_loss           | 8e-09       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.014   |\n",
      "| time/              |          |\n",
      "|    fps             | 818      |\n",
      "|    iterations      | 246      |\n",
      "|    time_elapsed    | 3029     |\n",
      "|    total_timesteps | 2479680  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0141     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 818         |\n",
      "|    iterations           | 248         |\n",
      "|    time_elapsed         | 3052        |\n",
      "|    total_timesteps      | 2499840     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009089254 |\n",
      "|    clip_fraction        | 0.23        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 11.9        |\n",
      "|    explained_variance   | 0.197       |\n",
      "|    learning_rate        | 0.00126     |\n",
      "|    loss                 | 0.000346    |\n",
      "|    n_updates            | 2470        |\n",
      "|    policy_gradient_loss | 0.00891     |\n",
      "|    std                  | 0.022       |\n",
      "|    value_loss           | 3.32e-09    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0136     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 819         |\n",
      "|    iterations           | 250         |\n",
      "|    time_elapsed         | 3074        |\n",
      "|    total_timesteps      | 2520000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019580293 |\n",
      "|    clip_fraction        | 0.267       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 12          |\n",
      "|    explained_variance   | -0.0909     |\n",
      "|    learning_rate        | 0.00125     |\n",
      "|    loss                 | 0.00526     |\n",
      "|    n_updates            | 2490        |\n",
      "|    policy_gradient_loss | 0.0157      |\n",
      "|    std                  | 0.0219      |\n",
      "|    value_loss           | 1.96e-09    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2540160, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.00195    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2540160     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008164223 |\n",
      "|    clip_fraction        | 0.266       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 12          |\n",
      "|    explained_variance   | 0.13        |\n",
      "|    learning_rate        | 0.00123     |\n",
      "|    loss                 | -0.00201    |\n",
      "|    n_updates            | 2510        |\n",
      "|    policy_gradient_loss | 0.0108      |\n",
      "|    std                  | 0.0213      |\n",
      "|    value_loss           | 5.97e-09    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0134  |\n",
      "| time/              |          |\n",
      "|    fps             | 818      |\n",
      "|    iterations      | 252      |\n",
      "|    time_elapsed    | 3103     |\n",
      "|    total_timesteps | 2540160  |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.26e+03   |\n",
      "|    ep_rew_mean          | -0.0136    |\n",
      "| time/                   |            |\n",
      "|    fps                  | 818        |\n",
      "|    iterations           | 254        |\n",
      "|    time_elapsed         | 3126       |\n",
      "|    total_timesteps      | 2560320    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01301357 |\n",
      "|    clip_fraction        | 0.225      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 12.1       |\n",
      "|    explained_variance   | -0.343     |\n",
      "|    learning_rate        | 0.00122    |\n",
      "|    loss                 | 0.00907    |\n",
      "|    n_updates            | 2530       |\n",
      "|    policy_gradient_loss | 0.00961    |\n",
      "|    std                  | 0.0208     |\n",
      "|    value_loss           | 1.05e-08   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0137     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 819         |\n",
      "|    iterations           | 256         |\n",
      "|    time_elapsed         | 3148        |\n",
      "|    total_timesteps      | 2580480     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015649812 |\n",
      "|    clip_fraction        | 0.321       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 12.1        |\n",
      "|    explained_variance   | -0.265      |\n",
      "|    learning_rate        | 0.00121     |\n",
      "|    loss                 | 0.0099      |\n",
      "|    n_updates            | 2550        |\n",
      "|    policy_gradient_loss | 0.0159      |\n",
      "|    std                  | 0.0206      |\n",
      "|    value_loss           | 2.95e-09    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2600640, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0059     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2600640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010149913 |\n",
      "|    clip_fraction        | 0.239       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 12.2        |\n",
      "|    explained_variance   | 0.0951      |\n",
      "|    learning_rate        | 0.0012      |\n",
      "|    loss                 | 0.0114      |\n",
      "|    n_updates            | 2570        |\n",
      "|    policy_gradient_loss | 0.00853     |\n",
      "|    std                  | 0.0203      |\n",
      "|    value_loss           | 3.97e-09    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0132  |\n",
      "| time/              |          |\n",
      "|    fps             | 818      |\n",
      "|    iterations      | 258      |\n",
      "|    time_elapsed    | 3177     |\n",
      "|    total_timesteps | 2600640  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0129     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 818         |\n",
      "|    iterations           | 260         |\n",
      "|    time_elapsed         | 3200        |\n",
      "|    total_timesteps      | 2620800     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015178664 |\n",
      "|    clip_fraction        | 0.265       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 12.2        |\n",
      "|    explained_variance   | 0.0463      |\n",
      "|    learning_rate        | 0.00119     |\n",
      "|    loss                 | 0.00704     |\n",
      "|    n_updates            | 2590        |\n",
      "|    policy_gradient_loss | 0.00445     |\n",
      "|    std                  | 0.0199      |\n",
      "|    value_loss           | 6.24e-09    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0126     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 819         |\n",
      "|    iterations           | 262         |\n",
      "|    time_elapsed         | 3222        |\n",
      "|    total_timesteps      | 2640960     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018949332 |\n",
      "|    clip_fraction        | 0.231       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 12.3        |\n",
      "|    explained_variance   | -0.594      |\n",
      "|    learning_rate        | 0.00118     |\n",
      "|    loss                 | 0.0174      |\n",
      "|    n_updates            | 2610        |\n",
      "|    policy_gradient_loss | 0.00868     |\n",
      "|    std                  | 0.0197      |\n",
      "|    value_loss           | 2.29e-09    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2661120, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.00779    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2661120     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018796638 |\n",
      "|    clip_fraction        | 0.263       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 12.3        |\n",
      "|    explained_variance   | 0.0259      |\n",
      "|    learning_rate        | 0.00117     |\n",
      "|    loss                 | -0.00375    |\n",
      "|    n_updates            | 2630        |\n",
      "|    policy_gradient_loss | 0.00554     |\n",
      "|    std                  | 0.0196      |\n",
      "|    value_loss           | 9.86e-09    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0126  |\n",
      "| time/              |          |\n",
      "|    fps             | 818      |\n",
      "|    iterations      | 264      |\n",
      "|    time_elapsed    | 3251     |\n",
      "|    total_timesteps | 2661120  |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.26e+03   |\n",
      "|    ep_rew_mean          | -0.0125    |\n",
      "| time/                   |            |\n",
      "|    fps                  | 818        |\n",
      "|    iterations           | 266        |\n",
      "|    time_elapsed         | 3274       |\n",
      "|    total_timesteps      | 2681280    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02546969 |\n",
      "|    clip_fraction        | 0.325      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 12.3       |\n",
      "|    explained_variance   | -0.174     |\n",
      "|    learning_rate        | 0.00116    |\n",
      "|    loss                 | 0.00612    |\n",
      "|    n_updates            | 2650       |\n",
      "|    policy_gradient_loss | 0.0149     |\n",
      "|    std                  | 0.0195     |\n",
      "|    value_loss           | 3.61e-09   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0124     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 819         |\n",
      "|    iterations           | 268         |\n",
      "|    time_elapsed         | 3296        |\n",
      "|    total_timesteps      | 2701440     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010670373 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 12.3        |\n",
      "|    explained_variance   | -0.311      |\n",
      "|    learning_rate        | 0.00115     |\n",
      "|    loss                 | 0.00392     |\n",
      "|    n_updates            | 2670        |\n",
      "|    policy_gradient_loss | 0.0197      |\n",
      "|    std                  | 0.0193      |\n",
      "|    value_loss           | 2.76e-09    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2721600, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.00756    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2721600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.044205684 |\n",
      "|    clip_fraction        | 0.404       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 12.4        |\n",
      "|    explained_variance   | -0.183      |\n",
      "|    learning_rate        | 0.00114     |\n",
      "|    loss                 | 0.0551      |\n",
      "|    n_updates            | 2690        |\n",
      "|    policy_gradient_loss | 0.023       |\n",
      "|    std                  | 0.0191      |\n",
      "|    value_loss           | 5.02e-09    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0123  |\n",
      "| time/              |          |\n",
      "|    fps             | 818      |\n",
      "|    iterations      | 270      |\n",
      "|    time_elapsed    | 3325     |\n",
      "|    total_timesteps | 2721600  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0123     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 818         |\n",
      "|    iterations           | 272         |\n",
      "|    time_elapsed         | 3348        |\n",
      "|    total_timesteps      | 2741760     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021512419 |\n",
      "|    clip_fraction        | 0.314       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 12.4        |\n",
      "|    explained_variance   | -0.191      |\n",
      "|    learning_rate        | 0.00113     |\n",
      "|    loss                 | 0.0157      |\n",
      "|    n_updates            | 2710        |\n",
      "|    policy_gradient_loss | 0.0162      |\n",
      "|    std                  | 0.0189      |\n",
      "|    value_loss           | 4.16e-09    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0126     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 819         |\n",
      "|    iterations           | 274         |\n",
      "|    time_elapsed         | 3370        |\n",
      "|    total_timesteps      | 2761920     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026974322 |\n",
      "|    clip_fraction        | 0.233       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 12.4        |\n",
      "|    explained_variance   | -0.0695     |\n",
      "|    learning_rate        | 0.00112     |\n",
      "|    loss                 | 0.0206      |\n",
      "|    n_updates            | 2730        |\n",
      "|    policy_gradient_loss | 0.00335     |\n",
      "|    std                  | 0.0185      |\n",
      "|    value_loss           | 1.07e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2782080, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.00661    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2782080     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011032781 |\n",
      "|    clip_fraction        | 0.255       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 12.5        |\n",
      "|    explained_variance   | 0.0414      |\n",
      "|    learning_rate        | 0.00111     |\n",
      "|    loss                 | -0.00539    |\n",
      "|    n_updates            | 2750        |\n",
      "|    policy_gradient_loss | 0.00449     |\n",
      "|    std                  | 0.0183      |\n",
      "|    value_loss           | 7.64e-09    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0129  |\n",
      "| time/              |          |\n",
      "|    fps             | 818      |\n",
      "|    iterations      | 276      |\n",
      "|    time_elapsed    | 3399     |\n",
      "|    total_timesteps | 2782080  |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.26e+03   |\n",
      "|    ep_rew_mean          | -0.0127    |\n",
      "| time/                   |            |\n",
      "|    fps                  | 818        |\n",
      "|    iterations           | 278        |\n",
      "|    time_elapsed         | 3422       |\n",
      "|    total_timesteps      | 2802240    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01827851 |\n",
      "|    clip_fraction        | 0.359      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 12.5       |\n",
      "|    explained_variance   | 0.0862     |\n",
      "|    learning_rate        | 0.0011     |\n",
      "|    loss                 | 0.0135     |\n",
      "|    n_updates            | 2770       |\n",
      "|    policy_gradient_loss | 0.0237     |\n",
      "|    std                  | 0.0181     |\n",
      "|    value_loss           | 3.54e-09   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0128     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 819         |\n",
      "|    iterations           | 280         |\n",
      "|    time_elapsed         | 3444        |\n",
      "|    total_timesteps      | 2822400     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036880977 |\n",
      "|    clip_fraction        | 0.362       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 12.5        |\n",
      "|    explained_variance   | -0.186      |\n",
      "|    learning_rate        | 0.00109     |\n",
      "|    loss                 | 0.0182      |\n",
      "|    n_updates            | 2790        |\n",
      "|    policy_gradient_loss | 0.0161      |\n",
      "|    std                  | 0.0179      |\n",
      "|    value_loss           | 5.94e-09    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2842560, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0036     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2842560     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034724545 |\n",
      "|    clip_fraction        | 0.28        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 12.6        |\n",
      "|    explained_variance   | 0.0324      |\n",
      "|    learning_rate        | 0.00108     |\n",
      "|    loss                 | 0.00233     |\n",
      "|    n_updates            | 2810        |\n",
      "|    policy_gradient_loss | 0.00678     |\n",
      "|    std                  | 0.0176      |\n",
      "|    value_loss           | 1.91e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0127  |\n",
      "| time/              |          |\n",
      "|    fps             | 818      |\n",
      "|    iterations      | 282      |\n",
      "|    time_elapsed    | 3473     |\n",
      "|    total_timesteps | 2842560  |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.26e+03   |\n",
      "|    ep_rew_mean          | -0.0124    |\n",
      "| time/                   |            |\n",
      "|    fps                  | 818        |\n",
      "|    iterations           | 284        |\n",
      "|    time_elapsed         | 3496       |\n",
      "|    total_timesteps      | 2862720    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02018042 |\n",
      "|    clip_fraction        | 0.25       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 12.6       |\n",
      "|    explained_variance   | 0.0544     |\n",
      "|    learning_rate        | 0.00107    |\n",
      "|    loss                 | 0.0249     |\n",
      "|    n_updates            | 2830       |\n",
      "|    policy_gradient_loss | 0.00792    |\n",
      "|    std                  | 0.0176     |\n",
      "|    value_loss           | 6.39e-09   |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.26e+03   |\n",
      "|    ep_rew_mean          | -0.0122    |\n",
      "| time/                   |            |\n",
      "|    fps                  | 819        |\n",
      "|    iterations           | 286        |\n",
      "|    time_elapsed         | 3518       |\n",
      "|    total_timesteps      | 2882880    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02703237 |\n",
      "|    clip_fraction        | 0.261      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 12.7       |\n",
      "|    explained_variance   | -0.0113    |\n",
      "|    learning_rate        | 0.00106    |\n",
      "|    loss                 | 0.00171    |\n",
      "|    n_updates            | 2850       |\n",
      "|    policy_gradient_loss | 0.0118     |\n",
      "|    std                  | 0.0173     |\n",
      "|    value_loss           | 4.19e-09   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=2903040, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00518     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2903040      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0113431495 |\n",
      "|    clip_fraction        | 0.215        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 12.7         |\n",
      "|    explained_variance   | -0.144       |\n",
      "|    learning_rate        | 0.00105      |\n",
      "|    loss                 | 0.00481      |\n",
      "|    n_updates            | 2870         |\n",
      "|    policy_gradient_loss | 0.00696      |\n",
      "|    std                  | 0.017        |\n",
      "|    value_loss           | 4.73e-09     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0118  |\n",
      "| time/              |          |\n",
      "|    fps             | 818      |\n",
      "|    iterations      | 288      |\n",
      "|    time_elapsed    | 3547     |\n",
      "|    total_timesteps | 2903040  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0112     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 818         |\n",
      "|    iterations           | 290         |\n",
      "|    time_elapsed         | 3570        |\n",
      "|    total_timesteps      | 2923200     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041258026 |\n",
      "|    clip_fraction        | 0.232       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 12.8        |\n",
      "|    explained_variance   | -0.0836     |\n",
      "|    learning_rate        | 0.00104     |\n",
      "|    loss                 | -4.09e-05   |\n",
      "|    n_updates            | 2890        |\n",
      "|    policy_gradient_loss | 0.00618     |\n",
      "|    std                  | 0.0165      |\n",
      "|    value_loss           | 1.27e-08    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0108     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 819         |\n",
      "|    iterations           | 292         |\n",
      "|    time_elapsed         | 3593        |\n",
      "|    total_timesteps      | 2943360     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032951236 |\n",
      "|    clip_fraction        | 0.303       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 12.8        |\n",
      "|    explained_variance   | -0.26       |\n",
      "|    learning_rate        | 0.00103     |\n",
      "|    loss                 | 0.0298      |\n",
      "|    n_updates            | 2910        |\n",
      "|    policy_gradient_loss | 0.0145      |\n",
      "|    std                  | 0.0162      |\n",
      "|    value_loss           | 3.13e-09    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2963520, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.00459    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2963520     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.055157892 |\n",
      "|    clip_fraction        | 0.327       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 12.9        |\n",
      "|    explained_variance   | -0.402      |\n",
      "|    learning_rate        | 0.00102     |\n",
      "|    loss                 | 0.0726      |\n",
      "|    n_updates            | 2930        |\n",
      "|    policy_gradient_loss | 0.0233      |\n",
      "|    std                  | 0.016       |\n",
      "|    value_loss           | 2.31e-09    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0105  |\n",
      "| time/              |          |\n",
      "|    fps             | 818      |\n",
      "|    iterations      | 294      |\n",
      "|    time_elapsed    | 3621     |\n",
      "|    total_timesteps | 2963520  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0104     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 818         |\n",
      "|    iterations           | 296         |\n",
      "|    time_elapsed         | 3644        |\n",
      "|    total_timesteps      | 2983680     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025062563 |\n",
      "|    clip_fraction        | 0.274       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 12.9        |\n",
      "|    explained_variance   | 0.116       |\n",
      "|    learning_rate        | 0.00101     |\n",
      "|    loss                 | 0.00938     |\n",
      "|    n_updates            | 2950        |\n",
      "|    policy_gradient_loss | 0.0119      |\n",
      "|    std                  | 0.0158      |\n",
      "|    value_loss           | 3.9e-09     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0102     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 819         |\n",
      "|    iterations           | 298         |\n",
      "|    time_elapsed         | 3666        |\n",
      "|    total_timesteps      | 3003840     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034302477 |\n",
      "|    clip_fraction        | 0.448       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 13          |\n",
      "|    explained_variance   | -0.626      |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 0.0445      |\n",
      "|    n_updates            | 2970        |\n",
      "|    policy_gradient_loss | 0.0353      |\n",
      "|    std                  | 0.0156      |\n",
      "|    value_loss           | 1.86e-09    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3024000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.00414    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3024000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019833602 |\n",
      "|    clip_fraction        | 0.266       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 13          |\n",
      "|    explained_variance   | 0.0853      |\n",
      "|    learning_rate        | 0.000993    |\n",
      "|    loss                 | 0.0164      |\n",
      "|    n_updates            | 2990        |\n",
      "|    policy_gradient_loss | 0.00846     |\n",
      "|    std                  | 0.0154      |\n",
      "|    value_loss           | 4.99e-09    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0102  |\n",
      "| time/              |          |\n",
      "|    fps             | 818      |\n",
      "|    iterations      | 300      |\n",
      "|    time_elapsed    | 3695     |\n",
      "|    total_timesteps | 3024000  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0098     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 818         |\n",
      "|    iterations           | 302         |\n",
      "|    time_elapsed         | 3718        |\n",
      "|    total_timesteps      | 3044160     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023299988 |\n",
      "|    clip_fraction        | 0.249       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 13          |\n",
      "|    explained_variance   | -0.253      |\n",
      "|    learning_rate        | 0.000983    |\n",
      "|    loss                 | 0.0365      |\n",
      "|    n_updates            | 3010        |\n",
      "|    policy_gradient_loss | 0.0106      |\n",
      "|    std                  | 0.0151      |\n",
      "|    value_loss           | 3.03e-09    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.26e+03   |\n",
      "|    ep_rew_mean          | -0.00966   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 818        |\n",
      "|    iterations           | 304        |\n",
      "|    time_elapsed         | 3741       |\n",
      "|    total_timesteps      | 3064320    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02900018 |\n",
      "|    clip_fraction        | 0.473      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 13.1       |\n",
      "|    explained_variance   | -0.36      |\n",
      "|    learning_rate        | 0.000973   |\n",
      "|    loss                 | 0.00427    |\n",
      "|    n_updates            | 3030       |\n",
      "|    policy_gradient_loss | 0.0323     |\n",
      "|    std                  | 0.0149     |\n",
      "|    value_loss           | 4.57e-09   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=3084480, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.00249    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3084480     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011069227 |\n",
      "|    clip_fraction        | 0.292       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 13.1        |\n",
      "|    explained_variance   | 0.0463      |\n",
      "|    learning_rate        | 0.000963    |\n",
      "|    loss                 | 0.00786     |\n",
      "|    n_updates            | 3050        |\n",
      "|    policy_gradient_loss | 0.0149      |\n",
      "|    std                  | 0.0147      |\n",
      "|    value_loss           | 3.17e-09    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.00945 |\n",
      "| time/              |          |\n",
      "|    fps             | 818      |\n",
      "|    iterations      | 306      |\n",
      "|    time_elapsed    | 3770     |\n",
      "|    total_timesteps | 3084480  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.00948    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 818         |\n",
      "|    iterations           | 308         |\n",
      "|    time_elapsed         | 3793        |\n",
      "|    total_timesteps      | 3104640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023712026 |\n",
      "|    clip_fraction        | 0.22        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 13.2        |\n",
      "|    explained_variance   | -0.162      |\n",
      "|    learning_rate        | 0.000953    |\n",
      "|    loss                 | 0.00642     |\n",
      "|    n_updates            | 3070        |\n",
      "|    policy_gradient_loss | 0.00573     |\n",
      "|    std                  | 0.0146      |\n",
      "|    value_loss           | 1.42e-08    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.00932    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 818         |\n",
      "|    iterations           | 310         |\n",
      "|    time_elapsed         | 3815        |\n",
      "|    total_timesteps      | 3124800     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023141678 |\n",
      "|    clip_fraction        | 0.25        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 13.2        |\n",
      "|    explained_variance   | -0.638      |\n",
      "|    learning_rate        | 0.000943    |\n",
      "|    loss                 | 0.00231     |\n",
      "|    n_updates            | 3090        |\n",
      "|    policy_gradient_loss | 0.0082      |\n",
      "|    std                  | 0.0144      |\n",
      "|    value_loss           | 2.53e-09    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3144960, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.00214    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3144960     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021871243 |\n",
      "|    clip_fraction        | 0.217       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 13.3        |\n",
      "|    explained_variance   | 0.0967      |\n",
      "|    learning_rate        | 0.000933    |\n",
      "|    loss                 | 0.0229      |\n",
      "|    n_updates            | 3110        |\n",
      "|    policy_gradient_loss | 0.0142      |\n",
      "|    std                  | 0.0142      |\n",
      "|    value_loss           | 1.5e-09     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.00889 |\n",
      "| time/              |          |\n",
      "|    fps             | 818      |\n",
      "|    iterations      | 312      |\n",
      "|    time_elapsed    | 3844     |\n",
      "|    total_timesteps | 3144960  |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.26e+03     |\n",
      "|    ep_rew_mean          | -0.00886     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 818          |\n",
      "|    iterations           | 314          |\n",
      "|    time_elapsed         | 3867         |\n",
      "|    total_timesteps      | 3165120      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0102284225 |\n",
      "|    clip_fraction        | 0.196        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 13.3         |\n",
      "|    explained_variance   | -0.378       |\n",
      "|    learning_rate        | 0.000922     |\n",
      "|    loss                 | 0.014        |\n",
      "|    n_updates            | 3130         |\n",
      "|    policy_gradient_loss | 0.00907      |\n",
      "|    std                  | 0.0139       |\n",
      "|    value_loss           | 2.62e-09     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.00877    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 818         |\n",
      "|    iterations           | 316         |\n",
      "|    time_elapsed         | 3889        |\n",
      "|    total_timesteps      | 3185280     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022262288 |\n",
      "|    clip_fraction        | 0.32        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 13.4        |\n",
      "|    explained_variance   | -0.231      |\n",
      "|    learning_rate        | 0.000912    |\n",
      "|    loss                 | 0.0189      |\n",
      "|    n_updates            | 3150        |\n",
      "|    policy_gradient_loss | 0.0188      |\n",
      "|    std                  | 0.0137      |\n",
      "|    value_loss           | 2.83e-09    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3205440, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.00341   |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3205440    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02411595 |\n",
      "|    clip_fraction        | 0.295      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 13.4       |\n",
      "|    explained_variance   | -0.187     |\n",
      "|    learning_rate        | 0.000902   |\n",
      "|    loss                 | 0.0228     |\n",
      "|    n_updates            | 3170       |\n",
      "|    policy_gradient_loss | 0.0148     |\n",
      "|    std                  | 0.0134     |\n",
      "|    value_loss           | 3.55e-09   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.00875 |\n",
      "| time/              |          |\n",
      "|    fps             | 818      |\n",
      "|    iterations      | 318      |\n",
      "|    time_elapsed    | 3918     |\n",
      "|    total_timesteps | 3205440  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.00853    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 818         |\n",
      "|    iterations           | 320         |\n",
      "|    time_elapsed         | 3941        |\n",
      "|    total_timesteps      | 3225600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021246912 |\n",
      "|    clip_fraction        | 0.246       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 13.5        |\n",
      "|    explained_variance   | -0.112      |\n",
      "|    learning_rate        | 0.000892    |\n",
      "|    loss                 | 0.00564     |\n",
      "|    n_updates            | 3190        |\n",
      "|    policy_gradient_loss | 0.00748     |\n",
      "|    std                  | 0.0131      |\n",
      "|    value_loss           | 7.36e-09    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.26e+03   |\n",
      "|    ep_rew_mean          | -0.00849   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 818        |\n",
      "|    iterations           | 322        |\n",
      "|    time_elapsed         | 3963       |\n",
      "|    total_timesteps      | 3245760    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01867065 |\n",
      "|    clip_fraction        | 0.374      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 13.5       |\n",
      "|    explained_variance   | -0.0722    |\n",
      "|    learning_rate        | 0.000882   |\n",
      "|    loss                 | 0.00448    |\n",
      "|    n_updates            | 3210       |\n",
      "|    policy_gradient_loss | 0.0157     |\n",
      "|    std                  | 0.0129     |\n",
      "|    value_loss           | 1.26e-08   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=3265920, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.00341   |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3265920    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08840522 |\n",
      "|    clip_fraction        | 0.383      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 13.6       |\n",
      "|    explained_variance   | -1.22      |\n",
      "|    learning_rate        | 0.000872   |\n",
      "|    loss                 | 0.0624     |\n",
      "|    n_updates            | 3230       |\n",
      "|    policy_gradient_loss | 0.027      |\n",
      "|    std                  | 0.0127     |\n",
      "|    value_loss           | 2.79e-09   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.00848 |\n",
      "| time/              |          |\n",
      "|    fps             | 817      |\n",
      "|    iterations      | 324      |\n",
      "|    time_elapsed    | 3993     |\n",
      "|    total_timesteps | 3265920  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.00848    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 817         |\n",
      "|    iterations           | 326         |\n",
      "|    time_elapsed         | 4018        |\n",
      "|    total_timesteps      | 3286080     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.084738076 |\n",
      "|    clip_fraction        | 0.348       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 13.6        |\n",
      "|    explained_variance   | -0.163      |\n",
      "|    learning_rate        | 0.000862    |\n",
      "|    loss                 | 0.0219      |\n",
      "|    n_updates            | 3250        |\n",
      "|    policy_gradient_loss | 0.0236      |\n",
      "|    std                  | 0.0124      |\n",
      "|    value_loss           | 4.53e-09    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.00828    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 817         |\n",
      "|    iterations           | 328         |\n",
      "|    time_elapsed         | 4044        |\n",
      "|    total_timesteps      | 3306240     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009711157 |\n",
      "|    clip_fraction        | 0.26        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 13.7        |\n",
      "|    explained_variance   | -0.117      |\n",
      "|    learning_rate        | 0.000852    |\n",
      "|    loss                 | 0.0037      |\n",
      "|    n_updates            | 3270        |\n",
      "|    policy_gradient_loss | 0.0086      |\n",
      "|    std                  | 0.0123      |\n",
      "|    value_loss           | 8.83e-09    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3326400, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.00149    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3326400     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018429518 |\n",
      "|    clip_fraction        | 0.379       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 13.7        |\n",
      "|    explained_variance   | -0.23       |\n",
      "|    learning_rate        | 0.000842    |\n",
      "|    loss                 | 0.0116      |\n",
      "|    n_updates            | 3290        |\n",
      "|    policy_gradient_loss | 0.0318      |\n",
      "|    std                  | 0.0121      |\n",
      "|    value_loss           | 1.49e-09    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.00816 |\n",
      "| time/              |          |\n",
      "|    fps             | 815      |\n",
      "|    iterations      | 330      |\n",
      "|    time_elapsed    | 4076     |\n",
      "|    total_timesteps | 3326400  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.00792    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 815         |\n",
      "|    iterations           | 332         |\n",
      "|    time_elapsed         | 4102        |\n",
      "|    total_timesteps      | 3346560     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011035034 |\n",
      "|    clip_fraction        | 0.27        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 13.8        |\n",
      "|    explained_variance   | -0.209      |\n",
      "|    learning_rate        | 0.000832    |\n",
      "|    loss                 | 0.000498    |\n",
      "|    n_updates            | 3310        |\n",
      "|    policy_gradient_loss | 0.00898     |\n",
      "|    std                  | 0.012       |\n",
      "|    value_loss           | 3.96e-09    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.00763    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 815         |\n",
      "|    iterations           | 334         |\n",
      "|    time_elapsed         | 4127        |\n",
      "|    total_timesteps      | 3366720     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028062593 |\n",
      "|    clip_fraction        | 0.322       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 13.8        |\n",
      "|    explained_variance   | -0.103      |\n",
      "|    learning_rate        | 0.000822    |\n",
      "|    loss                 | 0.0117      |\n",
      "|    n_updates            | 3330        |\n",
      "|    policy_gradient_loss | 0.011       |\n",
      "|    std                  | 0.0119      |\n",
      "|    value_loss           | 4.13e-09    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3386880, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0023    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3386880    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07717271 |\n",
      "|    clip_fraction        | 0.391      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 13.8       |\n",
      "|    explained_variance   | -1.38      |\n",
      "|    learning_rate        | 0.000812   |\n",
      "|    loss                 | 0.077      |\n",
      "|    n_updates            | 3350       |\n",
      "|    policy_gradient_loss | 0.0304     |\n",
      "|    std                  | 0.0117     |\n",
      "|    value_loss           | 1.31e-09   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.00737 |\n",
      "| time/              |          |\n",
      "|    fps             | 814      |\n",
      "|    iterations      | 336      |\n",
      "|    time_elapsed    | 4160     |\n",
      "|    total_timesteps | 3386880  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.00736    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 813         |\n",
      "|    iterations           | 338         |\n",
      "|    time_elapsed         | 4187        |\n",
      "|    total_timesteps      | 3407040     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024037244 |\n",
      "|    clip_fraction        | 0.245       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 13.9        |\n",
      "|    explained_variance   | -0.28       |\n",
      "|    learning_rate        | 0.000802    |\n",
      "|    loss                 | 0.00268     |\n",
      "|    n_updates            | 3370        |\n",
      "|    policy_gradient_loss | 0.0094      |\n",
      "|    std                  | 0.0116      |\n",
      "|    value_loss           | 7.43e-09    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.00748    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 813         |\n",
      "|    iterations           | 340         |\n",
      "|    time_elapsed         | 4213        |\n",
      "|    total_timesteps      | 3427200     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039935075 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 13.9        |\n",
      "|    explained_variance   | -0.598      |\n",
      "|    learning_rate        | 0.000791    |\n",
      "|    loss                 | 0.0189      |\n",
      "|    n_updates            | 3390        |\n",
      "|    policy_gradient_loss | 0.0244      |\n",
      "|    std                  | 0.0116      |\n",
      "|    value_loss           | 2.45e-09    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3447360, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.00268    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3447360     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.062833935 |\n",
      "|    clip_fraction        | 0.323       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 13.9        |\n",
      "|    explained_variance   | -0.165      |\n",
      "|    learning_rate        | 0.000781    |\n",
      "|    loss                 | 0.058       |\n",
      "|    n_updates            | 3410        |\n",
      "|    policy_gradient_loss | 0.0173      |\n",
      "|    std                  | 0.0114      |\n",
      "|    value_loss           | 5.65e-09    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.00747 |\n",
      "| time/              |          |\n",
      "|    fps             | 812      |\n",
      "|    iterations      | 342      |\n",
      "|    time_elapsed    | 4245     |\n",
      "|    total_timesteps | 3447360  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.00735    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 811         |\n",
      "|    iterations           | 344         |\n",
      "|    time_elapsed         | 4270        |\n",
      "|    total_timesteps      | 3467520     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011174726 |\n",
      "|    clip_fraction        | 0.483       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 13.9        |\n",
      "|    explained_variance   | -0.0917     |\n",
      "|    learning_rate        | 0.000771    |\n",
      "|    loss                 | 0.00358     |\n",
      "|    n_updates            | 3430        |\n",
      "|    policy_gradient_loss | 0.037       |\n",
      "|    std                  | 0.0114      |\n",
      "|    value_loss           | 4.82e-09    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.00722    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 812         |\n",
      "|    iterations           | 346         |\n",
      "|    time_elapsed         | 4294        |\n",
      "|    total_timesteps      | 3487680     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014047272 |\n",
      "|    clip_fraction        | 0.211       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 14          |\n",
      "|    explained_variance   | -2.43       |\n",
      "|    learning_rate        | 0.000761    |\n",
      "|    loss                 | 0.000422    |\n",
      "|    n_updates            | 3450        |\n",
      "|    policy_gradient_loss | 0.0102      |\n",
      "|    std                  | 0.0113      |\n",
      "|    value_loss           | 1.76e-09    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3507840, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.00286   |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3507840    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01414392 |\n",
      "|    clip_fraction        | 0.444      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 14         |\n",
      "|    explained_variance   | -0.888     |\n",
      "|    learning_rate        | 0.000751   |\n",
      "|    loss                 | 0.0142     |\n",
      "|    n_updates            | 3470       |\n",
      "|    policy_gradient_loss | 0.0463     |\n",
      "|    std                  | 0.0113     |\n",
      "|    value_loss           | 1.64e-09   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.00722 |\n",
      "| time/              |          |\n",
      "|    fps             | 811      |\n",
      "|    iterations      | 348      |\n",
      "|    time_elapsed    | 4323     |\n",
      "|    total_timesteps | 3507840  |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.26e+03   |\n",
      "|    ep_rew_mean          | -0.00722   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 811        |\n",
      "|    iterations           | 350        |\n",
      "|    time_elapsed         | 4345       |\n",
      "|    total_timesteps      | 3528000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01245269 |\n",
      "|    clip_fraction        | 0.202      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 14         |\n",
      "|    explained_variance   | -0.521     |\n",
      "|    learning_rate        | 0.000741   |\n",
      "|    loss                 | 0.00184    |\n",
      "|    n_updates            | 3490       |\n",
      "|    policy_gradient_loss | 0.00545    |\n",
      "|    std                  | 0.0112     |\n",
      "|    value_loss           | 3.81e-09   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.00723    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 812         |\n",
      "|    iterations           | 352         |\n",
      "|    time_elapsed         | 4368        |\n",
      "|    total_timesteps      | 3548160     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019123014 |\n",
      "|    clip_fraction        | 0.255       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 14          |\n",
      "|    explained_variance   | -0.202      |\n",
      "|    learning_rate        | 0.000731    |\n",
      "|    loss                 | 0.00181     |\n",
      "|    n_updates            | 3510        |\n",
      "|    policy_gradient_loss | 0.00762     |\n",
      "|    std                  | 0.0112      |\n",
      "|    value_loss           | 5.44e-09    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3568320, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.00469    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3568320     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.056237347 |\n",
      "|    clip_fraction        | 0.333       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 14.1        |\n",
      "|    explained_variance   | -0.544      |\n",
      "|    learning_rate        | 0.000721    |\n",
      "|    loss                 | 0.0268      |\n",
      "|    n_updates            | 3530        |\n",
      "|    policy_gradient_loss | 0.0199      |\n",
      "|    std                  | 0.0111      |\n",
      "|    value_loss           | 2.27e-09    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.00715 |\n",
      "| time/              |          |\n",
      "|    fps             | 811      |\n",
      "|    iterations      | 354      |\n",
      "|    time_elapsed    | 4397     |\n",
      "|    total_timesteps | 3568320  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.00737    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 811         |\n",
      "|    iterations           | 356         |\n",
      "|    time_elapsed         | 4419        |\n",
      "|    total_timesteps      | 3588480     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028008318 |\n",
      "|    clip_fraction        | 0.497       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 14.1        |\n",
      "|    explained_variance   | -1.1        |\n",
      "|    learning_rate        | 0.000711    |\n",
      "|    loss                 | 0.0279      |\n",
      "|    n_updates            | 3550        |\n",
      "|    policy_gradient_loss | 0.0475      |\n",
      "|    std                  | 0.011       |\n",
      "|    value_loss           | 2.4e-09     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.00787    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 812         |\n",
      "|    iterations           | 358         |\n",
      "|    time_elapsed         | 4442        |\n",
      "|    total_timesteps      | 3608640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041698124 |\n",
      "|    clip_fraction        | 0.252       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 14.1        |\n",
      "|    explained_variance   | -1.18       |\n",
      "|    learning_rate        | 0.000701    |\n",
      "|    loss                 | 0.00369     |\n",
      "|    n_updates            | 3570        |\n",
      "|    policy_gradient_loss | 0.0112      |\n",
      "|    std                  | 0.011       |\n",
      "|    value_loss           | 3.63e-09    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3628800, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1.26e+03  |\n",
      "|    mean_reward          | -0.00401  |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 3628800   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0174855 |\n",
      "|    clip_fraction        | 0.29      |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | 14.1      |\n",
      "|    explained_variance   | 0.0461    |\n",
      "|    learning_rate        | 0.000691  |\n",
      "|    loss                 | 0.0096    |\n",
      "|    n_updates            | 3590      |\n",
      "|    policy_gradient_loss | 0.00947   |\n",
      "|    std                  | 0.0109    |\n",
      "|    value_loss           | 1.54e-08  |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.00827 |\n",
      "| time/              |          |\n",
      "|    fps             | 811      |\n",
      "|    iterations      | 360      |\n",
      "|    time_elapsed    | 4471     |\n",
      "|    total_timesteps | 3628800  |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.26e+03   |\n",
      "|    ep_rew_mean          | -0.0082    |\n",
      "| time/                   |            |\n",
      "|    fps                  | 811        |\n",
      "|    iterations           | 362        |\n",
      "|    time_elapsed         | 4494       |\n",
      "|    total_timesteps      | 3648960    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02241565 |\n",
      "|    clip_fraction        | 0.429      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | -0.385     |\n",
      "|    learning_rate        | 0.000681   |\n",
      "|    loss                 | 0.00604    |\n",
      "|    n_updates            | 3610       |\n",
      "|    policy_gradient_loss | 0.0359     |\n",
      "|    std                  | 0.0108     |\n",
      "|    value_loss           | 2.1e-09    |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.00795    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 812         |\n",
      "|    iterations           | 364         |\n",
      "|    time_elapsed         | 4516        |\n",
      "|    total_timesteps      | 3669120     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020864362 |\n",
      "|    clip_fraction        | 0.385       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 14.2        |\n",
      "|    explained_variance   | -0.766      |\n",
      "|    learning_rate        | 0.00067     |\n",
      "|    loss                 | 0.0117      |\n",
      "|    n_updates            | 3630        |\n",
      "|    policy_gradient_loss | 0.0268      |\n",
      "|    std                  | 0.0108      |\n",
      "|    value_loss           | 1.72e-09    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3689280, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.00332   |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3689280    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01876699 |\n",
      "|    clip_fraction        | 0.249      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | -0.202     |\n",
      "|    learning_rate        | 0.00066    |\n",
      "|    loss                 | -0.00596   |\n",
      "|    n_updates            | 3650       |\n",
      "|    policy_gradient_loss | 0.00394    |\n",
      "|    std                  | 0.0107     |\n",
      "|    value_loss           | 2.41e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.00829 |\n",
      "| time/              |          |\n",
      "|    fps             | 811      |\n",
      "|    iterations      | 366      |\n",
      "|    time_elapsed    | 4545     |\n",
      "|    total_timesteps | 3689280  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.00822    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 812         |\n",
      "|    iterations           | 368         |\n",
      "|    time_elapsed         | 4567        |\n",
      "|    total_timesteps      | 3709440     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015036057 |\n",
      "|    clip_fraction        | 0.294       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 14.2        |\n",
      "|    explained_variance   | -0.025      |\n",
      "|    learning_rate        | 0.00065     |\n",
      "|    loss                 | 0.00226     |\n",
      "|    n_updates            | 3670        |\n",
      "|    policy_gradient_loss | 0.0114      |\n",
      "|    std                  | 0.0106      |\n",
      "|    value_loss           | 7.06e-09    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.00784    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 812         |\n",
      "|    iterations           | 370         |\n",
      "|    time_elapsed         | 4590        |\n",
      "|    total_timesteps      | 3729600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017511899 |\n",
      "|    clip_fraction        | 0.322       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 14.2        |\n",
      "|    explained_variance   | -0.456      |\n",
      "|    learning_rate        | 0.00064     |\n",
      "|    loss                 | 0.000747    |\n",
      "|    n_updates            | 3690        |\n",
      "|    policy_gradient_loss | 0.0163      |\n",
      "|    std                  | 0.0106      |\n",
      "|    value_loss           | 3.61e-09    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3749760, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.00412    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3749760     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019218072 |\n",
      "|    clip_fraction        | 0.232       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 14.3        |\n",
      "|    explained_variance   | -0.476      |\n",
      "|    learning_rate        | 0.00063     |\n",
      "|    loss                 | 0.0162      |\n",
      "|    n_updates            | 3710        |\n",
      "|    policy_gradient_loss | 0.0109      |\n",
      "|    std                  | 0.0106      |\n",
      "|    value_loss           | 3.3e-09     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.00735 |\n",
      "| time/              |          |\n",
      "|    fps             | 811      |\n",
      "|    iterations      | 372      |\n",
      "|    time_elapsed    | 4618     |\n",
      "|    total_timesteps | 3749760  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.00719    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 812         |\n",
      "|    iterations           | 374         |\n",
      "|    time_elapsed         | 4641        |\n",
      "|    total_timesteps      | 3769920     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018831998 |\n",
      "|    clip_fraction        | 0.271       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 14.3        |\n",
      "|    explained_variance   | -0.652      |\n",
      "|    learning_rate        | 0.00062     |\n",
      "|    loss                 | 0.0187      |\n",
      "|    n_updates            | 3730        |\n",
      "|    policy_gradient_loss | 0.0115      |\n",
      "|    std                  | 0.0106      |\n",
      "|    value_loss           | 4.23e-09    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.00721    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 812         |\n",
      "|    iterations           | 376         |\n",
      "|    time_elapsed         | 4664        |\n",
      "|    total_timesteps      | 3790080     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017887963 |\n",
      "|    clip_fraction        | 0.236       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 14.3        |\n",
      "|    explained_variance   | -0.278      |\n",
      "|    learning_rate        | 0.00061     |\n",
      "|    loss                 | 0.014       |\n",
      "|    n_updates            | 3750        |\n",
      "|    policy_gradient_loss | 0.0068      |\n",
      "|    std                  | 0.0104      |\n",
      "|    value_loss           | 4.02e-09    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3810240, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.00416    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3810240     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012610429 |\n",
      "|    clip_fraction        | 0.278       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 14.4        |\n",
      "|    explained_variance   | -0.1        |\n",
      "|    learning_rate        | 0.0006      |\n",
      "|    loss                 | -0.00267    |\n",
      "|    n_updates            | 3770        |\n",
      "|    policy_gradient_loss | 0.0128      |\n",
      "|    std                  | 0.0102      |\n",
      "|    value_loss           | 5.64e-09    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.00697 |\n",
      "| time/              |          |\n",
      "|    fps             | 811      |\n",
      "|    iterations      | 378      |\n",
      "|    time_elapsed    | 4693     |\n",
      "|    total_timesteps | 3810240  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.00675    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 812         |\n",
      "|    iterations           | 380         |\n",
      "|    time_elapsed         | 4715        |\n",
      "|    total_timesteps      | 3830400     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020826146 |\n",
      "|    clip_fraction        | 0.281       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 14.4        |\n",
      "|    explained_variance   | -1.69       |\n",
      "|    learning_rate        | 0.00059     |\n",
      "|    loss                 | 0.00574     |\n",
      "|    n_updates            | 3790        |\n",
      "|    policy_gradient_loss | 0.0139      |\n",
      "|    std                  | 0.0101      |\n",
      "|    value_loss           | 1.22e-09    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.00673    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 812         |\n",
      "|    iterations           | 382         |\n",
      "|    time_elapsed         | 4738        |\n",
      "|    total_timesteps      | 3850560     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021319805 |\n",
      "|    clip_fraction        | 0.325       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 14.4        |\n",
      "|    explained_variance   | -0.406      |\n",
      "|    learning_rate        | 0.00058     |\n",
      "|    loss                 | 0.0104      |\n",
      "|    n_updates            | 3810        |\n",
      "|    policy_gradient_loss | 0.0163      |\n",
      "|    std                  | 0.01        |\n",
      "|    value_loss           | 4.44e-09    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3870720, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00292     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3870720      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0156030385 |\n",
      "|    clip_fraction        | 0.305        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 14.5         |\n",
      "|    explained_variance   | -0.607       |\n",
      "|    learning_rate        | 0.00057      |\n",
      "|    loss                 | 0.000867     |\n",
      "|    n_updates            | 3830         |\n",
      "|    policy_gradient_loss | 0.0204       |\n",
      "|    std                  | 0.00989      |\n",
      "|    value_loss           | 2.27e-09     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.00675 |\n",
      "| time/              |          |\n",
      "|    fps             | 811      |\n",
      "|    iterations      | 384      |\n",
      "|    time_elapsed    | 4766     |\n",
      "|    total_timesteps | 3870720  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.00658    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 812         |\n",
      "|    iterations           | 386         |\n",
      "|    time_elapsed         | 4789        |\n",
      "|    total_timesteps      | 3890880     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036724318 |\n",
      "|    clip_fraction        | 0.267       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 14.5        |\n",
      "|    explained_variance   | -1.32       |\n",
      "|    learning_rate        | 0.00056     |\n",
      "|    loss                 | 0.031       |\n",
      "|    n_updates            | 3850        |\n",
      "|    policy_gradient_loss | 0.0119      |\n",
      "|    std                  | 0.00973     |\n",
      "|    value_loss           | 3.04e-09    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.00657    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 812         |\n",
      "|    iterations           | 388         |\n",
      "|    time_elapsed         | 4812        |\n",
      "|    total_timesteps      | 3911040     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016755614 |\n",
      "|    clip_fraction        | 0.206       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 14.5        |\n",
      "|    explained_variance   | -0.744      |\n",
      "|    learning_rate        | 0.00055     |\n",
      "|    loss                 | 0.0206      |\n",
      "|    n_updates            | 3870        |\n",
      "|    policy_gradient_loss | 0.0083      |\n",
      "|    std                  | 0.00971     |\n",
      "|    value_loss           | 2.93e-09    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3931200, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.00182    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3931200     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028272979 |\n",
      "|    clip_fraction        | 0.305       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 14.6        |\n",
      "|    explained_variance   | -1.51       |\n",
      "|    learning_rate        | 0.000539    |\n",
      "|    loss                 | 0.0416      |\n",
      "|    n_updates            | 3890        |\n",
      "|    policy_gradient_loss | 0.0181      |\n",
      "|    std                  | 0.00964     |\n",
      "|    value_loss           | 1.56e-09    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.00612 |\n",
      "| time/              |          |\n",
      "|    fps             | 811      |\n",
      "|    iterations      | 390      |\n",
      "|    time_elapsed    | 4841     |\n",
      "|    total_timesteps | 3931200  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.00613    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 812         |\n",
      "|    iterations           | 392         |\n",
      "|    time_elapsed         | 4864        |\n",
      "|    total_timesteps      | 3951360     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021425046 |\n",
      "|    clip_fraction        | 0.267       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 14.6        |\n",
      "|    explained_variance   | -0.947      |\n",
      "|    learning_rate        | 0.000529    |\n",
      "|    loss                 | 0.000864    |\n",
      "|    n_updates            | 3910        |\n",
      "|    policy_gradient_loss | 0.0133      |\n",
      "|    std                  | 0.00946     |\n",
      "|    value_loss           | 2.67e-09    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.26e+03     |\n",
      "|    ep_rew_mean          | -0.00608     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 812          |\n",
      "|    iterations           | 394          |\n",
      "|    time_elapsed         | 4886         |\n",
      "|    total_timesteps      | 3971520      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0149754565 |\n",
      "|    clip_fraction        | 0.314        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 14.6         |\n",
      "|    explained_variance   | -2.22        |\n",
      "|    learning_rate        | 0.000519     |\n",
      "|    loss                 | 0.0133       |\n",
      "|    n_updates            | 3930         |\n",
      "|    policy_gradient_loss | 0.0152       |\n",
      "|    std                  | 0.00946      |\n",
      "|    value_loss           | 5.23e-09     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3991680, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0035    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3991680    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08571456 |\n",
      "|    clip_fraction        | 0.296      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | -1.4       |\n",
      "|    learning_rate        | 0.000509   |\n",
      "|    loss                 | 0.000768   |\n",
      "|    n_updates            | 3950       |\n",
      "|    policy_gradient_loss | 0.0215     |\n",
      "|    std                  | 0.0094     |\n",
      "|    value_loss           | 1.74e-09   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.00592 |\n",
      "| time/              |          |\n",
      "|    fps             | 812      |\n",
      "|    iterations      | 396      |\n",
      "|    time_elapsed    | 4915     |\n",
      "|    total_timesteps | 3991680  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.00574    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 812         |\n",
      "|    iterations           | 398         |\n",
      "|    time_elapsed         | 4937        |\n",
      "|    total_timesteps      | 4011840     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024370521 |\n",
      "|    clip_fraction        | 0.409       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 14.7        |\n",
      "|    explained_variance   | -1.88       |\n",
      "|    learning_rate        | 0.000499    |\n",
      "|    loss                 | 0.00902     |\n",
      "|    n_updates            | 3970        |\n",
      "|    policy_gradient_loss | 0.0289      |\n",
      "|    std                  | 0.00931     |\n",
      "|    value_loss           | 2.32e-09    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.00553    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 812         |\n",
      "|    iterations           | 400         |\n",
      "|    time_elapsed         | 4960        |\n",
      "|    total_timesteps      | 4032000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016351916 |\n",
      "|    clip_fraction        | 0.194       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 14.7        |\n",
      "|    explained_variance   | -0.502      |\n",
      "|    learning_rate        | 0.000489    |\n",
      "|    loss                 | 0.00367     |\n",
      "|    n_updates            | 3990        |\n",
      "|    policy_gradient_loss | 0.00598     |\n",
      "|    std                  | 0.00924     |\n",
      "|    value_loss           | 3.37e-09    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4052160, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.00303   |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4052160    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04673153 |\n",
      "|    clip_fraction        | 0.248      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | -1.56      |\n",
      "|    learning_rate        | 0.000479   |\n",
      "|    loss                 | 0.0875     |\n",
      "|    n_updates            | 4010       |\n",
      "|    policy_gradient_loss | 0.0189     |\n",
      "|    std                  | 0.00921    |\n",
      "|    value_loss           | 1.78e-09   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.00552 |\n",
      "| time/              |          |\n",
      "|    fps             | 812      |\n",
      "|    iterations      | 402      |\n",
      "|    time_elapsed    | 4989     |\n",
      "|    total_timesteps | 4052160  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.00552    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 812         |\n",
      "|    iterations           | 404         |\n",
      "|    time_elapsed         | 5012        |\n",
      "|    total_timesteps      | 4072320     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021505306 |\n",
      "|    clip_fraction        | 0.247       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 14.7        |\n",
      "|    explained_variance   | -0.854      |\n",
      "|    learning_rate        | 0.000469    |\n",
      "|    loss                 | 0.0108      |\n",
      "|    n_updates            | 4030        |\n",
      "|    policy_gradient_loss | 0.0124      |\n",
      "|    std                  | 0.0091      |\n",
      "|    value_loss           | 1.85e-09    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.00525    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 812         |\n",
      "|    iterations           | 406         |\n",
      "|    time_elapsed         | 5035        |\n",
      "|    total_timesteps      | 4092480     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027971359 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 14.8        |\n",
      "|    explained_variance   | -0.875      |\n",
      "|    learning_rate        | 0.000459    |\n",
      "|    loss                 | 0.0162      |\n",
      "|    n_updates            | 4050        |\n",
      "|    policy_gradient_loss | 0.0228      |\n",
      "|    std                  | 0.00902     |\n",
      "|    value_loss           | 1.64e-09    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4112640, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.00176   |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4112640    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01914966 |\n",
      "|    clip_fraction        | 0.335      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | -1.6       |\n",
      "|    learning_rate        | 0.000449   |\n",
      "|    loss                 | 0.0103     |\n",
      "|    n_updates            | 4070       |\n",
      "|    policy_gradient_loss | 0.0224     |\n",
      "|    std                  | 0.00888    |\n",
      "|    value_loss           | 2.62e-09   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.00509 |\n",
      "| time/              |          |\n",
      "|    fps             | 812      |\n",
      "|    iterations      | 408      |\n",
      "|    time_elapsed    | 5063     |\n",
      "|    total_timesteps | 4112640  |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.26e+03     |\n",
      "|    ep_rew_mean          | -0.00512     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 812          |\n",
      "|    iterations           | 410          |\n",
      "|    time_elapsed         | 5086         |\n",
      "|    total_timesteps      | 4132800      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067666713 |\n",
      "|    clip_fraction        | 0.302        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 14.8         |\n",
      "|    explained_variance   | -1.42        |\n",
      "|    learning_rate        | 0.000439     |\n",
      "|    loss                 | 0.00162      |\n",
      "|    n_updates            | 4090         |\n",
      "|    policy_gradient_loss | 0.0195       |\n",
      "|    std                  | 0.00885      |\n",
      "|    value_loss           | 2.82e-09     |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.26e+03   |\n",
      "|    ep_rew_mean          | -0.00501   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 812        |\n",
      "|    iterations           | 412        |\n",
      "|    time_elapsed         | 5108       |\n",
      "|    total_timesteps      | 4152960    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07607003 |\n",
      "|    clip_fraction        | 0.274      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | -0.561     |\n",
      "|    learning_rate        | 0.000429   |\n",
      "|    loss                 | 0.0593     |\n",
      "|    n_updates            | 4110       |\n",
      "|    policy_gradient_loss | 0.0132     |\n",
      "|    std                  | 0.00881    |\n",
      "|    value_loss           | 3e-09      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=4173120, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.00233    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4173120     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039278757 |\n",
      "|    clip_fraction        | 0.295       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 14.9        |\n",
      "|    explained_variance   | -1.18       |\n",
      "|    learning_rate        | 0.000418    |\n",
      "|    loss                 | 0.0491      |\n",
      "|    n_updates            | 4130        |\n",
      "|    policy_gradient_loss | 0.0201      |\n",
      "|    std                  | 0.00881     |\n",
      "|    value_loss           | 1.68e-09    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.00513 |\n",
      "| time/              |          |\n",
      "|    fps             | 812      |\n",
      "|    iterations      | 414      |\n",
      "|    time_elapsed    | 5137     |\n",
      "|    total_timesteps | 4173120  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0052     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 812         |\n",
      "|    iterations           | 416         |\n",
      "|    time_elapsed         | 5159        |\n",
      "|    total_timesteps      | 4193280     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014449505 |\n",
      "|    clip_fraction        | 0.116       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 14.9        |\n",
      "|    explained_variance   | -0.416      |\n",
      "|    learning_rate        | 0.000408    |\n",
      "|    loss                 | 0.0092      |\n",
      "|    n_updates            | 4150        |\n",
      "|    policy_gradient_loss | 0.00247     |\n",
      "|    std                  | 0.00875     |\n",
      "|    value_loss           | 6.88e-09    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.00525    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 812         |\n",
      "|    iterations           | 418         |\n",
      "|    time_elapsed         | 5182        |\n",
      "|    total_timesteps      | 4213440     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036357563 |\n",
      "|    clip_fraction        | 0.234       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 14.9        |\n",
      "|    explained_variance   | -1.33       |\n",
      "|    learning_rate        | 0.000398    |\n",
      "|    loss                 | 0.0344      |\n",
      "|    n_updates            | 4170        |\n",
      "|    policy_gradient_loss | 0.0107      |\n",
      "|    std                  | 0.00871     |\n",
      "|    value_loss           | 3.18e-09    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4233600, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.00183    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4233600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014802141 |\n",
      "|    clip_fraction        | 0.172       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 14.9        |\n",
      "|    explained_variance   | -1.05       |\n",
      "|    learning_rate        | 0.000388    |\n",
      "|    loss                 | 0.00406     |\n",
      "|    n_updates            | 4190        |\n",
      "|    policy_gradient_loss | 0.00435     |\n",
      "|    std                  | 0.00871     |\n",
      "|    value_loss           | 7.78e-09    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.00522 |\n",
      "| time/              |          |\n",
      "|    fps             | 812      |\n",
      "|    iterations      | 420      |\n",
      "|    time_elapsed    | 5211     |\n",
      "|    total_timesteps | 4233600  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.00524    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 812         |\n",
      "|    iterations           | 422         |\n",
      "|    time_elapsed         | 5234        |\n",
      "|    total_timesteps      | 4253760     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024415884 |\n",
      "|    clip_fraction        | 0.306       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 14.9        |\n",
      "|    explained_variance   | -1.41       |\n",
      "|    learning_rate        | 0.000378    |\n",
      "|    loss                 | 0.00805     |\n",
      "|    n_updates            | 4210        |\n",
      "|    policy_gradient_loss | 0.0202      |\n",
      "|    std                  | 0.00867     |\n",
      "|    value_loss           | 3e-09       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.00536    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 813         |\n",
      "|    iterations           | 424         |\n",
      "|    time_elapsed         | 5256        |\n",
      "|    total_timesteps      | 4273920     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.053706907 |\n",
      "|    clip_fraction        | 0.288       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 14.9        |\n",
      "|    explained_variance   | -0.686      |\n",
      "|    learning_rate        | 0.000368    |\n",
      "|    loss                 | 0.00481     |\n",
      "|    n_updates            | 4230        |\n",
      "|    policy_gradient_loss | 0.0165      |\n",
      "|    std                  | 0.00862     |\n",
      "|    value_loss           | 4.89e-09    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4294080, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00137     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4294080      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036567478 |\n",
      "|    clip_fraction        | 0.283        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 14.9         |\n",
      "|    explained_variance   | -1.18        |\n",
      "|    learning_rate        | 0.000358     |\n",
      "|    loss                 | 4.57e-05     |\n",
      "|    n_updates            | 4250         |\n",
      "|    policy_gradient_loss | 0.015        |\n",
      "|    std                  | 0.00862      |\n",
      "|    value_loss           | 2.65e-09     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.00547 |\n",
      "| time/              |          |\n",
      "|    fps             | 812      |\n",
      "|    iterations      | 426      |\n",
      "|    time_elapsed    | 5285     |\n",
      "|    total_timesteps | 4294080  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.00514    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 812         |\n",
      "|    iterations           | 428         |\n",
      "|    time_elapsed         | 5307        |\n",
      "|    total_timesteps      | 4314240     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015188735 |\n",
      "|    clip_fraction        | 0.341       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 15          |\n",
      "|    explained_variance   | -1.36       |\n",
      "|    learning_rate        | 0.000348    |\n",
      "|    loss                 | 0.000889    |\n",
      "|    n_updates            | 4270        |\n",
      "|    policy_gradient_loss | 0.0228      |\n",
      "|    std                  | 0.00859     |\n",
      "|    value_loss           | 1.99e-09    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.00502    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 813         |\n",
      "|    iterations           | 430         |\n",
      "|    time_elapsed         | 5330        |\n",
      "|    total_timesteps      | 4334400     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007988749 |\n",
      "|    clip_fraction        | 0.175       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 15          |\n",
      "|    explained_variance   | -1.08       |\n",
      "|    learning_rate        | 0.000338    |\n",
      "|    loss                 | 0.00342     |\n",
      "|    n_updates            | 4290        |\n",
      "|    policy_gradient_loss | 0.00794     |\n",
      "|    std                  | 0.00856     |\n",
      "|    value_loss           | 1.7e-09     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4354560, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.00151    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4354560     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009083127 |\n",
      "|    clip_fraction        | 0.116       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 15          |\n",
      "|    explained_variance   | -2.01       |\n",
      "|    learning_rate        | 0.000328    |\n",
      "|    loss                 | 0.000153    |\n",
      "|    n_updates            | 4310        |\n",
      "|    policy_gradient_loss | 0.00347     |\n",
      "|    std                  | 0.00857     |\n",
      "|    value_loss           | 2.63e-09    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.00496 |\n",
      "| time/              |          |\n",
      "|    fps             | 812      |\n",
      "|    iterations      | 432      |\n",
      "|    time_elapsed    | 5359     |\n",
      "|    total_timesteps | 4354560  |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.26e+03     |\n",
      "|    ep_rew_mean          | -0.00506     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 812          |\n",
      "|    iterations           | 434          |\n",
      "|    time_elapsed         | 5382         |\n",
      "|    total_timesteps      | 4374720      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067114932 |\n",
      "|    clip_fraction        | 0.127        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 15           |\n",
      "|    explained_variance   | -0.955       |\n",
      "|    learning_rate        | 0.000318     |\n",
      "|    loss                 | 0.00903      |\n",
      "|    n_updates            | 4330         |\n",
      "|    policy_gradient_loss | 0.00456      |\n",
      "|    std                  | 0.00852      |\n",
      "|    value_loss           | 3.35e-09     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.00495    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 813         |\n",
      "|    iterations           | 436         |\n",
      "|    time_elapsed         | 5404        |\n",
      "|    total_timesteps      | 4394880     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012059362 |\n",
      "|    clip_fraction        | 0.209       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 15          |\n",
      "|    explained_variance   | -2.25       |\n",
      "|    learning_rate        | 0.000308    |\n",
      "|    loss                 | 0.00309     |\n",
      "|    n_updates            | 4350        |\n",
      "|    policy_gradient_loss | 0.00889     |\n",
      "|    std                  | 0.00845     |\n",
      "|    value_loss           | 1.73e-09    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4415040, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.00291    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4415040     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.059488773 |\n",
      "|    clip_fraction        | 0.245       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 15          |\n",
      "|    explained_variance   | -0.445      |\n",
      "|    learning_rate        | 0.000298    |\n",
      "|    loss                 | 0.00691     |\n",
      "|    n_updates            | 4370        |\n",
      "|    policy_gradient_loss | 0.0136      |\n",
      "|    std                  | 0.00842     |\n",
      "|    value_loss           | 2.78e-09    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.00472 |\n",
      "| time/              |          |\n",
      "|    fps             | 812      |\n",
      "|    iterations      | 438      |\n",
      "|    time_elapsed    | 5433     |\n",
      "|    total_timesteps | 4415040  |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.26e+03   |\n",
      "|    ep_rew_mean          | -0.0048    |\n",
      "| time/                   |            |\n",
      "|    fps                  | 812        |\n",
      "|    iterations           | 440        |\n",
      "|    time_elapsed         | 5455       |\n",
      "|    total_timesteps      | 4435200    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02848557 |\n",
      "|    clip_fraction        | 0.297      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 15.1       |\n",
      "|    explained_variance   | -0.329     |\n",
      "|    learning_rate        | 0.000287   |\n",
      "|    loss                 | 0.0358     |\n",
      "|    n_updates            | 4390       |\n",
      "|    policy_gradient_loss | 0.0118     |\n",
      "|    std                  | 0.00841    |\n",
      "|    value_loss           | 4.61e-09   |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.26e+03   |\n",
      "|    ep_rew_mean          | -0.00484   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 813        |\n",
      "|    iterations           | 442        |\n",
      "|    time_elapsed         | 5478       |\n",
      "|    total_timesteps      | 4455360    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07016171 |\n",
      "|    clip_fraction        | 0.296      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 15.1       |\n",
      "|    explained_variance   | -0.472     |\n",
      "|    learning_rate        | 0.000277   |\n",
      "|    loss                 | 0.0632     |\n",
      "|    n_updates            | 4410       |\n",
      "|    policy_gradient_loss | 0.0144     |\n",
      "|    std                  | 0.00836    |\n",
      "|    value_loss           | 5.59e-09   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=4475520, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0017      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4475520      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023996667 |\n",
      "|    clip_fraction        | 0.173        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 15.1         |\n",
      "|    explained_variance   | -1.36        |\n",
      "|    learning_rate        | 0.000267     |\n",
      "|    loss                 | -0.00137     |\n",
      "|    n_updates            | 4430         |\n",
      "|    policy_gradient_loss | 0.00811      |\n",
      "|    std                  | 0.00833      |\n",
      "|    value_loss           | 1.55e-09     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.00479 |\n",
      "| time/              |          |\n",
      "|    fps             | 812      |\n",
      "|    iterations      | 444      |\n",
      "|    time_elapsed    | 5506     |\n",
      "|    total_timesteps | 4475520  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.00478    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 813         |\n",
      "|    iterations           | 446         |\n",
      "|    time_elapsed         | 5529        |\n",
      "|    total_timesteps      | 4495680     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014384083 |\n",
      "|    clip_fraction        | 0.222       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 15.1        |\n",
      "|    explained_variance   | -0.449      |\n",
      "|    learning_rate        | 0.000257    |\n",
      "|    loss                 | -0.000787   |\n",
      "|    n_updates            | 4450        |\n",
      "|    policy_gradient_loss | 0.00837     |\n",
      "|    std                  | 0.00828     |\n",
      "|    value_loss           | 4.97e-09    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.00458    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 813         |\n",
      "|    iterations           | 448         |\n",
      "|    time_elapsed         | 5552        |\n",
      "|    total_timesteps      | 4515840     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016617352 |\n",
      "|    clip_fraction        | 0.157       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 15.1        |\n",
      "|    explained_variance   | -2.77       |\n",
      "|    learning_rate        | 0.000247    |\n",
      "|    loss                 | 0.0093      |\n",
      "|    n_updates            | 4470        |\n",
      "|    policy_gradient_loss | 0.00652     |\n",
      "|    std                  | 0.00821     |\n",
      "|    value_loss           | 2.84e-09    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4536000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00112     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4536000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054022465 |\n",
      "|    clip_fraction        | 0.166        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 15.1         |\n",
      "|    explained_variance   | -0.55        |\n",
      "|    learning_rate        | 0.000237     |\n",
      "|    loss                 | 0.00109      |\n",
      "|    n_updates            | 4490         |\n",
      "|    policy_gradient_loss | 0.00664      |\n",
      "|    std                  | 0.00815      |\n",
      "|    value_loss           | 2.43e-09     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.00464 |\n",
      "| time/              |          |\n",
      "|    fps             | 812      |\n",
      "|    iterations      | 450      |\n",
      "|    time_elapsed    | 5581     |\n",
      "|    total_timesteps | 4536000  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.00476    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 813         |\n",
      "|    iterations           | 452         |\n",
      "|    time_elapsed         | 5603        |\n",
      "|    total_timesteps      | 4556160     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013240008 |\n",
      "|    clip_fraction        | 0.136       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 15.2        |\n",
      "|    explained_variance   | -1.24       |\n",
      "|    learning_rate        | 0.000227    |\n",
      "|    loss                 | 0.00292     |\n",
      "|    n_updates            | 4510        |\n",
      "|    policy_gradient_loss | 0.00221     |\n",
      "|    std                  | 0.00813     |\n",
      "|    value_loss           | 9.14e-09    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.26e+03   |\n",
      "|    ep_rew_mean          | -0.00468   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 813        |\n",
      "|    iterations           | 454        |\n",
      "|    time_elapsed         | 5626       |\n",
      "|    total_timesteps      | 4576320    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03594795 |\n",
      "|    clip_fraction        | 0.219      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | -1.42      |\n",
      "|    learning_rate        | 0.000217   |\n",
      "|    loss                 | -0.00106   |\n",
      "|    n_updates            | 4530       |\n",
      "|    policy_gradient_loss | 0.0121     |\n",
      "|    std                  | 0.0081     |\n",
      "|    value_loss           | 2.51e-09   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=4596480, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00178     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4596480      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074692597 |\n",
      "|    clip_fraction        | 0.136        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 15.2         |\n",
      "|    explained_variance   | -0.517       |\n",
      "|    learning_rate        | 0.000207     |\n",
      "|    loss                 | -0.00355     |\n",
      "|    n_updates            | 4550         |\n",
      "|    policy_gradient_loss | 0.00381      |\n",
      "|    std                  | 0.00809      |\n",
      "|    value_loss           | 3.8e-09      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.00489 |\n",
      "| time/              |          |\n",
      "|    fps             | 812      |\n",
      "|    iterations      | 456      |\n",
      "|    time_elapsed    | 5655     |\n",
      "|    total_timesteps | 4596480  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.00479    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 813         |\n",
      "|    iterations           | 458         |\n",
      "|    time_elapsed         | 5677        |\n",
      "|    total_timesteps      | 4616640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004442186 |\n",
      "|    clip_fraction        | 0.0857      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 15.2        |\n",
      "|    explained_variance   | -1.42       |\n",
      "|    learning_rate        | 0.000197    |\n",
      "|    loss                 | -0.00114    |\n",
      "|    n_updates            | 4570        |\n",
      "|    policy_gradient_loss | 0.00263     |\n",
      "|    std                  | 0.00805     |\n",
      "|    value_loss           | 2.06e-09    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.00484    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 813         |\n",
      "|    iterations           | 460         |\n",
      "|    time_elapsed         | 5700        |\n",
      "|    total_timesteps      | 4636800     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006899404 |\n",
      "|    clip_fraction        | 0.147       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 15.2        |\n",
      "|    explained_variance   | -1.54       |\n",
      "|    learning_rate        | 0.000187    |\n",
      "|    loss                 | -0.000766   |\n",
      "|    n_updates            | 4590        |\n",
      "|    policy_gradient_loss | 0.0053      |\n",
      "|    std                  | 0.00801     |\n",
      "|    value_loss           | 2.1e-09     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4656960, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.00228    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4656960     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002858727 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 15.2        |\n",
      "|    explained_variance   | -0.574      |\n",
      "|    learning_rate        | 0.000177    |\n",
      "|    loss                 | -0.00119    |\n",
      "|    n_updates            | 4610        |\n",
      "|    policy_gradient_loss | 0.00337     |\n",
      "|    std                  | 0.00798     |\n",
      "|    value_loss           | 2.81e-09    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0048  |\n",
      "| time/              |          |\n",
      "|    fps             | 812      |\n",
      "|    iterations      | 462      |\n",
      "|    time_elapsed    | 5729     |\n",
      "|    total_timesteps | 4656960  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.00471    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 813         |\n",
      "|    iterations           | 464         |\n",
      "|    time_elapsed         | 5751        |\n",
      "|    total_timesteps      | 4677120     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012431391 |\n",
      "|    clip_fraction        | 0.136       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 15.2        |\n",
      "|    explained_variance   | -1.4        |\n",
      "|    learning_rate        | 0.000166    |\n",
      "|    loss                 | -0.000797   |\n",
      "|    n_updates            | 4630        |\n",
      "|    policy_gradient_loss | 0.00372     |\n",
      "|    std                  | 0.00794     |\n",
      "|    value_loss           | 2.99e-09    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.00466    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 813         |\n",
      "|    iterations           | 466         |\n",
      "|    time_elapsed         | 5774        |\n",
      "|    total_timesteps      | 4697280     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009165245 |\n",
      "|    clip_fraction        | 0.111       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 15.2        |\n",
      "|    explained_variance   | -1.95       |\n",
      "|    learning_rate        | 0.000156    |\n",
      "|    loss                 | 0.000505    |\n",
      "|    n_updates            | 4650        |\n",
      "|    policy_gradient_loss | 0.00383     |\n",
      "|    std                  | 0.00791     |\n",
      "|    value_loss           | 2.05e-09    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4717440, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.00184    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4717440     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008197712 |\n",
      "|    clip_fraction        | 0.0538      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 15.3        |\n",
      "|    explained_variance   | -2.26       |\n",
      "|    learning_rate        | 0.000146    |\n",
      "|    loss                 | 0.00753     |\n",
      "|    n_updates            | 4670        |\n",
      "|    policy_gradient_loss | -8.15e-05   |\n",
      "|    std                  | 0.00788     |\n",
      "|    value_loss           | 1.81e-09    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.00446 |\n",
      "| time/              |          |\n",
      "|    fps             | 812      |\n",
      "|    iterations      | 468      |\n",
      "|    time_elapsed    | 5803     |\n",
      "|    total_timesteps | 4717440  |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.26e+03   |\n",
      "|    ep_rew_mean          | -0.00429   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 813        |\n",
      "|    iterations           | 470        |\n",
      "|    time_elapsed         | 5826       |\n",
      "|    total_timesteps      | 4737600    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00685835 |\n",
      "|    clip_fraction        | 0.0517     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | -1.45      |\n",
      "|    learning_rate        | 0.000136   |\n",
      "|    loss                 | 0.000124   |\n",
      "|    n_updates            | 4690       |\n",
      "|    policy_gradient_loss | 0.00132    |\n",
      "|    std                  | 0.00787    |\n",
      "|    value_loss           | 1.7e-09    |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.26e+03     |\n",
      "|    ep_rew_mean          | -0.00434     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 813          |\n",
      "|    iterations           | 472          |\n",
      "|    time_elapsed         | 5848         |\n",
      "|    total_timesteps      | 4757760      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034054175 |\n",
      "|    clip_fraction        | 0.0432       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 15.3         |\n",
      "|    explained_variance   | -0.928       |\n",
      "|    learning_rate        | 0.000126     |\n",
      "|    loss                 | -5.68e-05    |\n",
      "|    n_updates            | 4710         |\n",
      "|    policy_gradient_loss | 0.000674     |\n",
      "|    std                  | 0.00785      |\n",
      "|    value_loss           | 2.99e-09     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4777920, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0023     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4777920     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002997459 |\n",
      "|    clip_fraction        | 0.0525      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 15.3        |\n",
      "|    explained_variance   | -1.17       |\n",
      "|    learning_rate        | 0.000116    |\n",
      "|    loss                 | 0.000682    |\n",
      "|    n_updates            | 4730        |\n",
      "|    policy_gradient_loss | 0.000685    |\n",
      "|    std                  | 0.00785     |\n",
      "|    value_loss           | 3.82e-09    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.00438 |\n",
      "| time/              |          |\n",
      "|    fps             | 812      |\n",
      "|    iterations      | 474      |\n",
      "|    time_elapsed    | 5877     |\n",
      "|    total_timesteps | 4777920  |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.26e+03     |\n",
      "|    ep_rew_mean          | -0.00434     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 813          |\n",
      "|    iterations           | 476          |\n",
      "|    time_elapsed         | 5900         |\n",
      "|    total_timesteps      | 4798080      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033276037 |\n",
      "|    clip_fraction        | 0.0582       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 15.3         |\n",
      "|    explained_variance   | -1.1         |\n",
      "|    learning_rate        | 0.000106     |\n",
      "|    loss                 | -0.00132     |\n",
      "|    n_updates            | 4750         |\n",
      "|    policy_gradient_loss | -0.000742    |\n",
      "|    std                  | 0.00782      |\n",
      "|    value_loss           | 1.7e-09      |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.00437    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 813         |\n",
      "|    iterations           | 478         |\n",
      "|    time_elapsed         | 5923        |\n",
      "|    total_timesteps      | 4818240     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004831175 |\n",
      "|    clip_fraction        | 0.068       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 15.3        |\n",
      "|    explained_variance   | -1.05       |\n",
      "|    learning_rate        | 9.59e-05    |\n",
      "|    loss                 | -0.000365   |\n",
      "|    n_updates            | 4770        |\n",
      "|    policy_gradient_loss | 0.000261    |\n",
      "|    std                  | 0.00782     |\n",
      "|    value_loss           | 3.12e-09    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4838400, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00163     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4838400      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047239074 |\n",
      "|    clip_fraction        | 0.0915       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 15.3         |\n",
      "|    explained_variance   | -0.435       |\n",
      "|    learning_rate        | 8.58e-05     |\n",
      "|    loss                 | -0.00146     |\n",
      "|    n_updates            | 4790         |\n",
      "|    policy_gradient_loss | 0.000173     |\n",
      "|    std                  | 0.0078       |\n",
      "|    value_loss           | 5.71e-09     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.00446 |\n",
      "| time/              |          |\n",
      "|    fps             | 812      |\n",
      "|    iterations      | 480      |\n",
      "|    time_elapsed    | 5951     |\n",
      "|    total_timesteps | 4838400  |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.26e+03     |\n",
      "|    ep_rew_mean          | -0.00441     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 813          |\n",
      "|    iterations           | 482          |\n",
      "|    time_elapsed         | 5974         |\n",
      "|    total_timesteps      | 4858560      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044438066 |\n",
      "|    clip_fraction        | 0.0559       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 15.3         |\n",
      "|    explained_variance   | -3.02        |\n",
      "|    learning_rate        | 7.58e-05     |\n",
      "|    loss                 | -0.00232     |\n",
      "|    n_updates            | 4810         |\n",
      "|    policy_gradient_loss | -0.000938    |\n",
      "|    std                  | 0.00778      |\n",
      "|    value_loss           | 1.43e-09     |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.26e+03   |\n",
      "|    ep_rew_mean          | -0.00445   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 813        |\n",
      "|    iterations           | 484        |\n",
      "|    time_elapsed         | 5996       |\n",
      "|    total_timesteps      | 4878720    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00607103 |\n",
      "|    clip_fraction        | 0.0292     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | -1.99      |\n",
      "|    learning_rate        | 6.57e-05   |\n",
      "|    loss                 | 0.00083    |\n",
      "|    n_updates            | 4830       |\n",
      "|    policy_gradient_loss | 0.000126   |\n",
      "|    std                  | 0.00776    |\n",
      "|    value_loss           | 1.62e-09   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=4898880, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.00147    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4898880     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002011444 |\n",
      "|    clip_fraction        | 0.039       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 15.4        |\n",
      "|    explained_variance   | -1.51       |\n",
      "|    learning_rate        | 5.56e-05    |\n",
      "|    loss                 | -0.00227    |\n",
      "|    n_updates            | 4850        |\n",
      "|    policy_gradient_loss | -0.000957   |\n",
      "|    std                  | 0.00774     |\n",
      "|    value_loss           | 4.18e-09    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.00425 |\n",
      "| time/              |          |\n",
      "|    fps             | 813      |\n",
      "|    iterations      | 486      |\n",
      "|    time_elapsed    | 6025     |\n",
      "|    total_timesteps | 4898880  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.00428    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 813         |\n",
      "|    iterations           | 488         |\n",
      "|    time_elapsed         | 6048        |\n",
      "|    total_timesteps      | 4919040     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002319022 |\n",
      "|    clip_fraction        | 0.0229      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 15.4        |\n",
      "|    explained_variance   | -0.786      |\n",
      "|    learning_rate        | 4.55e-05    |\n",
      "|    loss                 | 0.000482    |\n",
      "|    n_updates            | 4870        |\n",
      "|    policy_gradient_loss | -0.00089    |\n",
      "|    std                  | 0.00773     |\n",
      "|    value_loss           | 3.34e-09    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.00421    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 813         |\n",
      "|    iterations           | 490         |\n",
      "|    time_elapsed         | 6070        |\n",
      "|    total_timesteps      | 4939200     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004420603 |\n",
      "|    clip_fraction        | 0.0243      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 15.4        |\n",
      "|    explained_variance   | -2.39       |\n",
      "|    learning_rate        | 3.54e-05    |\n",
      "|    loss                 | -0.00317    |\n",
      "|    n_updates            | 4890        |\n",
      "|    policy_gradient_loss | -0.000853   |\n",
      "|    std                  | 0.00772     |\n",
      "|    value_loss           | 1.85e-09    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4959360, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.00172    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4959360     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007240496 |\n",
      "|    clip_fraction        | 0.0426      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 15.4        |\n",
      "|    explained_variance   | -1.03       |\n",
      "|    learning_rate        | 2.54e-05    |\n",
      "|    loss                 | -0.000435   |\n",
      "|    n_updates            | 4910        |\n",
      "|    policy_gradient_loss | -0.000337   |\n",
      "|    std                  | 0.00772     |\n",
      "|    value_loss           | 2.01e-09    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.00423 |\n",
      "| time/              |          |\n",
      "|    fps             | 813      |\n",
      "|    iterations      | 492      |\n",
      "|    time_elapsed    | 6099     |\n",
      "|    total_timesteps | 4959360  |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.26e+03     |\n",
      "|    ep_rew_mean          | -0.00426     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 813          |\n",
      "|    iterations           | 494          |\n",
      "|    time_elapsed         | 6122         |\n",
      "|    total_timesteps      | 4979520      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028332819 |\n",
      "|    clip_fraction        | 0.0212       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 15.4         |\n",
      "|    explained_variance   | -1.34        |\n",
      "|    learning_rate        | 1.53e-05     |\n",
      "|    loss                 | -0.00158     |\n",
      "|    n_updates            | 4930         |\n",
      "|    policy_gradient_loss | -0.000447    |\n",
      "|    std                  | 0.00771      |\n",
      "|    value_loss           | 1.82e-09     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.00433    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 813         |\n",
      "|    iterations           | 496         |\n",
      "|    time_elapsed         | 6145        |\n",
      "|    total_timesteps      | 4999680     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003694955 |\n",
      "|    clip_fraction        | 0.0113      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 15.4        |\n",
      "|    explained_variance   | -1.49       |\n",
      "|    learning_rate        | 5.2e-06     |\n",
      "|    loss                 | -0.00146    |\n",
      "|    n_updates            | 4950        |\n",
      "|    policy_gradient_loss | -0.000712   |\n",
      "|    std                  | 0.00771     |\n",
      "|    value_loss           | 3.1e-09     |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    policy_kwargs = dict(activation_fn=th.nn.LeakyReLU,\n",
    "                     net_arch=dict(pi=[512,512,256,128,64,64,64,64,36,18], vf=[512,512,256,128,64,64,64,64,36,18], optimizers_class = th.optim.Adam, use_sde = True, sde_sample_freq = 1260*4)) #\n",
    "    # Add magnus\n",
    "    envs = VecMonitor(DummyVecEnv([\n",
    "        lambda: tradingEng(paths1,action = 'big-Magnus', obs = 'xs'),\n",
    "        lambda: tradingEng(paths2,action = 'big-Magnus', obs = 'xs')\n",
    "    ]),filename='logs-TrainMagB')\n",
    "    ev_env = VecMonitor(DummyVecEnv([\n",
    "        lambda: tradingEng(paths_ev,action = 'big-Magnus', obs = 'xs'),\n",
    "    ]))\n",
    "\n",
    "    eval_callback = EvalCallback(\n",
    "        ev_env,\n",
    "        best_model_save_path='./logs/best_modelMagB',\n",
    "        log_path='./logs/eval_logs/evMagB',\n",
    "        eval_freq=252*8*15,\n",
    "        deterministic=True,\n",
    "        render=False,\n",
    "        verbose = True,\n",
    "        n_eval_episodes = 6\n",
    "    )\n",
    "\n",
    "    model = PPO(\"MlpPolicy\", envs, batch_size = 252*5, learning_rate=linear_schedule(0.005), policy_kwargs=policy_kwargs, n_steps=252*4*5, normalize_advantage=True, gamma = 0.9, verbose = 1) \n",
    "\n",
    "    model.learn(total_timesteps=5e6, log_interval=2, callback=eval_callback) \n",
    "\n",
    "    # Add ent and magnus\n",
    "    envs = VecMonitor(DummyVecEnv([\n",
    "        lambda: tradingEng(paths1,action = 'small-Magnus', obs = 'xs'),\n",
    "        lambda: tradingEng(paths2,action = 'small-Magnus', obs = 'xs')\n",
    "    ]),filename='logs-TrainMagS')\n",
    "    ev_env = VecMonitor(DummyVecEnv([\n",
    "        lambda: tradingEng(paths_ev,action = 'small-Magnus', obs = 'xs'),\n",
    "    ]))\n",
    "\n",
    "    eval_callback = EvalCallback(\n",
    "        ev_env,\n",
    "        best_model_save_path='./logs/best_modelMagS',\n",
    "        log_path='./logs/eval_logs/evMagS',\n",
    "        eval_freq=252*8*15,\n",
    "        deterministic=True,\n",
    "        render=False,\n",
    "        verbose = True,\n",
    "        n_eval_episodes = 6\n",
    "    )\n",
    "\n",
    "    model = PPO(\"MlpPolicy\", envs, batch_size = 252*5, learning_rate=linear_schedule(0.005), policy_kwargs=policy_kwargs, n_steps=252*4*5, normalize_advantage=True, gamma = 0.9, verbose = 1) \n",
    "\n",
    "    model.learn(total_timesteps=5e6, log_interval=2, callback=eval_callback) \n",
    "\n",
    "except:\n",
    "    print(\"Magnus failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.26e+03     |\n",
      "|    ep_rew_mean          | -0.364       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 877          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 45           |\n",
      "|    total_timesteps      | 40320        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019163904 |\n",
      "|    clip_fraction        | 0.00337      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.81        |\n",
      "|    explained_variance   | -1.2         |\n",
      "|    learning_rate        | 0.0015       |\n",
      "|    loss                 | -0.00125     |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.000381    |\n",
      "|    std                  | 0.974        |\n",
      "|    value_loss           | 3.27e-05     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.339      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 862         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 93          |\n",
      "|    total_timesteps      | 80640       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007153818 |\n",
      "|    clip_fraction        | 0.0276      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.66       |\n",
      "|    explained_variance   | 0.0455      |\n",
      "|    learning_rate        | 0.00149     |\n",
      "|    loss                 | -0.00536    |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00359    |\n",
      "|    std                  | 0.896       |\n",
      "|    value_loss           | 1.82e-06    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.26e+03     |\n",
      "|    ep_rew_mean          | -0.346       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 138          |\n",
      "|    total_timesteps      | 120960       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062310016 |\n",
      "|    clip_fraction        | 0.029        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.5         |\n",
      "|    explained_variance   | 0.0492       |\n",
      "|    learning_rate        | 0.00148      |\n",
      "|    loss                 | -0.00248     |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00194     |\n",
      "|    std                  | 0.836        |\n",
      "|    value_loss           | 4.53e-06     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.34       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 879         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 183         |\n",
      "|    total_timesteps      | 161280      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007085764 |\n",
      "|    clip_fraction        | 0.0251      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.36       |\n",
      "|    explained_variance   | 0.151       |\n",
      "|    learning_rate        | 0.00148     |\n",
      "|    loss                 | -0.00407    |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00225    |\n",
      "|    std                  | 0.776       |\n",
      "|    value_loss           | 2.9e-06     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.328      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 878         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 229         |\n",
      "|    total_timesteps      | 201600      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006435351 |\n",
      "|    clip_fraction        | 0.0257      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.23       |\n",
      "|    explained_variance   | 0.066       |\n",
      "|    learning_rate        | 0.00147     |\n",
      "|    loss                 | -0.00367    |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00182    |\n",
      "|    std                  | 0.735       |\n",
      "|    value_loss           | 5.24e-06    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.26e+03     |\n",
      "|    ep_rew_mean          | -0.302       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 883          |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 273          |\n",
      "|    total_timesteps      | 241920       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077756783 |\n",
      "|    clip_fraction        | 0.0434       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.01        |\n",
      "|    explained_variance   | 0.139        |\n",
      "|    learning_rate        | 0.00147      |\n",
      "|    loss                 | -0.0116      |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.00479     |\n",
      "|    std                  | 0.658        |\n",
      "|    value_loss           | 1.02e-06     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.284      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 886         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 318         |\n",
      "|    total_timesteps      | 282240      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006580102 |\n",
      "|    clip_fraction        | 0.0339      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.84       |\n",
      "|    explained_variance   | 0.174       |\n",
      "|    learning_rate        | 0.00146     |\n",
      "|    loss                 | -0.00465    |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.00238    |\n",
      "|    std                  | 0.608       |\n",
      "|    value_loss           | 2.79e-06    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.26e+03     |\n",
      "|    ep_rew_mean          | -0.269       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 889          |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 362          |\n",
      "|    total_timesteps      | 322560       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037997155 |\n",
      "|    clip_fraction        | 0.0461       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.68        |\n",
      "|    explained_variance   | 0.0488       |\n",
      "|    learning_rate        | 0.00145      |\n",
      "|    loss                 | -0.002       |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.00126     |\n",
      "|    std                  | 0.576        |\n",
      "|    value_loss           | 8.91e-06     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.262      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 891         |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 406         |\n",
      "|    total_timesteps      | 362880      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006491204 |\n",
      "|    clip_fraction        | 0.0402      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.56       |\n",
      "|    explained_variance   | 0.244       |\n",
      "|    learning_rate        | 0.00145     |\n",
      "|    loss                 | -0.0053     |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.00228    |\n",
      "|    std                  | 0.539       |\n",
      "|    value_loss           | 3e-06       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=403200, episode_reward=-0.10 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0963     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 403200      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006937043 |\n",
      "|    clip_fraction        | 0.0361      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.38       |\n",
      "|    explained_variance   | 0.109       |\n",
      "|    learning_rate        | 0.00144     |\n",
      "|    loss                 | -0.00515    |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.00231    |\n",
      "|    std                  | 0.492       |\n",
      "|    value_loss           | 2.01e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.248   |\n",
      "| time/              |          |\n",
      "|    fps             | 880      |\n",
      "|    iterations      | 20       |\n",
      "|    time_elapsed    | 457      |\n",
      "|    total_timesteps | 403200   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.222      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 886         |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 500         |\n",
      "|    total_timesteps      | 443520      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010960935 |\n",
      "|    clip_fraction        | 0.0673      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.17       |\n",
      "|    explained_variance   | 0.242       |\n",
      "|    learning_rate        | 0.00144     |\n",
      "|    loss                 | -0.00875    |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.00468    |\n",
      "|    std                  | 0.447       |\n",
      "|    value_loss           | 5.2e-07     |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.26e+03     |\n",
      "|    ep_rew_mean          | -0.196       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 890          |\n",
      "|    iterations           | 24           |\n",
      "|    time_elapsed         | 543          |\n",
      "|    total_timesteps      | 483840       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0080681965 |\n",
      "|    clip_fraction        | 0.0486       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.923       |\n",
      "|    explained_variance   | 0.0484       |\n",
      "|    learning_rate        | 0.00143      |\n",
      "|    loss                 | -0.00103     |\n",
      "|    n_updates            | 230          |\n",
      "|    policy_gradient_loss | -0.00205     |\n",
      "|    std                  | 0.397        |\n",
      "|    value_loss           | 5.39e-06     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.175      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 895         |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 585         |\n",
      "|    total_timesteps      | 524160      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007908276 |\n",
      "|    clip_fraction        | 0.0932      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.73       |\n",
      "|    explained_variance   | 0.29        |\n",
      "|    learning_rate        | 0.00142     |\n",
      "|    loss                 | -0.00739    |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.00398    |\n",
      "|    std                  | 0.36        |\n",
      "|    value_loss           | 3.02e-07    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.155      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 898         |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 627         |\n",
      "|    total_timesteps      | 564480      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010631228 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.493      |\n",
      "|    explained_variance   | 0.255       |\n",
      "|    learning_rate        | 0.00142     |\n",
      "|    loss                 | -0.0109     |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.00567    |\n",
      "|    std                  | 0.319       |\n",
      "|    value_loss           | 1e-07       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.26e+03     |\n",
      "|    ep_rew_mean          | -0.144       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 901          |\n",
      "|    iterations           | 30           |\n",
      "|    time_elapsed         | 670          |\n",
      "|    total_timesteps      | 604800       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050088763 |\n",
      "|    clip_fraction        | 0.0735       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.35        |\n",
      "|    explained_variance   | 0.246        |\n",
      "|    learning_rate        | 0.00141      |\n",
      "|    loss                 | -0.00387     |\n",
      "|    n_updates            | 290          |\n",
      "|    policy_gradient_loss | -0.00115     |\n",
      "|    std                  | 0.301        |\n",
      "|    value_loss           | 5.03e-07     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.26e+03     |\n",
      "|    ep_rew_mean          | -0.136       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 905          |\n",
      "|    iterations           | 32           |\n",
      "|    time_elapsed         | 712          |\n",
      "|    total_timesteps      | 645120       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065858164 |\n",
      "|    clip_fraction        | 0.121        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.198       |\n",
      "|    explained_variance   | 0.312        |\n",
      "|    learning_rate        | 0.00141      |\n",
      "|    loss                 | -0.00303     |\n",
      "|    n_updates            | 310          |\n",
      "|    policy_gradient_loss | -0.00132     |\n",
      "|    std                  | 0.278        |\n",
      "|    value_loss           | 4.24e-07     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.135      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 908         |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 754         |\n",
      "|    total_timesteps      | 685440      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018316418 |\n",
      "|    clip_fraction        | 0.135       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0766     |\n",
      "|    explained_variance   | 0.0861      |\n",
      "|    learning_rate        | 0.0014      |\n",
      "|    loss                 | 0.00518     |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.00034    |\n",
      "|    std                  | 0.264       |\n",
      "|    value_loss           | 1.32e-06    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.26e+03   |\n",
      "|    ep_rew_mean          | -0.127     |\n",
      "| time/                   |            |\n",
      "|    fps                  | 911        |\n",
      "|    iterations           | 36         |\n",
      "|    time_elapsed         | 796        |\n",
      "|    total_timesteps      | 725760     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00739766 |\n",
      "|    clip_fraction        | 0.146      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0174    |\n",
      "|    explained_variance   | 0.134      |\n",
      "|    learning_rate        | 0.00139    |\n",
      "|    loss                 | -0.00206   |\n",
      "|    n_updates            | 350        |\n",
      "|    policy_gradient_loss | 7.57e-05   |\n",
      "|    std                  | 0.258      |\n",
      "|    value_loss           | 1.16e-06   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.123      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 904         |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 846         |\n",
      "|    total_timesteps      | 766080      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018154852 |\n",
      "|    clip_fraction        | 0.182       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.0619      |\n",
      "|    explained_variance   | 0.288       |\n",
      "|    learning_rate        | 0.00139     |\n",
      "|    loss                 | 0.00782     |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | 0.00135     |\n",
      "|    std                  | 0.247       |\n",
      "|    value_loss           | 5.88e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=806400, episode_reward=-0.07 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0708     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 806400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013931058 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.163       |\n",
      "|    explained_variance   | 0.191       |\n",
      "|    learning_rate        | 0.00138     |\n",
      "|    loss                 | -0.00327    |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | 0.000126    |\n",
      "|    std                  | 0.235       |\n",
      "|    value_loss           | 1.27e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.115   |\n",
      "| time/              |          |\n",
      "|    fps             | 878      |\n",
      "|    iterations      | 40       |\n",
      "|    time_elapsed    | 917      |\n",
      "|    total_timesteps | 806400   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.26e+03     |\n",
      "|    ep_rew_mean          | -0.112       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 869          |\n",
      "|    iterations           | 42           |\n",
      "|    time_elapsed         | 973          |\n",
      "|    total_timesteps      | 846720       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057986034 |\n",
      "|    clip_fraction        | 0.145        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 0.288        |\n",
      "|    explained_variance   | 0.47         |\n",
      "|    learning_rate        | 0.00138      |\n",
      "|    loss                 | -0.00406     |\n",
      "|    n_updates            | 410          |\n",
      "|    policy_gradient_loss | 0.000526     |\n",
      "|    std                  | 0.221        |\n",
      "|    value_loss           | 3.73e-07     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.11       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 865         |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 1025        |\n",
      "|    total_timesteps      | 887040      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010904535 |\n",
      "|    clip_fraction        | 0.158       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.379       |\n",
      "|    explained_variance   | 0.432       |\n",
      "|    learning_rate        | 0.00137     |\n",
      "|    loss                 | -0.00634    |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | 0.00181     |\n",
      "|    std                  | 0.211       |\n",
      "|    value_loss           | 9.15e-08    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.26e+03     |\n",
      "|    ep_rew_mean          | -0.105       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 855          |\n",
      "|    iterations           | 46           |\n",
      "|    time_elapsed         | 1084         |\n",
      "|    total_timesteps      | 927360       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065487614 |\n",
      "|    clip_fraction        | 0.13         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 0.448        |\n",
      "|    explained_variance   | 0.292        |\n",
      "|    learning_rate        | 0.00136      |\n",
      "|    loss                 | -0.00265     |\n",
      "|    n_updates            | 450          |\n",
      "|    policy_gradient_loss | 0.000153     |\n",
      "|    std                  | 0.204        |\n",
      "|    value_loss           | 3.48e-07     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.105      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 841         |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 1150        |\n",
      "|    total_timesteps      | 967680      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031523958 |\n",
      "|    clip_fraction        | 0.229       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.529       |\n",
      "|    explained_variance   | 0.111       |\n",
      "|    learning_rate        | 0.00136     |\n",
      "|    loss                 | 0.00455     |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | 0.000424    |\n",
      "|    std                  | 0.196       |\n",
      "|    value_loss           | 1.02e-05    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.104      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 832         |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 1210        |\n",
      "|    total_timesteps      | 1008000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008716745 |\n",
      "|    clip_fraction        | 0.155       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.581       |\n",
      "|    explained_variance   | -0.104      |\n",
      "|    learning_rate        | 0.00135     |\n",
      "|    loss                 | -0.00315    |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | 0.000412    |\n",
      "|    std                  | 0.191       |\n",
      "|    value_loss           | 2.63e-07    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0987     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 825         |\n",
      "|    iterations           | 52          |\n",
      "|    time_elapsed         | 1270        |\n",
      "|    total_timesteps      | 1048320     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008111948 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.658       |\n",
      "|    explained_variance   | -0.0122     |\n",
      "|    learning_rate        | 0.00135     |\n",
      "|    loss                 | -0.0047     |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.000486   |\n",
      "|    std                  | 0.183       |\n",
      "|    value_loss           | 1.41e-07    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.26e+03     |\n",
      "|    ep_rew_mean          | -0.0919      |\n",
      "| time/                   |              |\n",
      "|    fps                  | 818          |\n",
      "|    iterations           | 54           |\n",
      "|    time_elapsed         | 1330         |\n",
      "|    total_timesteps      | 1088640      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066545717 |\n",
      "|    clip_fraction        | 0.139        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 0.723        |\n",
      "|    explained_variance   | 0.0771       |\n",
      "|    learning_rate        | 0.00134      |\n",
      "|    loss                 | -0.00464     |\n",
      "|    n_updates            | 530          |\n",
      "|    policy_gradient_loss | -0.000665    |\n",
      "|    std                  | 0.177        |\n",
      "|    value_loss           | 7.93e-08     |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.26e+03   |\n",
      "|    ep_rew_mean          | -0.091     |\n",
      "| time/                   |            |\n",
      "|    fps                  | 808        |\n",
      "|    iterations           | 56         |\n",
      "|    time_elapsed         | 1396       |\n",
      "|    total_timesteps      | 1128960    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01776883 |\n",
      "|    clip_fraction        | 0.151      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 0.773      |\n",
      "|    explained_variance   | 0.058      |\n",
      "|    learning_rate        | 0.00133    |\n",
      "|    loss                 | 0.00693    |\n",
      "|    n_updates            | 550        |\n",
      "|    policy_gradient_loss | 0.00225    |\n",
      "|    std                  | 0.174      |\n",
      "|    value_loss           | 3.1e-07    |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.26e+03   |\n",
      "|    ep_rew_mean          | -0.0934    |\n",
      "| time/                   |            |\n",
      "|    fps                  | 800        |\n",
      "|    iterations           | 58         |\n",
      "|    time_elapsed         | 1460       |\n",
      "|    total_timesteps      | 1169280    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01598278 |\n",
      "|    clip_fraction        | 0.107      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 0.84       |\n",
      "|    explained_variance   | 0.15       |\n",
      "|    learning_rate        | 0.00133    |\n",
      "|    loss                 | 0.00324    |\n",
      "|    n_updates            | 570        |\n",
      "|    policy_gradient_loss | 0.000813   |\n",
      "|    std                  | 0.168      |\n",
      "|    value_loss           | 3.95e-07   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=1209600, episode_reward=-0.05 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0455     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1209600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015066033 |\n",
      "|    clip_fraction        | 0.0921      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.916       |\n",
      "|    explained_variance   | 0.124       |\n",
      "|    learning_rate        | 0.00132     |\n",
      "|    loss                 | 0.0037      |\n",
      "|    n_updates            | 590         |\n",
      "|    policy_gradient_loss | -0.000737   |\n",
      "|    std                  | 0.16        |\n",
      "|    value_loss           | 1.2e-07     |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0937  |\n",
      "| time/              |          |\n",
      "|    fps             | 792      |\n",
      "|    iterations      | 60       |\n",
      "|    time_elapsed    | 1525     |\n",
      "|    total_timesteps | 1209600  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0879     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 787         |\n",
      "|    iterations           | 62          |\n",
      "|    time_elapsed         | 1587        |\n",
      "|    total_timesteps      | 1249920     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010763644 |\n",
      "|    clip_fraction        | 0.215       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.971       |\n",
      "|    explained_variance   | 0.117       |\n",
      "|    learning_rate        | 0.00132     |\n",
      "|    loss                 | 0.00125     |\n",
      "|    n_updates            | 610         |\n",
      "|    policy_gradient_loss | 0.00274     |\n",
      "|    std                  | 0.157       |\n",
      "|    value_loss           | 2.79e-07    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.26e+03   |\n",
      "|    ep_rew_mean          | -0.0821    |\n",
      "| time/                   |            |\n",
      "|    fps                  | 784        |\n",
      "|    iterations           | 64         |\n",
      "|    time_elapsed         | 1644       |\n",
      "|    total_timesteps      | 1290240    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06784001 |\n",
      "|    clip_fraction        | 0.218      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 1.04       |\n",
      "|    explained_variance   | 0.0685     |\n",
      "|    learning_rate        | 0.00131    |\n",
      "|    loss                 | 0.00442    |\n",
      "|    n_updates            | 630        |\n",
      "|    policy_gradient_loss | 0.0087     |\n",
      "|    std                  | 0.151      |\n",
      "|    value_loss           | 7.06e-08   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0832     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 780         |\n",
      "|    iterations           | 66          |\n",
      "|    time_elapsed         | 1705        |\n",
      "|    total_timesteps      | 1330560     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029123634 |\n",
      "|    clip_fraction        | 0.203       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.09        |\n",
      "|    explained_variance   | 0.114       |\n",
      "|    learning_rate        | 0.0013      |\n",
      "|    loss                 | 0.0252      |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | 0.00567     |\n",
      "|    std                  | 0.148       |\n",
      "|    value_loss           | 4.18e-07    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0837     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 775         |\n",
      "|    iterations           | 68          |\n",
      "|    time_elapsed         | 1768        |\n",
      "|    total_timesteps      | 1370880     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023157438 |\n",
      "|    clip_fraction        | 0.206       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.13        |\n",
      "|    explained_variance   | 0.129       |\n",
      "|    learning_rate        | 0.0013      |\n",
      "|    loss                 | 0.00228     |\n",
      "|    n_updates            | 670         |\n",
      "|    policy_gradient_loss | 0.00291     |\n",
      "|    std                  | 0.144       |\n",
      "|    value_loss           | 1.28e-07    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0857     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 771         |\n",
      "|    iterations           | 70          |\n",
      "|    time_elapsed         | 1829        |\n",
      "|    total_timesteps      | 1411200     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033066873 |\n",
      "|    clip_fraction        | 0.219       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.17        |\n",
      "|    explained_variance   | 0.127       |\n",
      "|    learning_rate        | 0.00129     |\n",
      "|    loss                 | 0.00858     |\n",
      "|    n_updates            | 690         |\n",
      "|    policy_gradient_loss | 0.00395     |\n",
      "|    std                  | 0.141       |\n",
      "|    value_loss           | 2.3e-07     |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.26e+03   |\n",
      "|    ep_rew_mean          | -0.0826    |\n",
      "| time/                   |            |\n",
      "|    fps                  | 769        |\n",
      "|    iterations           | 72         |\n",
      "|    time_elapsed         | 1886       |\n",
      "|    total_timesteps      | 1451520    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01695146 |\n",
      "|    clip_fraction        | 0.236      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 1.22       |\n",
      "|    explained_variance   | 0.119      |\n",
      "|    learning_rate        | 0.00129    |\n",
      "|    loss                 | 0.0127     |\n",
      "|    n_updates            | 710        |\n",
      "|    policy_gradient_loss | 0.00444    |\n",
      "|    std                  | 0.139      |\n",
      "|    value_loss           | 1.24e-07   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0802     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 763         |\n",
      "|    iterations           | 74          |\n",
      "|    time_elapsed         | 1954        |\n",
      "|    total_timesteps      | 1491840     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013497917 |\n",
      "|    clip_fraction        | 0.244       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.27        |\n",
      "|    explained_variance   | 0.201       |\n",
      "|    learning_rate        | 0.00128     |\n",
      "|    loss                 | -0.00499    |\n",
      "|    n_updates            | 730         |\n",
      "|    policy_gradient_loss | 0.00947     |\n",
      "|    std                  | 0.135       |\n",
      "|    value_loss           | 7.14e-08    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0838     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 763         |\n",
      "|    iterations           | 76          |\n",
      "|    time_elapsed         | 2007        |\n",
      "|    total_timesteps      | 1532160     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040482447 |\n",
      "|    clip_fraction        | 0.22        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.32        |\n",
      "|    explained_variance   | 0.169       |\n",
      "|    learning_rate        | 0.00127     |\n",
      "|    loss                 | 0.00119     |\n",
      "|    n_updates            | 750         |\n",
      "|    policy_gradient_loss | 0.00428     |\n",
      "|    std                  | 0.132       |\n",
      "|    value_loss           | 2.25e-07    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0834     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 764         |\n",
      "|    iterations           | 78          |\n",
      "|    time_elapsed         | 2056        |\n",
      "|    total_timesteps      | 1572480     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015432388 |\n",
      "|    clip_fraction        | 0.167       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.36        |\n",
      "|    explained_variance   | 0.179       |\n",
      "|    learning_rate        | 0.00127     |\n",
      "|    loss                 | -0.000624   |\n",
      "|    n_updates            | 770         |\n",
      "|    policy_gradient_loss | 0.00386     |\n",
      "|    std                  | 0.129       |\n",
      "|    value_loss           | 1.02e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1612800, episode_reward=-0.06 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0641     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1612800     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008126547 |\n",
      "|    clip_fraction        | 0.186       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.4         |\n",
      "|    explained_variance   | 0.0729      |\n",
      "|    learning_rate        | 0.00126     |\n",
      "|    loss                 | -0.000371   |\n",
      "|    n_updates            | 790         |\n",
      "|    policy_gradient_loss | 0.00351     |\n",
      "|    std                  | 0.127       |\n",
      "|    value_loss           | 2.68e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0792  |\n",
      "| time/              |          |\n",
      "|    fps             | 760      |\n",
      "|    iterations      | 80       |\n",
      "|    time_elapsed    | 2121     |\n",
      "|    total_timesteps | 1612800  |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.26e+03     |\n",
      "|    ep_rew_mean          | -0.0798      |\n",
      "| time/                   |              |\n",
      "|    fps                  | 758          |\n",
      "|    iterations           | 82           |\n",
      "|    time_elapsed         | 2178         |\n",
      "|    total_timesteps      | 1653120      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075940816 |\n",
      "|    clip_fraction        | 0.139        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 1.44         |\n",
      "|    explained_variance   | 0.218        |\n",
      "|    learning_rate        | 0.00126      |\n",
      "|    loss                 | -0.00119     |\n",
      "|    n_updates            | 810          |\n",
      "|    policy_gradient_loss | 0.00192      |\n",
      "|    std                  | 0.124        |\n",
      "|    value_loss           | 2.43e-07     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0759     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 759         |\n",
      "|    iterations           | 84          |\n",
      "|    time_elapsed         | 2229        |\n",
      "|    total_timesteps      | 1693440     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030442875 |\n",
      "|    clip_fraction        | 0.287       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.48        |\n",
      "|    explained_variance   | 0.1         |\n",
      "|    learning_rate        | 0.00125     |\n",
      "|    loss                 | 0.00438     |\n",
      "|    n_updates            | 830         |\n",
      "|    policy_gradient_loss | 0.00542     |\n",
      "|    std                  | 0.122       |\n",
      "|    value_loss           | 4.41e-07    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.077      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 761         |\n",
      "|    iterations           | 86          |\n",
      "|    time_elapsed         | 2276        |\n",
      "|    total_timesteps      | 1733760     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023545774 |\n",
      "|    clip_fraction        | 0.254       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.55        |\n",
      "|    explained_variance   | 0.13        |\n",
      "|    learning_rate        | 0.00124     |\n",
      "|    loss                 | 0.00862     |\n",
      "|    n_updates            | 850         |\n",
      "|    policy_gradient_loss | 0.00584     |\n",
      "|    std                  | 0.118       |\n",
      "|    value_loss           | 4.03e-07    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0734     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 763         |\n",
      "|    iterations           | 88          |\n",
      "|    time_elapsed         | 2323        |\n",
      "|    total_timesteps      | 1774080     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033934712 |\n",
      "|    clip_fraction        | 0.314       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.59        |\n",
      "|    explained_variance   | 0.188       |\n",
      "|    learning_rate        | 0.00124     |\n",
      "|    loss                 | -0.000802   |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | 0.0155      |\n",
      "|    std                  | 0.115       |\n",
      "|    value_loss           | 1.63e-07    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0741     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 763         |\n",
      "|    iterations           | 90          |\n",
      "|    time_elapsed         | 2376        |\n",
      "|    total_timesteps      | 1814400     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015730472 |\n",
      "|    clip_fraction        | 0.151       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.64        |\n",
      "|    explained_variance   | 0.114       |\n",
      "|    learning_rate        | 0.00123     |\n",
      "|    loss                 | -0.00208    |\n",
      "|    n_updates            | 890         |\n",
      "|    policy_gradient_loss | 0.00221     |\n",
      "|    std                  | 0.112       |\n",
      "|    value_loss           | 1.97e-07    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0765     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 765         |\n",
      "|    iterations           | 92          |\n",
      "|    time_elapsed         | 2422        |\n",
      "|    total_timesteps      | 1854720     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026974248 |\n",
      "|    clip_fraction        | 0.281       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.69        |\n",
      "|    explained_variance   | 0.157       |\n",
      "|    learning_rate        | 0.00122     |\n",
      "|    loss                 | 0.00219     |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | 0.00852     |\n",
      "|    std                  | 0.11        |\n",
      "|    value_loss           | 1.56e-07    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0765     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 767         |\n",
      "|    iterations           | 94          |\n",
      "|    time_elapsed         | 2469        |\n",
      "|    total_timesteps      | 1895040     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013325607 |\n",
      "|    clip_fraction        | 0.153       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.73        |\n",
      "|    explained_variance   | 0.166       |\n",
      "|    learning_rate        | 0.00122     |\n",
      "|    loss                 | 0.000862    |\n",
      "|    n_updates            | 930         |\n",
      "|    policy_gradient_loss | 0.00263     |\n",
      "|    std                  | 0.108       |\n",
      "|    value_loss           | 8.76e-08    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.26e+03   |\n",
      "|    ep_rew_mean          | -0.0749    |\n",
      "| time/                   |            |\n",
      "|    fps                  | 768        |\n",
      "|    iterations           | 96         |\n",
      "|    time_elapsed         | 2519       |\n",
      "|    total_timesteps      | 1935360    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03169745 |\n",
      "|    clip_fraction        | 0.234      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 1.76       |\n",
      "|    explained_variance   | 0.258      |\n",
      "|    learning_rate        | 0.00121    |\n",
      "|    loss                 | 0.011      |\n",
      "|    n_updates            | 950        |\n",
      "|    policy_gradient_loss | 0.00851    |\n",
      "|    std                  | 0.106      |\n",
      "|    value_loss           | 7.37e-08   |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.26e+03   |\n",
      "|    ep_rew_mean          | -0.0721    |\n",
      "| time/                   |            |\n",
      "|    fps                  | 769        |\n",
      "|    iterations           | 98         |\n",
      "|    time_elapsed         | 2566       |\n",
      "|    total_timesteps      | 1975680    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02614129 |\n",
      "|    clip_fraction        | 0.192      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 1.81       |\n",
      "|    explained_variance   | 0.158      |\n",
      "|    learning_rate        | 0.00121    |\n",
      "|    loss                 | 0.0163     |\n",
      "|    n_updates            | 970        |\n",
      "|    policy_gradient_loss | 0.00647    |\n",
      "|    std                  | 0.103      |\n",
      "|    value_loss           | 6.39e-08   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=2016000, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0528     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2016000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005626377 |\n",
      "|    clip_fraction        | 0.144       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.85        |\n",
      "|    explained_variance   | 0.267       |\n",
      "|    learning_rate        | 0.0012      |\n",
      "|    loss                 | 8.46e-05    |\n",
      "|    n_updates            | 990         |\n",
      "|    policy_gradient_loss | 0.00146     |\n",
      "|    std                  | 0.101       |\n",
      "|    value_loss           | 1.07e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0723  |\n",
      "| time/              |          |\n",
      "|    fps             | 769      |\n",
      "|    iterations      | 100      |\n",
      "|    time_elapsed    | 2621     |\n",
      "|    total_timesteps | 2016000  |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.26e+03   |\n",
      "|    ep_rew_mean          | -0.0702    |\n",
      "| time/                   |            |\n",
      "|    fps                  | 770        |\n",
      "|    iterations           | 102        |\n",
      "|    time_elapsed         | 2667       |\n",
      "|    total_timesteps      | 2056320    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01716144 |\n",
      "|    clip_fraction        | 0.224      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 1.92       |\n",
      "|    explained_variance   | 0.117      |\n",
      "|    learning_rate        | 0.00119    |\n",
      "|    loss                 | 0.0232     |\n",
      "|    n_updates            | 1010       |\n",
      "|    policy_gradient_loss | 0.0073     |\n",
      "|    std                  | 0.0981     |\n",
      "|    value_loss           | 1.89e-07   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0703     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 772         |\n",
      "|    iterations           | 104         |\n",
      "|    time_elapsed         | 2714        |\n",
      "|    total_timesteps      | 2096640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020045998 |\n",
      "|    clip_fraction        | 0.221       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.95        |\n",
      "|    explained_variance   | 0.211       |\n",
      "|    learning_rate        | 0.00119     |\n",
      "|    loss                 | 0.00315     |\n",
      "|    n_updates            | 1030        |\n",
      "|    policy_gradient_loss | 0.0097      |\n",
      "|    std                  | 0.0965      |\n",
      "|    value_loss           | 3.05e-08    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0673     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 773         |\n",
      "|    iterations           | 106         |\n",
      "|    time_elapsed         | 2761        |\n",
      "|    total_timesteps      | 2136960     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037761264 |\n",
      "|    clip_fraction        | 0.289       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.98        |\n",
      "|    explained_variance   | 0.403       |\n",
      "|    learning_rate        | 0.00118     |\n",
      "|    loss                 | 0.00602     |\n",
      "|    n_updates            | 1050        |\n",
      "|    policy_gradient_loss | 0.0146      |\n",
      "|    std                  | 0.0952      |\n",
      "|    value_loss           | 1.1e-07     |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.26e+03   |\n",
      "|    ep_rew_mean          | -0.066     |\n",
      "| time/                   |            |\n",
      "|    fps                  | 777        |\n",
      "|    iterations           | 108        |\n",
      "|    time_elapsed         | 2800       |\n",
      "|    total_timesteps      | 2177280    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11404714 |\n",
      "|    clip_fraction        | 0.457      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 2          |\n",
      "|    explained_variance   | 0.179      |\n",
      "|    learning_rate        | 0.00118    |\n",
      "|    loss                 | 0.00692    |\n",
      "|    n_updates            | 1070       |\n",
      "|    policy_gradient_loss | 0.0353     |\n",
      "|    std                  | 0.094      |\n",
      "|    value_loss           | 7.73e-08   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0673     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 781         |\n",
      "|    iterations           | 110         |\n",
      "|    time_elapsed         | 2838        |\n",
      "|    total_timesteps      | 2217600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013469176 |\n",
      "|    clip_fraction        | 0.224       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.02        |\n",
      "|    explained_variance   | 0.236       |\n",
      "|    learning_rate        | 0.00117     |\n",
      "|    loss                 | -0.00107    |\n",
      "|    n_updates            | 1090        |\n",
      "|    policy_gradient_loss | 0.00425     |\n",
      "|    std                  | 0.0934      |\n",
      "|    value_loss           | 1.79e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1.26e+03  |\n",
      "|    ep_rew_mean          | -0.0704   |\n",
      "| time/                   |           |\n",
      "|    fps                  | 784       |\n",
      "|    iterations           | 112       |\n",
      "|    time_elapsed         | 2878      |\n",
      "|    total_timesteps      | 2257920   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0293708 |\n",
      "|    clip_fraction        | 0.271     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | 2.04      |\n",
      "|    explained_variance   | 0.201     |\n",
      "|    learning_rate        | 0.00116   |\n",
      "|    loss                 | 0.0156    |\n",
      "|    n_updates            | 1110      |\n",
      "|    policy_gradient_loss | 0.00796   |\n",
      "|    std                  | 0.0921    |\n",
      "|    value_loss           | 1.31e-07  |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.26e+03   |\n",
      "|    ep_rew_mean          | -0.0697    |\n",
      "| time/                   |            |\n",
      "|    fps                  | 787        |\n",
      "|    iterations           | 114        |\n",
      "|    time_elapsed         | 2916       |\n",
      "|    total_timesteps      | 2298240    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07282627 |\n",
      "|    clip_fraction        | 0.45       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 2.06       |\n",
      "|    explained_variance   | 0.207      |\n",
      "|    learning_rate        | 0.00116    |\n",
      "|    loss                 | 0.00644    |\n",
      "|    n_updates            | 1130       |\n",
      "|    policy_gradient_loss | 0.0247     |\n",
      "|    std                  | 0.0917     |\n",
      "|    value_loss           | 1.93e-07   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0694     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 791         |\n",
      "|    iterations           | 116         |\n",
      "|    time_elapsed         | 2955        |\n",
      "|    total_timesteps      | 2338560     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028785702 |\n",
      "|    clip_fraction        | 0.277       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.1         |\n",
      "|    explained_variance   | 0.236       |\n",
      "|    learning_rate        | 0.00115     |\n",
      "|    loss                 | 0.016       |\n",
      "|    n_updates            | 1150        |\n",
      "|    policy_gradient_loss | 0.0107      |\n",
      "|    std                  | 0.0896      |\n",
      "|    value_loss           | 1.52e-07    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0723     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 794         |\n",
      "|    iterations           | 118         |\n",
      "|    time_elapsed         | 2993        |\n",
      "|    total_timesteps      | 2378880     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010277244 |\n",
      "|    clip_fraction        | 0.211       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.14        |\n",
      "|    explained_variance   | 0.262       |\n",
      "|    learning_rate        | 0.00115     |\n",
      "|    loss                 | 0.00108     |\n",
      "|    n_updates            | 1170        |\n",
      "|    policy_gradient_loss | 0.00543     |\n",
      "|    std                  | 0.0881      |\n",
      "|    value_loss           | 8.08e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2419200, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0532     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2419200     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022492776 |\n",
      "|    clip_fraction        | 0.198       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.17        |\n",
      "|    explained_variance   | 0.269       |\n",
      "|    learning_rate        | 0.00114     |\n",
      "|    loss                 | 0.00846     |\n",
      "|    n_updates            | 1190        |\n",
      "|    policy_gradient_loss | 0.00492     |\n",
      "|    std                  | 0.0867      |\n",
      "|    value_loss           | 7.71e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.071   |\n",
      "| time/              |          |\n",
      "|    fps             | 796      |\n",
      "|    iterations      | 120      |\n",
      "|    time_elapsed    | 3038     |\n",
      "|    total_timesteps | 2419200  |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.26e+03   |\n",
      "|    ep_rew_mean          | -0.0685    |\n",
      "| time/                   |            |\n",
      "|    fps                  | 799        |\n",
      "|    iterations           | 122        |\n",
      "|    time_elapsed         | 3076       |\n",
      "|    total_timesteps      | 2459520    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12238327 |\n",
      "|    clip_fraction        | 0.523      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 2.2        |\n",
      "|    explained_variance   | 0.213      |\n",
      "|    learning_rate        | 0.00113    |\n",
      "|    loss                 | 0.0438     |\n",
      "|    n_updates            | 1210       |\n",
      "|    policy_gradient_loss | 0.0457     |\n",
      "|    std                  | 0.0859     |\n",
      "|    value_loss           | 5.18e-08   |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.26e+03   |\n",
      "|    ep_rew_mean          | -0.0664    |\n",
      "| time/                   |            |\n",
      "|    fps                  | 802        |\n",
      "|    iterations           | 124        |\n",
      "|    time_elapsed         | 3114       |\n",
      "|    total_timesteps      | 2499840    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12713905 |\n",
      "|    clip_fraction        | 0.542      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 2.21       |\n",
      "|    explained_variance   | 0.25       |\n",
      "|    learning_rate        | 0.00113    |\n",
      "|    loss                 | 0.0455     |\n",
      "|    n_updates            | 1230       |\n",
      "|    policy_gradient_loss | 0.0396     |\n",
      "|    std                  | 0.0856     |\n",
      "|    value_loss           | 7.23e-08   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.069      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 804         |\n",
      "|    iterations           | 126         |\n",
      "|    time_elapsed         | 3157        |\n",
      "|    total_timesteps      | 2540160     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019724678 |\n",
      "|    clip_fraction        | 0.124       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.22        |\n",
      "|    explained_variance   | 0.248       |\n",
      "|    learning_rate        | 0.00112     |\n",
      "|    loss                 | 0.00921     |\n",
      "|    n_updates            | 1250        |\n",
      "|    policy_gradient_loss | 0.00165     |\n",
      "|    std                  | 0.0845      |\n",
      "|    value_loss           | 2.59e-07    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0709     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 806         |\n",
      "|    iterations           | 128         |\n",
      "|    time_elapsed         | 3200        |\n",
      "|    total_timesteps      | 2580480     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007133231 |\n",
      "|    clip_fraction        | 0.243       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.26        |\n",
      "|    explained_variance   | 0.175       |\n",
      "|    learning_rate        | 0.00112     |\n",
      "|    loss                 | -0.00494    |\n",
      "|    n_updates            | 1270        |\n",
      "|    policy_gradient_loss | 0.00638     |\n",
      "|    std                  | 0.0835      |\n",
      "|    value_loss           | 1.27e-07    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.26e+03   |\n",
      "|    ep_rew_mean          | -0.0743    |\n",
      "| time/                   |            |\n",
      "|    fps                  | 809        |\n",
      "|    iterations           | 130        |\n",
      "|    time_elapsed         | 3238       |\n",
      "|    total_timesteps      | 2620800    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06469929 |\n",
      "|    clip_fraction        | 0.316      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 2.29       |\n",
      "|    explained_variance   | 0.23       |\n",
      "|    learning_rate        | 0.00111    |\n",
      "|    loss                 | 0.0268     |\n",
      "|    n_updates            | 1290       |\n",
      "|    policy_gradient_loss | 0.0118     |\n",
      "|    std                  | 0.0825     |\n",
      "|    value_loss           | 2.25e-07   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0704     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 812         |\n",
      "|    iterations           | 132         |\n",
      "|    time_elapsed         | 3276        |\n",
      "|    total_timesteps      | 2661120     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023509765 |\n",
      "|    clip_fraction        | 0.264       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.32        |\n",
      "|    explained_variance   | 0.25        |\n",
      "|    learning_rate        | 0.0011      |\n",
      "|    loss                 | -0.00442    |\n",
      "|    n_updates            | 1310        |\n",
      "|    policy_gradient_loss | 0.00891     |\n",
      "|    std                  | 0.0809      |\n",
      "|    value_loss           | 4.49e-08    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0722     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 814         |\n",
      "|    iterations           | 134         |\n",
      "|    time_elapsed         | 3314        |\n",
      "|    total_timesteps      | 2701440     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.051672243 |\n",
      "|    clip_fraction        | 0.413       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.34        |\n",
      "|    explained_variance   | 0.162       |\n",
      "|    learning_rate        | 0.0011      |\n",
      "|    loss                 | 0.0263      |\n",
      "|    n_updates            | 1330        |\n",
      "|    policy_gradient_loss | 0.0188      |\n",
      "|    std                  | 0.0805      |\n",
      "|    value_loss           | 1.18e-07    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0679     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 817         |\n",
      "|    iterations           | 136         |\n",
      "|    time_elapsed         | 3353        |\n",
      "|    total_timesteps      | 2741760     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017772397 |\n",
      "|    clip_fraction        | 0.237       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.35        |\n",
      "|    explained_variance   | 0.331       |\n",
      "|    learning_rate        | 0.00109     |\n",
      "|    loss                 | 0.0034      |\n",
      "|    n_updates            | 1350        |\n",
      "|    policy_gradient_loss | 0.0075      |\n",
      "|    std                  | 0.0798      |\n",
      "|    value_loss           | 2.66e-08    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0703     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 820         |\n",
      "|    iterations           | 138         |\n",
      "|    time_elapsed         | 3391        |\n",
      "|    total_timesteps      | 2782080     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023809657 |\n",
      "|    clip_fraction        | 0.344       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.37        |\n",
      "|    explained_variance   | 0.255       |\n",
      "|    learning_rate        | 0.00109     |\n",
      "|    loss                 | 0.0108      |\n",
      "|    n_updates            | 1370        |\n",
      "|    policy_gradient_loss | 0.013       |\n",
      "|    std                  | 0.0791      |\n",
      "|    value_loss           | 8.21e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2822400, episode_reward=-0.06 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0575    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2822400    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06748566 |\n",
      "|    clip_fraction        | 0.287      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 2.41       |\n",
      "|    explained_variance   | 0.278      |\n",
      "|    learning_rate        | 0.00108    |\n",
      "|    loss                 | 0.0413     |\n",
      "|    n_updates            | 1390       |\n",
      "|    policy_gradient_loss | 0.016      |\n",
      "|    std                  | 0.0767     |\n",
      "|    value_loss           | 4.55e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.068   |\n",
      "| time/              |          |\n",
      "|    fps             | 821      |\n",
      "|    iterations      | 140      |\n",
      "|    time_elapsed    | 3436     |\n",
      "|    total_timesteps | 2822400  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0678     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 823         |\n",
      "|    iterations           | 142         |\n",
      "|    time_elapsed         | 3476        |\n",
      "|    total_timesteps      | 2862720     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.063114315 |\n",
      "|    clip_fraction        | 0.453       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.43        |\n",
      "|    explained_variance   | 0.174       |\n",
      "|    learning_rate        | 0.00107     |\n",
      "|    loss                 | 0.0203      |\n",
      "|    n_updates            | 1410        |\n",
      "|    policy_gradient_loss | 0.0169      |\n",
      "|    std                  | 0.0768      |\n",
      "|    value_loss           | 2.13e-07    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.26e+03   |\n",
      "|    ep_rew_mean          | -0.0657    |\n",
      "| time/                   |            |\n",
      "|    fps                  | 825        |\n",
      "|    iterations           | 144        |\n",
      "|    time_elapsed         | 3515       |\n",
      "|    total_timesteps      | 2903040    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07913167 |\n",
      "|    clip_fraction        | 0.397      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 2.45       |\n",
      "|    explained_variance   | 0.193      |\n",
      "|    learning_rate        | 0.00107    |\n",
      "|    loss                 | 0.00513    |\n",
      "|    n_updates            | 1430       |\n",
      "|    policy_gradient_loss | 0.016      |\n",
      "|    std                  | 0.0759     |\n",
      "|    value_loss           | 8.7e-08    |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.067      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 828         |\n",
      "|    iterations           | 146         |\n",
      "|    time_elapsed         | 3553        |\n",
      "|    total_timesteps      | 2943360     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036454737 |\n",
      "|    clip_fraction        | 0.293       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.46        |\n",
      "|    explained_variance   | 0.179       |\n",
      "|    learning_rate        | 0.00106     |\n",
      "|    loss                 | 0.0072      |\n",
      "|    n_updates            | 1450        |\n",
      "|    policy_gradient_loss | 0.0109      |\n",
      "|    std                  | 0.0754      |\n",
      "|    value_loss           | 9.01e-08    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.068      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 830         |\n",
      "|    iterations           | 148         |\n",
      "|    time_elapsed         | 3592        |\n",
      "|    total_timesteps      | 2983680     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012827071 |\n",
      "|    clip_fraction        | 0.167       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.48        |\n",
      "|    explained_variance   | 0.263       |\n",
      "|    learning_rate        | 0.00106     |\n",
      "|    loss                 | 0.014       |\n",
      "|    n_updates            | 1470        |\n",
      "|    policy_gradient_loss | 0.00153     |\n",
      "|    std                  | 0.0744      |\n",
      "|    value_loss           | 3.44e-08    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0692     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 833         |\n",
      "|    iterations           | 150         |\n",
      "|    time_elapsed         | 3630        |\n",
      "|    total_timesteps      | 3024000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028539294 |\n",
      "|    clip_fraction        | 0.215       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.5         |\n",
      "|    explained_variance   | 0.0842      |\n",
      "|    learning_rate        | 0.00105     |\n",
      "|    loss                 | 0.00368     |\n",
      "|    n_updates            | 1490        |\n",
      "|    policy_gradient_loss | 0.00298     |\n",
      "|    std                  | 0.0739      |\n",
      "|    value_loss           | 3.67e-07    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.26e+03   |\n",
      "|    ep_rew_mean          | -0.0705    |\n",
      "| time/                   |            |\n",
      "|    fps                  | 835        |\n",
      "|    iterations           | 152        |\n",
      "|    time_elapsed         | 3668       |\n",
      "|    total_timesteps      | 3064320    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07684567 |\n",
      "|    clip_fraction        | 0.279      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 2.52       |\n",
      "|    explained_variance   | 0.251      |\n",
      "|    learning_rate        | 0.00104    |\n",
      "|    loss                 | 0.0136     |\n",
      "|    n_updates            | 1510       |\n",
      "|    policy_gradient_loss | 0.00946    |\n",
      "|    std                  | 0.0731     |\n",
      "|    value_loss           | 1.64e-07   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0742     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 837         |\n",
      "|    iterations           | 154         |\n",
      "|    time_elapsed         | 3706        |\n",
      "|    total_timesteps      | 3104640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.059088375 |\n",
      "|    clip_fraction        | 0.225       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.53        |\n",
      "|    explained_variance   | 0.195       |\n",
      "|    learning_rate        | 0.00104     |\n",
      "|    loss                 | 0.0122      |\n",
      "|    n_updates            | 1530        |\n",
      "|    policy_gradient_loss | 0.00402     |\n",
      "|    std                  | 0.073       |\n",
      "|    value_loss           | 6.38e-08    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0774     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 839         |\n",
      "|    iterations           | 156         |\n",
      "|    time_elapsed         | 3744        |\n",
      "|    total_timesteps      | 3144960     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.050453022 |\n",
      "|    clip_fraction        | 0.428       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.54        |\n",
      "|    explained_variance   | 0.117       |\n",
      "|    learning_rate        | 0.00103     |\n",
      "|    loss                 | 0.0189      |\n",
      "|    n_updates            | 1550        |\n",
      "|    policy_gradient_loss | 0.017       |\n",
      "|    std                  | 0.0727      |\n",
      "|    value_loss           | 1.45e-07    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.26e+03   |\n",
      "|    ep_rew_mean          | -0.074     |\n",
      "| time/                   |            |\n",
      "|    fps                  | 842        |\n",
      "|    iterations           | 158        |\n",
      "|    time_elapsed         | 3782       |\n",
      "|    total_timesteps      | 3185280    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03704644 |\n",
      "|    clip_fraction        | 0.286      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 2.55       |\n",
      "|    explained_variance   | 0.189      |\n",
      "|    learning_rate        | 0.00103    |\n",
      "|    loss                 | 0.0239     |\n",
      "|    n_updates            | 1570       |\n",
      "|    policy_gradient_loss | 0.0128     |\n",
      "|    std                  | 0.0721     |\n",
      "|    value_loss           | 2.11e-08   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=3225600, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0522    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3225600    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02209882 |\n",
      "|    clip_fraction        | 0.461      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 2.57       |\n",
      "|    explained_variance   | 0.272      |\n",
      "|    learning_rate        | 0.00102    |\n",
      "|    loss                 | 0.000534   |\n",
      "|    n_updates            | 1590       |\n",
      "|    policy_gradient_loss | 0.0462     |\n",
      "|    std                  | 0.0717     |\n",
      "|    value_loss           | 1.47e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0692  |\n",
      "| time/              |          |\n",
      "|    fps             | 842      |\n",
      "|    iterations      | 160      |\n",
      "|    time_elapsed    | 3827     |\n",
      "|    total_timesteps | 3225600  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0688     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 844         |\n",
      "|    iterations           | 162         |\n",
      "|    time_elapsed         | 3865        |\n",
      "|    total_timesteps      | 3265920     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036525045 |\n",
      "|    clip_fraction        | 0.24        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.59        |\n",
      "|    explained_variance   | 0.338       |\n",
      "|    learning_rate        | 0.00101     |\n",
      "|    loss                 | 0.00886     |\n",
      "|    n_updates            | 1610        |\n",
      "|    policy_gradient_loss | 0.00913     |\n",
      "|    std                  | 0.0701      |\n",
      "|    value_loss           | 4.62e-08    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0681     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 846         |\n",
      "|    iterations           | 164         |\n",
      "|    time_elapsed         | 3903        |\n",
      "|    total_timesteps      | 3306240     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.044957723 |\n",
      "|    clip_fraction        | 0.248       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.63        |\n",
      "|    explained_variance   | 0.253       |\n",
      "|    learning_rate        | 0.00101     |\n",
      "|    loss                 | 0.026       |\n",
      "|    n_updates            | 1630        |\n",
      "|    policy_gradient_loss | 0.00837     |\n",
      "|    std                  | 0.0691      |\n",
      "|    value_loss           | 7.88e-08    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0656     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 849         |\n",
      "|    iterations           | 166         |\n",
      "|    time_elapsed         | 3941        |\n",
      "|    total_timesteps      | 3346560     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023909364 |\n",
      "|    clip_fraction        | 0.287       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.65        |\n",
      "|    explained_variance   | 0.214       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 0.00861     |\n",
      "|    n_updates            | 1650        |\n",
      "|    policy_gradient_loss | 0.00981     |\n",
      "|    std                  | 0.0682      |\n",
      "|    value_loss           | 8.51e-08    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0645     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 850         |\n",
      "|    iterations           | 168         |\n",
      "|    time_elapsed         | 3979        |\n",
      "|    total_timesteps      | 3386880     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035213586 |\n",
      "|    clip_fraction        | 0.183       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.69        |\n",
      "|    explained_variance   | 0.212       |\n",
      "|    learning_rate        | 0.000995    |\n",
      "|    loss                 | 0.00182     |\n",
      "|    n_updates            | 1670        |\n",
      "|    policy_gradient_loss | 0.0043      |\n",
      "|    std                  | 0.0666      |\n",
      "|    value_loss           | 8.22e-08    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0619     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 852         |\n",
      "|    iterations           | 170         |\n",
      "|    time_elapsed         | 4018        |\n",
      "|    total_timesteps      | 3427200     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.046365518 |\n",
      "|    clip_fraction        | 0.328       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.71        |\n",
      "|    explained_variance   | 0.192       |\n",
      "|    learning_rate        | 0.000989    |\n",
      "|    loss                 | 0.00169     |\n",
      "|    n_updates            | 1690        |\n",
      "|    policy_gradient_loss | 0.0115      |\n",
      "|    std                  | 0.0664      |\n",
      "|    value_loss           | 1.6e-07     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0609     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 854         |\n",
      "|    iterations           | 172         |\n",
      "|    time_elapsed         | 4056        |\n",
      "|    total_timesteps      | 3467520     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017881626 |\n",
      "|    clip_fraction        | 0.311       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.72        |\n",
      "|    explained_variance   | 0.348       |\n",
      "|    learning_rate        | 0.000983    |\n",
      "|    loss                 | 0.0023      |\n",
      "|    n_updates            | 1710        |\n",
      "|    policy_gradient_loss | 0.0152      |\n",
      "|    std                  | 0.0661      |\n",
      "|    value_loss           | 8.18e-08    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.26e+03   |\n",
      "|    ep_rew_mean          | -0.0613    |\n",
      "| time/                   |            |\n",
      "|    fps                  | 856        |\n",
      "|    iterations           | 174        |\n",
      "|    time_elapsed         | 4095       |\n",
      "|    total_timesteps      | 3507840    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02076335 |\n",
      "|    clip_fraction        | 0.287      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 2.74       |\n",
      "|    explained_variance   | 0.305      |\n",
      "|    learning_rate        | 0.000977   |\n",
      "|    loss                 | -0.000305  |\n",
      "|    n_updates            | 1730       |\n",
      "|    policy_gradient_loss | 0.0119     |\n",
      "|    std                  | 0.0651     |\n",
      "|    value_loss           | 4.05e-08   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0615     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 858         |\n",
      "|    iterations           | 176         |\n",
      "|    time_elapsed         | 4133        |\n",
      "|    total_timesteps      | 3548160     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.055029333 |\n",
      "|    clip_fraction        | 0.42        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.76        |\n",
      "|    explained_variance   | 0.265       |\n",
      "|    learning_rate        | 0.000971    |\n",
      "|    loss                 | 0.0329      |\n",
      "|    n_updates            | 1750        |\n",
      "|    policy_gradient_loss | 0.0188      |\n",
      "|    std                  | 0.0649      |\n",
      "|    value_loss           | 8.83e-08    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.26e+03   |\n",
      "|    ep_rew_mean          | -0.0611    |\n",
      "| time/                   |            |\n",
      "|    fps                  | 859        |\n",
      "|    iterations           | 178        |\n",
      "|    time_elapsed         | 4172       |\n",
      "|    total_timesteps      | 3588480    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01990011 |\n",
      "|    clip_fraction        | 0.29       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 2.76       |\n",
      "|    explained_variance   | 0.151      |\n",
      "|    learning_rate        | 0.000965   |\n",
      "|    loss                 | -0.00338   |\n",
      "|    n_updates            | 1770       |\n",
      "|    policy_gradient_loss | 0.0066     |\n",
      "|    std                  | 0.0648     |\n",
      "|    value_loss           | 2.11e-07   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=3628800, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0524     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3628800     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014068438 |\n",
      "|    clip_fraction        | 0.346       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.78        |\n",
      "|    explained_variance   | 0.348       |\n",
      "|    learning_rate        | 0.000959    |\n",
      "|    loss                 | 0.00358     |\n",
      "|    n_updates            | 1790        |\n",
      "|    policy_gradient_loss | 0.0219      |\n",
      "|    std                  | 0.0642      |\n",
      "|    value_loss           | 6.52e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0605  |\n",
      "| time/              |          |\n",
      "|    fps             | 860      |\n",
      "|    iterations      | 180      |\n",
      "|    time_elapsed    | 4218     |\n",
      "|    total_timesteps | 3628800  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0627     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 861         |\n",
      "|    iterations           | 182         |\n",
      "|    time_elapsed         | 4257        |\n",
      "|    total_timesteps      | 3669120     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030369287 |\n",
      "|    clip_fraction        | 0.277       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.81        |\n",
      "|    explained_variance   | 0.182       |\n",
      "|    learning_rate        | 0.000953    |\n",
      "|    loss                 | 0.0252      |\n",
      "|    n_updates            | 1810        |\n",
      "|    policy_gradient_loss | 0.0138      |\n",
      "|    std                  | 0.0632      |\n",
      "|    value_loss           | 4.92e-08    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.062      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 863         |\n",
      "|    iterations           | 184         |\n",
      "|    time_elapsed         | 4295        |\n",
      "|    total_timesteps      | 3709440     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011164511 |\n",
      "|    clip_fraction        | 0.232       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.83        |\n",
      "|    explained_variance   | 0.291       |\n",
      "|    learning_rate        | 0.000947    |\n",
      "|    loss                 | 0.00757     |\n",
      "|    n_updates            | 1830        |\n",
      "|    policy_gradient_loss | 0.0138      |\n",
      "|    std                  | 0.0622      |\n",
      "|    value_loss           | 3.59e-08    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.26e+03   |\n",
      "|    ep_rew_mean          | -0.0634    |\n",
      "| time/                   |            |\n",
      "|    fps                  | 865        |\n",
      "|    iterations           | 186        |\n",
      "|    time_elapsed         | 4333       |\n",
      "|    total_timesteps      | 3749760    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06765683 |\n",
      "|    clip_fraction        | 0.258      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 2.86       |\n",
      "|    explained_variance   | 0.21       |\n",
      "|    learning_rate        | 0.000941   |\n",
      "|    loss                 | 0.0266     |\n",
      "|    n_updates            | 1850       |\n",
      "|    policy_gradient_loss | 0.00597    |\n",
      "|    std                  | 0.0616     |\n",
      "|    value_loss           | 3.53e-07   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.062      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 866         |\n",
      "|    iterations           | 188         |\n",
      "|    time_elapsed         | 4371        |\n",
      "|    total_timesteps      | 3790080     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.081490315 |\n",
      "|    clip_fraction        | 0.536       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.89        |\n",
      "|    explained_variance   | 0.215       |\n",
      "|    learning_rate        | 0.000935    |\n",
      "|    loss                 | 0.0375      |\n",
      "|    n_updates            | 1870        |\n",
      "|    policy_gradient_loss | 0.0279      |\n",
      "|    std                  | 0.0612      |\n",
      "|    value_loss           | 2.27e-07    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0624     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 868         |\n",
      "|    iterations           | 190         |\n",
      "|    time_elapsed         | 4410        |\n",
      "|    total_timesteps      | 3830400     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017002216 |\n",
      "|    clip_fraction        | 0.265       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.91        |\n",
      "|    explained_variance   | 0.226       |\n",
      "|    learning_rate        | 0.000928    |\n",
      "|    loss                 | 0.0102      |\n",
      "|    n_updates            | 1890        |\n",
      "|    policy_gradient_loss | 0.00861     |\n",
      "|    std                  | 0.0603      |\n",
      "|    value_loss           | 6.22e-08    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.26e+03   |\n",
      "|    ep_rew_mean          | -0.0632    |\n",
      "| time/                   |            |\n",
      "|    fps                  | 870        |\n",
      "|    iterations           | 192        |\n",
      "|    time_elapsed         | 4448       |\n",
      "|    total_timesteps      | 3870720    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08029135 |\n",
      "|    clip_fraction        | 0.423      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 2.93       |\n",
      "|    explained_variance   | 0.274      |\n",
      "|    learning_rate        | 0.000922   |\n",
      "|    loss                 | 0.0504     |\n",
      "|    n_updates            | 1910       |\n",
      "|    policy_gradient_loss | 0.0237     |\n",
      "|    std                  | 0.0596     |\n",
      "|    value_loss           | 9.97e-08   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0637     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 871         |\n",
      "|    iterations           | 194         |\n",
      "|    time_elapsed         | 4487        |\n",
      "|    total_timesteps      | 3911040     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.081125826 |\n",
      "|    clip_fraction        | 0.338       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.97        |\n",
      "|    explained_variance   | 0.33        |\n",
      "|    learning_rate        | 0.000916    |\n",
      "|    loss                 | 0.0224      |\n",
      "|    n_updates            | 1930        |\n",
      "|    policy_gradient_loss | 0.0158      |\n",
      "|    std                  | 0.0585      |\n",
      "|    value_loss           | 1.01e-07    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0634     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 873         |\n",
      "|    iterations           | 196         |\n",
      "|    time_elapsed         | 4525        |\n",
      "|    total_timesteps      | 3951360     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012841361 |\n",
      "|    clip_fraction        | 0.175       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.99        |\n",
      "|    explained_variance   | 0.347       |\n",
      "|    learning_rate        | 0.00091     |\n",
      "|    loss                 | 0.00263     |\n",
      "|    n_updates            | 1950        |\n",
      "|    policy_gradient_loss | 0.0021      |\n",
      "|    std                  | 0.058       |\n",
      "|    value_loss           | 1.12e-07    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0608     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 874         |\n",
      "|    iterations           | 198         |\n",
      "|    time_elapsed         | 4563        |\n",
      "|    total_timesteps      | 3991680     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024389472 |\n",
      "|    clip_fraction        | 0.287       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3           |\n",
      "|    explained_variance   | 0.259       |\n",
      "|    learning_rate        | 0.000904    |\n",
      "|    loss                 | 0.0253      |\n",
      "|    n_updates            | 1970        |\n",
      "|    policy_gradient_loss | 0.00938     |\n",
      "|    std                  | 0.0578      |\n",
      "|    value_loss           | 1.86e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4032000, episode_reward=-0.05 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.048      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4032000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007077441 |\n",
      "|    clip_fraction        | 0.241       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.02        |\n",
      "|    explained_variance   | 0.19        |\n",
      "|    learning_rate        | 0.000898    |\n",
      "|    loss                 | 0.00402     |\n",
      "|    n_updates            | 1990        |\n",
      "|    policy_gradient_loss | 0.00469     |\n",
      "|    std                  | 0.0576      |\n",
      "|    value_loss           | 2.36e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0607  |\n",
      "| time/              |          |\n",
      "|    fps             | 873      |\n",
      "|    iterations      | 200      |\n",
      "|    time_elapsed    | 4614     |\n",
      "|    total_timesteps | 4032000  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0637     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 874         |\n",
      "|    iterations           | 202         |\n",
      "|    time_elapsed         | 4657        |\n",
      "|    total_timesteps      | 4072320     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008860192 |\n",
      "|    clip_fraction        | 0.265       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.03        |\n",
      "|    explained_variance   | 0.18        |\n",
      "|    learning_rate        | 0.000892    |\n",
      "|    loss                 | 0.000599    |\n",
      "|    n_updates            | 2010        |\n",
      "|    policy_gradient_loss | 0.0108      |\n",
      "|    std                  | 0.057       |\n",
      "|    value_loss           | 1.24e-07    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0628     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 875         |\n",
      "|    iterations           | 204         |\n",
      "|    time_elapsed         | 4699        |\n",
      "|    total_timesteps      | 4112640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022823937 |\n",
      "|    clip_fraction        | 0.315       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.04        |\n",
      "|    explained_variance   | 0.254       |\n",
      "|    learning_rate        | 0.000886    |\n",
      "|    loss                 | 0.00957     |\n",
      "|    n_updates            | 2030        |\n",
      "|    policy_gradient_loss | 0.0136      |\n",
      "|    std                  | 0.0571      |\n",
      "|    value_loss           | 8e-08       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0646     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 875         |\n",
      "|    iterations           | 206         |\n",
      "|    time_elapsed         | 4741        |\n",
      "|    total_timesteps      | 4152960     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024567496 |\n",
      "|    clip_fraction        | 0.207       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.06        |\n",
      "|    explained_variance   | 0.251       |\n",
      "|    learning_rate        | 0.00088     |\n",
      "|    loss                 | 0.0176      |\n",
      "|    n_updates            | 2050        |\n",
      "|    policy_gradient_loss | 0.00542     |\n",
      "|    std                  | 0.0563      |\n",
      "|    value_loss           | 9.22e-08    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0651     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 208         |\n",
      "|    time_elapsed         | 4784        |\n",
      "|    total_timesteps      | 4193280     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010501729 |\n",
      "|    clip_fraction        | 0.198       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.09        |\n",
      "|    explained_variance   | 0.202       |\n",
      "|    learning_rate        | 0.000874    |\n",
      "|    loss                 | 0.00471     |\n",
      "|    n_updates            | 2070        |\n",
      "|    policy_gradient_loss | 0.00541     |\n",
      "|    std                  | 0.0557      |\n",
      "|    value_loss           | 8.25e-08    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0639     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 877         |\n",
      "|    iterations           | 210         |\n",
      "|    time_elapsed         | 4826        |\n",
      "|    total_timesteps      | 4233600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.066756256 |\n",
      "|    clip_fraction        | 0.42        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.11        |\n",
      "|    explained_variance   | 0.188       |\n",
      "|    learning_rate        | 0.000868    |\n",
      "|    loss                 | 0.0224      |\n",
      "|    n_updates            | 2090        |\n",
      "|    policy_gradient_loss | 0.019       |\n",
      "|    std                  | 0.0554      |\n",
      "|    value_loss           | 1.98e-07    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.26e+03     |\n",
      "|    ep_rew_mean          | -0.0633      |\n",
      "| time/                   |              |\n",
      "|    fps                  | 877          |\n",
      "|    iterations           | 212          |\n",
      "|    time_elapsed         | 4869         |\n",
      "|    total_timesteps      | 4273920      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0130461035 |\n",
      "|    clip_fraction        | 0.205        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 3.11         |\n",
      "|    explained_variance   | 0.243        |\n",
      "|    learning_rate        | 0.000862     |\n",
      "|    loss                 | 0.0109       |\n",
      "|    n_updates            | 2110         |\n",
      "|    policy_gradient_loss | 0.00375      |\n",
      "|    std                  | 0.0555       |\n",
      "|    value_loss           | 2.23e-07     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0596     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 878         |\n",
      "|    iterations           | 214         |\n",
      "|    time_elapsed         | 4911        |\n",
      "|    total_timesteps      | 4314240     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.044181865 |\n",
      "|    clip_fraction        | 0.3         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.14        |\n",
      "|    explained_variance   | 0.321       |\n",
      "|    learning_rate        | 0.000856    |\n",
      "|    loss                 | 0.00526     |\n",
      "|    n_updates            | 2130        |\n",
      "|    policy_gradient_loss | 0.0179      |\n",
      "|    std                  | 0.0545      |\n",
      "|    value_loss           | 4.34e-08    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0589     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 879         |\n",
      "|    iterations           | 216         |\n",
      "|    time_elapsed         | 4953        |\n",
      "|    total_timesteps      | 4354560     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020308057 |\n",
      "|    clip_fraction        | 0.206       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.18        |\n",
      "|    explained_variance   | 0.152       |\n",
      "|    learning_rate        | 0.00085     |\n",
      "|    loss                 | 0.0202      |\n",
      "|    n_updates            | 2150        |\n",
      "|    policy_gradient_loss | 0.00761     |\n",
      "|    std                  | 0.0535      |\n",
      "|    value_loss           | 4.93e-08    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.26e+03   |\n",
      "|    ep_rew_mean          | -0.0572    |\n",
      "| time/                   |            |\n",
      "|    fps                  | 879        |\n",
      "|    iterations           | 218        |\n",
      "|    time_elapsed         | 4996       |\n",
      "|    total_timesteps      | 4394880    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03434501 |\n",
      "|    clip_fraction        | 0.401      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 3.2        |\n",
      "|    explained_variance   | 0.183      |\n",
      "|    learning_rate        | 0.000844   |\n",
      "|    loss                 | -0.000254  |\n",
      "|    n_updates            | 2170       |\n",
      "|    policy_gradient_loss | 0.0176     |\n",
      "|    std                  | 0.0528     |\n",
      "|    value_loss           | 3.15e-07   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=4435200, episode_reward=-0.10 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0976     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4435200     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022841534 |\n",
      "|    clip_fraction        | 0.368       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.21        |\n",
      "|    explained_variance   | 0.213       |\n",
      "|    learning_rate        | 0.000838    |\n",
      "|    loss                 | 0.00492     |\n",
      "|    n_updates            | 2190        |\n",
      "|    policy_gradient_loss | 0.015       |\n",
      "|    std                  | 0.0527      |\n",
      "|    value_loss           | 1.24e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0608  |\n",
      "| time/              |          |\n",
      "|    fps             | 878      |\n",
      "|    iterations      | 220      |\n",
      "|    time_elapsed    | 5046     |\n",
      "|    total_timesteps | 4435200  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0576     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 879         |\n",
      "|    iterations           | 222         |\n",
      "|    time_elapsed         | 5089        |\n",
      "|    total_timesteps      | 4475520     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.049379185 |\n",
      "|    clip_fraction        | 0.45        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.22        |\n",
      "|    explained_variance   | 0.181       |\n",
      "|    learning_rate        | 0.000832    |\n",
      "|    loss                 | 0.0542      |\n",
      "|    n_updates            | 2210        |\n",
      "|    policy_gradient_loss | 0.0325      |\n",
      "|    std                  | 0.0528      |\n",
      "|    value_loss           | 2.82e-08    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0593     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 880         |\n",
      "|    iterations           | 224         |\n",
      "|    time_elapsed         | 5130        |\n",
      "|    total_timesteps      | 4515840     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018507402 |\n",
      "|    clip_fraction        | 0.252       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.22        |\n",
      "|    explained_variance   | 0.0772      |\n",
      "|    learning_rate        | 0.000826    |\n",
      "|    loss                 | 0.00764     |\n",
      "|    n_updates            | 2230        |\n",
      "|    policy_gradient_loss | 0.0063      |\n",
      "|    std                  | 0.0528      |\n",
      "|    value_loss           | 1.09e-07    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0602     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 880         |\n",
      "|    iterations           | 226         |\n",
      "|    time_elapsed         | 5172        |\n",
      "|    total_timesteps      | 4556160     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011209061 |\n",
      "|    clip_fraction        | 0.239       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.21        |\n",
      "|    explained_variance   | 0.25        |\n",
      "|    learning_rate        | 0.00082     |\n",
      "|    loss                 | -0.00484    |\n",
      "|    n_updates            | 2250        |\n",
      "|    policy_gradient_loss | 0.0105      |\n",
      "|    std                  | 0.053       |\n",
      "|    value_loss           | 4.79e-08    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0641     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 881         |\n",
      "|    iterations           | 228         |\n",
      "|    time_elapsed         | 5214        |\n",
      "|    total_timesteps      | 4596480     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017054502 |\n",
      "|    clip_fraction        | 0.25        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.22        |\n",
      "|    explained_variance   | 0.217       |\n",
      "|    learning_rate        | 0.000814    |\n",
      "|    loss                 | 0.00625     |\n",
      "|    n_updates            | 2270        |\n",
      "|    policy_gradient_loss | 0.00716     |\n",
      "|    std                  | 0.0529      |\n",
      "|    value_loss           | 8.77e-08    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0653     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 882         |\n",
      "|    iterations           | 230         |\n",
      "|    time_elapsed         | 5255        |\n",
      "|    total_timesteps      | 4636800     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013853459 |\n",
      "|    clip_fraction        | 0.275       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.25        |\n",
      "|    explained_variance   | 0.292       |\n",
      "|    learning_rate        | 0.000808    |\n",
      "|    loss                 | 0.00592     |\n",
      "|    n_updates            | 2290        |\n",
      "|    policy_gradient_loss | 0.00956     |\n",
      "|    std                  | 0.0518      |\n",
      "|    value_loss           | 1.68e-07    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.065      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 882         |\n",
      "|    iterations           | 232         |\n",
      "|    time_elapsed         | 5297        |\n",
      "|    total_timesteps      | 4677120     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021507667 |\n",
      "|    clip_fraction        | 0.27        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.29        |\n",
      "|    explained_variance   | 0.101       |\n",
      "|    learning_rate        | 0.000801    |\n",
      "|    loss                 | -0.00166    |\n",
      "|    n_updates            | 2310        |\n",
      "|    policy_gradient_loss | 0.00685     |\n",
      "|    std                  | 0.0511      |\n",
      "|    value_loss           | 9e-08       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0636     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 883         |\n",
      "|    iterations           | 234         |\n",
      "|    time_elapsed         | 5339        |\n",
      "|    total_timesteps      | 4717440     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022929486 |\n",
      "|    clip_fraction        | 0.252       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.29        |\n",
      "|    explained_variance   | 0.209       |\n",
      "|    learning_rate        | 0.000795    |\n",
      "|    loss                 | -0.00192    |\n",
      "|    n_updates            | 2330        |\n",
      "|    policy_gradient_loss | 0.00684     |\n",
      "|    std                  | 0.0512      |\n",
      "|    value_loss           | 6.24e-08    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.065      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 884         |\n",
      "|    iterations           | 236         |\n",
      "|    time_elapsed         | 5380        |\n",
      "|    total_timesteps      | 4757760     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.055315968 |\n",
      "|    clip_fraction        | 0.258       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.29        |\n",
      "|    explained_variance   | 0.219       |\n",
      "|    learning_rate        | 0.000789    |\n",
      "|    loss                 | 0.00747     |\n",
      "|    n_updates            | 2350        |\n",
      "|    policy_gradient_loss | 0.00651     |\n",
      "|    std                  | 0.0516      |\n",
      "|    value_loss           | 1.8e-07     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0654     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 884         |\n",
      "|    iterations           | 238         |\n",
      "|    time_elapsed         | 5422        |\n",
      "|    total_timesteps      | 4798080     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024881665 |\n",
      "|    clip_fraction        | 0.267       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.3         |\n",
      "|    explained_variance   | 0.231       |\n",
      "|    learning_rate        | 0.000783    |\n",
      "|    loss                 | -0.000752   |\n",
      "|    n_updates            | 2370        |\n",
      "|    policy_gradient_loss | 0.00885     |\n",
      "|    std                  | 0.0514      |\n",
      "|    value_loss           | 1.26e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4838400, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0755     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4838400     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027602829 |\n",
      "|    clip_fraction        | 0.272       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.3         |\n",
      "|    explained_variance   | 0.464       |\n",
      "|    learning_rate        | 0.000777    |\n",
      "|    loss                 | 0.0102      |\n",
      "|    n_updates            | 2390        |\n",
      "|    policy_gradient_loss | 0.011       |\n",
      "|    std                  | 0.0516      |\n",
      "|    value_loss           | 6.1e-08     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.068   |\n",
      "| time/              |          |\n",
      "|    fps             | 884      |\n",
      "|    iterations      | 240      |\n",
      "|    time_elapsed    | 5471     |\n",
      "|    total_timesteps | 4838400  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0711     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 884         |\n",
      "|    iterations           | 242         |\n",
      "|    time_elapsed         | 5513        |\n",
      "|    total_timesteps      | 4878720     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008809585 |\n",
      "|    clip_fraction        | 0.314       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.33        |\n",
      "|    explained_variance   | 0.25        |\n",
      "|    learning_rate        | 0.000771    |\n",
      "|    loss                 | 0.00313     |\n",
      "|    n_updates            | 2410        |\n",
      "|    policy_gradient_loss | 0.0104      |\n",
      "|    std                  | 0.0511      |\n",
      "|    value_loss           | 3.91e-07    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.26e+03   |\n",
      "|    ep_rew_mean          | -0.0701    |\n",
      "| time/                   |            |\n",
      "|    fps                  | 884        |\n",
      "|    iterations           | 244        |\n",
      "|    time_elapsed         | 5558       |\n",
      "|    total_timesteps      | 4919040    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04813266 |\n",
      "|    clip_fraction        | 0.328      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 3.33       |\n",
      "|    explained_variance   | 0.315      |\n",
      "|    learning_rate        | 0.000765   |\n",
      "|    loss                 | 0.0203     |\n",
      "|    n_updates            | 2430       |\n",
      "|    policy_gradient_loss | 0.0143     |\n",
      "|    std                  | 0.0514     |\n",
      "|    value_loss           | 9.82e-08   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0753     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 884         |\n",
      "|    iterations           | 246         |\n",
      "|    time_elapsed         | 5607        |\n",
      "|    total_timesteps      | 4959360     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021541607 |\n",
      "|    clip_fraction        | 0.252       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.34        |\n",
      "|    explained_variance   | 0.298       |\n",
      "|    learning_rate        | 0.000759    |\n",
      "|    loss                 | -0.000498   |\n",
      "|    n_updates            | 2450        |\n",
      "|    policy_gradient_loss | 0.00557     |\n",
      "|    std                  | 0.0512      |\n",
      "|    value_loss           | 1.93e-07    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0702     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 883         |\n",
      "|    iterations           | 248         |\n",
      "|    time_elapsed         | 5656        |\n",
      "|    total_timesteps      | 4999680     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026150374 |\n",
      "|    clip_fraction        | 0.172       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.34        |\n",
      "|    explained_variance   | 0.352       |\n",
      "|    learning_rate        | 0.000753    |\n",
      "|    loss                 | 0.00493     |\n",
      "|    n_updates            | 2470        |\n",
      "|    policy_gradient_loss | 0.00383     |\n",
      "|    std                  | 0.0516      |\n",
      "|    value_loss           | 1.1e-07     |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.26e+03   |\n",
      "|    ep_rew_mean          | -0.0725    |\n",
      "| time/                   |            |\n",
      "|    fps                  | 883        |\n",
      "|    iterations           | 250        |\n",
      "|    time_elapsed         | 5703       |\n",
      "|    total_timesteps      | 5040000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06121488 |\n",
      "|    clip_fraction        | 0.285      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 3.36       |\n",
      "|    explained_variance   | 0.156      |\n",
      "|    learning_rate        | 0.000747   |\n",
      "|    loss                 | 0.0228     |\n",
      "|    n_updates            | 2490       |\n",
      "|    policy_gradient_loss | 0.01       |\n",
      "|    std                  | 0.0508     |\n",
      "|    value_loss           | 9.84e-08   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0659     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 883         |\n",
      "|    iterations           | 252         |\n",
      "|    time_elapsed         | 5750        |\n",
      "|    total_timesteps      | 5080320     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031599585 |\n",
      "|    clip_fraction        | 0.379       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.37        |\n",
      "|    explained_variance   | 0.267       |\n",
      "|    learning_rate        | 0.000741    |\n",
      "|    loss                 | 0.00532     |\n",
      "|    n_updates            | 2510        |\n",
      "|    policy_gradient_loss | 0.026       |\n",
      "|    std                  | 0.0509      |\n",
      "|    value_loss           | 3.56e-08    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0639     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 882         |\n",
      "|    iterations           | 254         |\n",
      "|    time_elapsed         | 5801        |\n",
      "|    total_timesteps      | 5120640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010189527 |\n",
      "|    clip_fraction        | 0.194       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.38        |\n",
      "|    explained_variance   | 0.291       |\n",
      "|    learning_rate        | 0.000735    |\n",
      "|    loss                 | -0.0019     |\n",
      "|    n_updates            | 2530        |\n",
      "|    policy_gradient_loss | 0.00437     |\n",
      "|    std                  | 0.0507      |\n",
      "|    value_loss           | 1.45e-07    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0609     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 882         |\n",
      "|    iterations           | 256         |\n",
      "|    time_elapsed         | 5845        |\n",
      "|    total_timesteps      | 5160960     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015070976 |\n",
      "|    clip_fraction        | 0.182       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.38        |\n",
      "|    explained_variance   | 0.293       |\n",
      "|    learning_rate        | 0.000729    |\n",
      "|    loss                 | 0.00447     |\n",
      "|    n_updates            | 2550        |\n",
      "|    policy_gradient_loss | 0.00416     |\n",
      "|    std                  | 0.0505      |\n",
      "|    value_loss           | 9.48e-08    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0618     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 882         |\n",
      "|    iterations           | 258         |\n",
      "|    time_elapsed         | 5890        |\n",
      "|    total_timesteps      | 5201280     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013177946 |\n",
      "|    clip_fraction        | 0.258       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.4         |\n",
      "|    explained_variance   | 0.273       |\n",
      "|    learning_rate        | 0.000723    |\n",
      "|    loss                 | -0.00124    |\n",
      "|    n_updates            | 2570        |\n",
      "|    policy_gradient_loss | 0.00713     |\n",
      "|    std                  | 0.0501      |\n",
      "|    value_loss           | 9.99e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5241600, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0534     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5241600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021142364 |\n",
      "|    clip_fraction        | 0.227       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.41        |\n",
      "|    explained_variance   | 0.201       |\n",
      "|    learning_rate        | 0.000717    |\n",
      "|    loss                 | 0.00221     |\n",
      "|    n_updates            | 2590        |\n",
      "|    policy_gradient_loss | 0.00685     |\n",
      "|    std                  | 0.0503      |\n",
      "|    value_loss           | 1.46e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0589  |\n",
      "| time/              |          |\n",
      "|    fps             | 882      |\n",
      "|    iterations      | 260      |\n",
      "|    time_elapsed    | 5941     |\n",
      "|    total_timesteps | 5241600  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0559     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 882         |\n",
      "|    iterations           | 262         |\n",
      "|    time_elapsed         | 5988        |\n",
      "|    total_timesteps      | 5281920     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042105205 |\n",
      "|    clip_fraction        | 0.301       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.41        |\n",
      "|    explained_variance   | 0.229       |\n",
      "|    learning_rate        | 0.000711    |\n",
      "|    loss                 | 0.0241      |\n",
      "|    n_updates            | 2610        |\n",
      "|    policy_gradient_loss | 0.0148      |\n",
      "|    std                  | 0.0499      |\n",
      "|    value_loss           | 2.22e-08    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0537     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 881         |\n",
      "|    iterations           | 264         |\n",
      "|    time_elapsed         | 6036        |\n",
      "|    total_timesteps      | 5322240     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023253921 |\n",
      "|    clip_fraction        | 0.261       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.43        |\n",
      "|    explained_variance   | 0.1         |\n",
      "|    learning_rate        | 0.000705    |\n",
      "|    loss                 | 0.00342     |\n",
      "|    n_updates            | 2630        |\n",
      "|    policy_gradient_loss | 0.00727     |\n",
      "|    std                  | 0.0495      |\n",
      "|    value_loss           | 9.69e-08    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26e+03    |\n",
      "|    ep_rew_mean          | -0.0561     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 881         |\n",
      "|    iterations           | 266         |\n",
      "|    time_elapsed         | 6083        |\n",
      "|    total_timesteps      | 5362560     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040093876 |\n",
      "|    clip_fraction        | 0.2         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.43        |\n",
      "|    explained_variance   | 0.373       |\n",
      "|    learning_rate        | 0.000699    |\n",
      "|    loss                 | 0.0377      |\n",
      "|    n_updates            | 2650        |\n",
      "|    policy_gradient_loss | 0.0044      |\n",
      "|    std                  | 0.0496      |\n",
      "|    value_loss           | 8.62e-08    |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Correlations 0.7\n",
    "#t = start_and_release(paths1,action='small-More-Trust', obs = 'auto')\n",
    "with open(\"0.7Corr1Half.pkl\",\"rb\") as fp:\n",
    "    paths1 = pickle.load(fp)\n",
    "\n",
    "# Load Paths\n",
    "with open(\"0.7Corr2Half.pkl\",\"rb\") as fp:\n",
    "    paths2 = pickle.load(fp)\n",
    "\n",
    "with open(\"0.7CorrTest.pkl\",\"rb\") as fp:\n",
    "    paths_ev = pickle.load(fp)\n",
    "\n",
    "envs = VecMonitor(DummyVecEnv([\n",
    "    lambda: tradingEng(paths1,action = 'small-More-Trust', obs = 'xs'),\n",
    "    lambda: tradingEng(paths2,action = 'small-More-Trust', obs = 'xs')\n",
    "]),filename='logs07-train')\n",
    "\n",
    "ev_env = VecMonitor(DummyVecEnv([\n",
    "    lambda: tradingEng(paths_ev,action = 'small-More-Trust', obs = 'xs'),\n",
    "]))\n",
    "\n",
    "eval_callback = EvalCallback(\n",
    "    ev_env,\n",
    "    best_model_save_path='./logs/best_model07',\n",
    "    log_path='./logs/eval_logs07',\n",
    "    eval_freq=252*8*5*20,\n",
    "    deterministic=True,\n",
    "    render=False,\n",
    "    verbose = True,\n",
    "    n_eval_episodes = 8\n",
    ")\n",
    "\n",
    "# Instantiate the agent\n",
    "policy_kwargs = dict(activation_fn=th.nn.LeakyReLU,\n",
    "                     net_arch=dict(pi=[512,512,256,128,64,64,64,64,36,18], vf=[512,512,256,128,64,64,64,64,36,18], optimizers_class = th.optim.Adam, log_std_init = 0.005)) #\n",
    "model = PPO(\"MlpPolicy\", envs, batch_size = 252*4*5, learning_rate=linear_schedule(0.0015), policy_kwargs=policy_kwargs, n_steps=252*8*5, normalize_advantage=True, gamma = 0.9, verbose = 1) \n",
    "\n",
    "model.learn(total_timesteps=1e7, log_interval=2, callback=eval_callback) \n",
    "\n",
    "# Correlations -0.7\n",
    "#t = start_and_release(paths1,action='small-More-Trust', obs = 'auto')\n",
    "with open(\"NegCorrpt0.pkl\",\"rb\") as fp:\n",
    "    paths1 = pickle.load(fp)\n",
    "with open(\"NegCorrpt2.pkl\",\"rb\") as fp:\n",
    "    paths1 = paths1 + pickle.load(fp)\n",
    "# Load Paths\n",
    "with open(\"NegCorrpt4.pkl\",\"rb\") as fp:\n",
    "    paths2 = pickle.load(fp)\n",
    "with open(\"NegCorrpt3.pkl\",\"rb\") as fp:\n",
    "    paths2 = paths2 + pickle.load(fp)\n",
    "with open(\"n0.7CorrTest.pkl\",\"rb\") as fp:\n",
    "    paths_ev = pickle.load(fp)\n",
    "\n",
    "envs = VecMonitor(DummyVecEnv([\n",
    "    lambda: tradingEng(paths1,action = 'small-More-Trust', obs = 'xs'),\n",
    "    lambda: tradingEng(paths2,action = 'small-More-Trust', obs = 'xs')\n",
    "]),filename='logsm07-train')\n",
    "\n",
    "ev_env = VecMonitor(DummyVecEnv([\n",
    "    lambda: tradingEng(paths_ev,action = 'small-More-Trust', obs = 'xs'),\n",
    "]))\n",
    "\n",
    "eval_callback = EvalCallback(\n",
    "    ev_env,\n",
    "    best_model_save_path='./logs/best_modelm07',\n",
    "    log_path='./logs/eval_logsm07',\n",
    "    eval_freq=252*8*5*20,\n",
    "    deterministic=True,\n",
    "    render=False,\n",
    "    verbose = True,\n",
    "    n_eval_episodes = 8\n",
    ")\n",
    "\n",
    "# Instantiate the agent\n",
    "policy_kwargs = dict(activation_fn=th.nn.LeakyReLU,\n",
    "                     net_arch=dict(pi=[512,512,256,128,64,64,64,64,36,18], vf=[512,512,256,128,64,64,64,64,36,18], optimizers_class = th.optim.Adam, log_std_init = 0.005)) #\n",
    "model = PPO(\"MlpPolicy\", envs, batch_size = 252*4*5, learning_rate=linear_schedule(0.0015), policy_kwargs=policy_kwargs, n_steps=252*8*5, normalize_advantage=True, gamma = 0.9, verbose = 1) \n",
    "\n",
    "model.learn(total_timesteps=1e7, log_interval=2, callback=eval_callback) \n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
