{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import DDPG\n",
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import collections\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import tqdm\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from typing import Any, List, Sequence, Tuple\n",
    "\n",
    "import pickle\n",
    "\n",
    "import path_datatype\n",
    "import sys\n",
    "\n",
    "from env import tradingEng\n",
    "\n",
    "\n",
    "# Define environment\n",
    "with open(\"1.6kRunDemo.pkl\",\"rb\") as fp:\n",
    "    paths = pickle.load(fp)\n",
    "env = tradingEng(paths)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "n_actions = 18\n",
    "action_noise = OrnsteinUhlenbeckActionNoise(mean = np.zeros(n_actions), sigma = 0.05*np.ones(n_actions), theta = 0.01)\n",
    "model = DDPG(\"MlpPolicy\", env, action_noise=action_noise, verbose=1, batch_size=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.18e+03 |\n",
      "|    ep_rew_mean     | nan      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2        |\n",
      "|    fps             | 70       |\n",
      "|    time_elapsed    | 33       |\n",
      "|    total_timesteps | 2359     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | nan      |\n",
      "|    critic_loss     | nan      |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 2258     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.18e+03 |\n",
      "|    ep_rew_mean     | nan      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 73       |\n",
      "|    time_elapsed    | 64       |\n",
      "|    total_timesteps | 4702     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | nan      |\n",
      "|    critic_loss     | nan      |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 4601     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.17e+03 |\n",
      "|    ep_rew_mean     | nan      |\n",
      "| time/              |          |\n",
      "|    episodes        | 6        |\n",
      "|    fps             | 74       |\n",
      "|    time_elapsed    | 94       |\n",
      "|    total_timesteps | 7038     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | nan      |\n",
      "|    critic_loss     | nan      |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 6937     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.17e+03 |\n",
      "|    ep_rew_mean     | nan      |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 70       |\n",
      "|    time_elapsed    | 132      |\n",
      "|    total_timesteps | 9373     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | nan      |\n",
      "|    critic_loss     | nan      |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 9272     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.17e+03 |\n",
      "|    ep_rew_mean     | nan      |\n",
      "| time/              |          |\n",
      "|    episodes        | 10       |\n",
      "|    fps             | 70       |\n",
      "|    time_elapsed    | 167      |\n",
      "|    total_timesteps | 11722    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | nan      |\n",
      "|    critic_loss     | nan      |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 11621    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.17e+03 |\n",
      "|    ep_rew_mean     | nan      |\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 70       |\n",
      "|    time_elapsed    | 198      |\n",
      "|    total_timesteps | 14074    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | nan      |\n",
      "|    critic_loss     | nan      |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 13973    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.17e+03 |\n",
      "|    ep_rew_mean     | nan      |\n",
      "| time/              |          |\n",
      "|    episodes        | 14       |\n",
      "|    fps             | 71       |\n",
      "|    time_elapsed    | 230      |\n",
      "|    total_timesteps | 16429    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | nan      |\n",
      "|    critic_loss     | nan      |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 16328    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.17e+03 |\n",
      "|    ep_rew_mean     | nan      |\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 72       |\n",
      "|    time_elapsed    | 260      |\n",
      "|    total_timesteps | 18782    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | nan      |\n",
      "|    critic_loss     | nan      |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 18681    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.17e+03 |\n",
      "|    ep_rew_mean     | nan      |\n",
      "| time/              |          |\n",
      "|    episodes        | 18       |\n",
      "|    fps             | 72       |\n",
      "|    time_elapsed    | 290      |\n",
      "|    total_timesteps | 21138    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | nan      |\n",
      "|    critic_loss     | nan      |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 21037    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.17e+03 |\n",
      "|    ep_rew_mean     | nan      |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 73       |\n",
      "|    time_elapsed    | 320      |\n",
      "|    total_timesteps | 23490    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | nan      |\n",
      "|    critic_loss     | nan      |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 23389    |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "Nruns = 10\n",
    "model.learn(total_timesteps=251*10*Nruns, log_interval=2)\n",
    "model.save(\"ddpg_fin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run a test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You have passed a tuple to the predict() function instead of a Numpy array or a Dict. You are probably mixing Gym API with SB3 VecEnv API: `obs, info = env.reset()` (Gym) vs `obs = vec_env.reset()` (SB3 VecEnv). See related issue https://github.com/DLR-RM/stable-baselines3/issues/1694 and documentation for more information: https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecenv-api-vs-gym-api",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[11], line 11\u001b[0m\n",
      "\u001b[0;32m      9\u001b[0m obs \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n",
      "\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m episode_over:\n",
      "\u001b[1;32m---> 11\u001b[0m     action, _states \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(obs, deterministic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# replace with actual agent\u001b[39;00m\n",
      "\u001b[0;32m     12\u001b[0m     obs, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "\u001b[0;32m     13\u001b[0m     rewards\u001b[38;5;241m.\u001b[39mappend(reward)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\osc16\\miniconda3\\envs\\d2d\\Lib\\site-packages\\stable_baselines3\\common\\base_class.py:557\u001b[0m, in \u001b[0;36mBaseAlgorithm.predict\u001b[1;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n",
      "\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\n",
      "\u001b[0;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n",
      "\u001b[0;32m    539\u001b[0m     observation: Union[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray]],\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m    542\u001b[0m     deterministic: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "\u001b[0;32m    543\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[np\u001b[38;5;241m.\u001b[39mndarray, Optional[\u001b[38;5;28mtuple\u001b[39m[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]]]:\n",
      "\u001b[0;32m    544\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[0;32m    545\u001b[0m \u001b[38;5;124;03m    Get the policy action from an observation (and optional hidden state).\u001b[39;00m\n",
      "\u001b[0;32m    546\u001b[0m \u001b[38;5;124;03m    Includes sugar-coating to handle different observations (e.g. normalizing images).\u001b[39;00m\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m    555\u001b[0m \u001b[38;5;124;03m        (used in recurrent policies)\u001b[39;00m\n",
      "\u001b[0;32m    556\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[1;32m--> 557\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mpredict(observation, state, episode_start, deterministic)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\osc16\\miniconda3\\envs\\d2d\\Lib\\site-packages\\stable_baselines3\\common\\policies.py:357\u001b[0m, in \u001b[0;36mBasePolicy.predict\u001b[1;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n",
      "\u001b[0;32m    354\u001b[0m \u001b[38;5;66;03m# Check for common mistake that the user does not mix Gym/VecEnv API\u001b[39;00m\n",
      "\u001b[0;32m    355\u001b[0m \u001b[38;5;66;03m# Tuple obs are not supported by SB3, so we can safely do that check\u001b[39;00m\n",
      "\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(observation, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(observation) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(observation[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mdict\u001b[39m):\n",
      "\u001b[1;32m--> 357\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n",
      "\u001b[0;32m    358\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have passed a tuple to the predict() function instead of a Numpy array or a Dict. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m    359\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are probably mixing Gym API with SB3 VecEnv API: `obs, info = env.reset()` (Gym) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m    360\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvs `obs = vec_env.reset()` (SB3 VecEnv). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m    361\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee related issue https://github.com/DLR-RM/stable-baselines3/issues/1694 \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m    362\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand documentation for more information: https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecenv-api-vs-gym-api\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m    363\u001b[0m     )\n",
      "\u001b[0;32m    365\u001b[0m obs_tensor, vectorized_env \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs_to_tensor(observation)\n",
      "\u001b[0;32m    367\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\n",
      "\u001b[1;31mValueError\u001b[0m: You have passed a tuple to the predict() function instead of a Numpy array or a Dict. You are probably mixing Gym API with SB3 VecEnv API: `obs, info = env.reset()` (Gym) vs `obs = vec_env.reset()` (SB3 VecEnv). See related issue https://github.com/DLR-RM/stable-baselines3/issues/1694 and documentation for more information: https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecenv-api-vs-gym-api"
     ]
    }
   ],
   "source": [
    "\n",
    "from gymnasium.wrappers import RecordEpisodeStatistics, RecordVideo\n",
    "num_eval_episodes = 1\n",
    "\n",
    "env = tradingEng(paths)\n",
    "\n",
    "episode_over = False\n",
    "rewards = list()\n",
    "actions = list()\n",
    "obs, info = env.reset()\n",
    "while not episode_over:\n",
    "    action, _states = model.predict(obs, deterministic=True)  # replace with actual agent\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    rewards.append(reward)\n",
    "    actions.append(action)\n",
    "    episode_over = terminated or truncated\n",
    "env.close()\n",
    "\n",
    "print(f'Example action taken: {actions[0]}')\n",
    "print(f'Episode rewards: {rewards}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
